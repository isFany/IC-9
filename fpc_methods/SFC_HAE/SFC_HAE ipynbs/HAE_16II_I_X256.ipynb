{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HAE_16II_I_X256.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"n4fcdzt1L0fL"},"source":["This file uses 3 subnetworks to compress the varibales to 256"]},{"cell_type":"markdown","metadata":{"id":"abetc_9V6xEH"},"source":["## Mounting your google drive\n","\n","You can use google drive to store and access files e.g. storing and loading data from numpy or CSV files.  \n","Use the following command to mount your GDrive and access your files."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydOU6YpVLaow","executionInfo":{"status":"ok","timestamp":1629606678504,"user_tz":-60,"elapsed":590,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"0fc77592-ae30-44a4-e481-e0e86dcb6314"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSRYEjk782Cc","executionInfo":{"status":"ok","timestamp":1629606385978,"user_tz":-60,"elapsed":24975,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"d8a1bc6d-2a70-4e7c-a9a9-18a3d7e5cb34"},"source":["!pip install ffmpeg\n","!pip install vtk"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting ffmpeg\n","  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n","Building wheels for collected packages: ffmpeg\n","  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=25bef3ea46192c232c19e200c60068ce7a516c4d017b4880ecd603c6b7c2f8fd\n","  Stored in directory: /root/.cache/pip/wheels/64/80/6e/caa3e16deb0267c3cbfd36862058a724144e19fdb9eb03af0f\n","Successfully built ffmpeg\n","Installing collected packages: ffmpeg\n","Successfully installed ffmpeg-1.4\n","Collecting vtk\n","  Downloading vtk-9.0.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (59.5 MB)\n","\u001b[K     |████████████████████████████████| 59.5 MB 34 kB/s \n","\u001b[?25hCollecting wslink>=0.1.3\n","  Downloading wslink-1.0.6-py3-none-any.whl (20 kB)\n","Collecting autobahn>=17.7.1\n","  Downloading autobahn-21.3.1-py2.py3-none-any.whl (495 kB)\n","\u001b[K     |████████████████████████████████| 495 kB 37.7 MB/s \n","\u001b[?25hCollecting Twisted>=17.5.0\n","  Downloading Twisted-21.7.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 43.8 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from vtk) (3.2.2)\n","Collecting cryptography>=3.4.6\n","  Downloading cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2 MB)\n","\u001b[K     |████████████████████████████████| 3.2 MB 34.5 MB/s \n","\u001b[?25hCollecting hyperlink>=21.0.0\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.3 MB/s \n","\u001b[?25hCollecting txaio>=21.2.1\n","  Downloading txaio-21.2.1-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.4.6->autobahn>=17.7.1->vtk) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.4.6->autobahn>=17.7.1->vtk) (2.20)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=21.0.0->autobahn>=17.7.1->vtk) (2.10)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (1.19.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.0.0->vtk) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk) (3.7.4.3)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting zope.interface>=4.4.2\n","  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n","\u001b[K     |████████████████████████████████| 251 kB 48.8 MB/s \n","\u001b[?25hCollecting Automat>=0.8.0\n","  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk) (21.2.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 35.6 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.4.2->Twisted>=17.5.0->vtk) (57.4.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 51.4 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 48.1 MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->wslink>=0.1.3->vtk) (3.0.4)\n","Installing collected packages: multidict, yarl, async-timeout, zope.interface, txaio, incremental, hyperlink, cryptography, constantly, Automat, aiohttp, wslink, Twisted, autobahn, vtk\n","Successfully installed Automat-20.2.0 Twisted-21.7.0 aiohttp-3.7.4.post0 async-timeout-3.0.1 autobahn-21.3.1 constantly-15.1.0 cryptography-3.4.7 hyperlink-21.0.0 incremental-21.3.0 multidict-5.1.0 txaio-21.2.1 vtk-9.0.3 wslink-1.0.6 yarl-1.6.3 zope.interface-5.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lD9BrjrtYDPi","executionInfo":{"status":"ok","timestamp":1629606387410,"user_tz":-60,"elapsed":1456,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["import os\n","# change the current path. The user can adjust the path depend on the requirement\n","os.chdir(\"/content/gdrive/MyDrive/Cola-Notebooks/FYP/YF\")\n","import vtktools"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaGMonalKI3E","executionInfo":{"status":"ok","timestamp":1629606387411,"user_tz":-60,"elapsed":9,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"8687a411-e4f4-4249-a665-a3d72a2cb0b4"},"source":["! /opt/bin/nvidia-smi"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Sun Aug 22 04:26:28 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k2FU1lqyFRva","executionInfo":{"status":"ok","timestamp":1629606387411,"user_tz":-60,"elapsed":5,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["# !unzip csv_data.zip "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqsQSr0eyMDy","executionInfo":{"status":"ok","timestamp":1629606394729,"user_tz":-60,"elapsed":7322,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"8209644f-9655-414f-cdf1-209293d08f91"},"source":["%matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import numpy.linalg as la\n","import scipy.linalg as sl\n","import scipy.sparse.linalg as spl\n","import matplotlib.pyplot as plt\n","import torch.nn as nn  # Neural network module\n","import scipy.sparse as sp\n","import scipy.optimize as sop\n","import progressbar\n","# making slopes\n","import torch\n","from torch.utils.data import TensorDataset\n","import torch.nn.functional as F\n","from matplotlib.pyplot import LinearLocator\n","import matplotlib as mpl\n","import matplotlib.colors as colors\n","\n","\n","# create an animation\n","from matplotlib import animation\n","from IPython.display import HTML\n","\n","from matplotlib import animation\n","import math\n","import ffmpeg\n","\n","!pip install pycm livelossplot\n","%pylab inline\n","from livelossplot import PlotLosses\n","\n","from torch.utils.data import DataLoader\n","import torch.utils.data as Data\n","\n","import time\n","import platform\n","print('python version', platform.python_version())\n","print('torch version', torch.__version__)\n","print('numpy version', np.version.version)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting pycm\n","  Downloading pycm-3.2-py2.py3-none-any.whl (64 kB)\n","\u001b[?25l\r\u001b[K     |█████                           | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 2.0 MB/s \n","\u001b[?25hCollecting livelossplot\n","  Downloading livelossplot-0.5.4-py3-none-any.whl (22 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pycm) (1.19.5)\n","Collecting art>=1.8\n","  Downloading art-5.2-py2.py3-none-any.whl (571 kB)\n","\u001b[K     |████████████████████████████████| 571 kB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot) (5.5.0)\n","Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.7.4.3)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.8.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (2.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (5.0.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.4.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (57.4.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.10.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->livelossplot) (0.7.0)\n","Installing collected packages: art, pycm, livelossplot\n","Successfully installed art-5.2 livelossplot-0.5.4 pycm-3.2\n","Populating the interactive namespace from numpy and matplotlib\n","python version 3.7.11\n","torch version 1.9.0+cu102\n","numpy version 1.19.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rk1Uza3iuS6d","executionInfo":{"status":"ok","timestamp":1629606395504,"user_tz":-60,"elapsed":783,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"96495ab4-55d1-4e09-f154-81994543cc52"},"source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = True\n","\n","    return True\n","\n","device = 'cuda'  # Set out device to GPU\n","\n","print('Cuda installed, running on GPU!')  # print sentence"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Cuda installed, running on GPU!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xs1RZFSN4PoK","executionInfo":{"status":"ok","timestamp":1629606395505,"user_tz":-60,"elapsed":7,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["# These functions are saved in function.py and the note are also added to that file\n","def saveIndex(path_train, path_valid, path_test,train_index, valid_index, test_index):\n","    # save training and validation loss    \n","    np.savetxt(path_train,train_index, delimiter=',')\n","    np.savetxt(path_valid,valid_index, delimiter=',')\n","    np.savetxt(path_test,test_index, delimiter=',')\n","\n","def getIndex(path_train,path_valid,path_test):\n","    train_index = np.loadtxt(path_train,delimiter=\",\")\n","    valid_index = np.loadtxt(path_valid,delimiter=\",\")\n","    test_index = np.loadtxt(path_test,delimiter=\",\")\n","    return train_index,valid_index,test_index\n","\n","def saveMode(path_train, path_valid, path_test,mode_train, mode_valid, mode_test):\n","    # save training and validation loss    \n","    np.savetxt(path_train,mode_train.cpu().data.numpy(), delimiter=',')\n","    np.savetxt(path_valid,mode_valid.cpu().data.numpy(), delimiter=',')\n","    np.savetxt(path_test,mode_test.cpu().data.numpy(), delimiter=',')\n","\n","def getMode(path_train,path_valid,path_test):\n","    mode_train = np.loadtxt(path_train,delimiter=\",\")\n","    mode_valid = np.loadtxt(path_valid,delimiter=\",\")\n","    mode_test = np.loadtxt(path_test,delimiter=\",\")\n","    return mode_train,mode_valid,mode_test\n","\n","def saveCsv(pathcsv,EPOCH):\n","    # save training and validation loss\n","    losses_combined = np.zeros((EPOCH,3))\n","    losses_combined[:,0] = np.asarray(epoch_list)\n","    losses_combined[:,1] = np.asarray(loss_list)\n","    losses_combined[:,2] = np.asarray(loss_valid)\n","    np.savetxt(pathcsv, losses_combined , delimiter=',')\n","\n","def PlotMSELoss(pathName,name):\n","    epoch = pd.read_csv(pathName,usecols=[0]).values\n","    train_loss = pd.read_csv(pathName,usecols=[1]).values\n","    val_loss = pd.read_csv(pathName,usecols=[2]).values\n","\n","    fig = plt.figure(figsize=(10,7))\n","    axe1 = plt.subplot(111)\n","    axe1.semilogy(epoch,train_loss,label = \"train\")\n","    axe1.plot(epoch,val_loss,label = \"valid\")\n","    axe1.legend(loc = \"best\",fontsize=14)\n","    axe1.set_xlabel(\"$epoch$\",fontsize=14)\n","    axe1.set_ylabel(\"$MSE loss$\",fontsize=14)\n","    axe1.set_title(name,fontsize=14)\n","\n","def getTotal_decoded(training_decoded,valid_decoded,test_decoded,train_index,valid_index,test_index):\n","    total_decoded = np.zeros((nTotal,nNodes,2))\n","    for i in range(len(train_index)):\n","        total_decoded[int(train_index[i]),:,0] = training_decoded.cpu().detach().numpy()[i,:,0]\n","        total_decoded[int(train_index[i]),:,1] = training_decoded.cpu().detach().numpy()[i,:,1]\n","\n","    for i in range(len(valid_index)):\n","        total_decoded[int(valid_index[i]),:,0] = valid_decoded.cpu().detach().numpy()[i,:,0]\n","        total_decoded[int(valid_index[i]),:,1] = valid_decoded.cpu().detach().numpy()[i,:,1]\n","\n","    for i in range(len(test_index)):\n","        total_decoded[int(test_index[i]),:,0] = test_decoded.cpu().detach().numpy()[i,:,0]\n","        total_decoded[int(test_index[i]),:,1] = test_decoded.cpu().detach().numpy()[i,:,1]\n","    return total_decoded\n","\n","def getMSELoss(pathName):\n","    epoch = pd.read_csv(pathName,usecols=[0]).values\n","    train_loss = pd.read_csv(pathName,usecols=[1]).values\n","    val_loss = pd.read_csv(pathName,usecols=[2]).values\n","    return train_loss,val_loss,epoch\n","\n","\n","def index_split(train_ratio, valid_ratio, test_ratio, total_num):\n","    if train_ratio + valid_ratio + test_ratio != 1:\n","        raise ValueError(\"Three input ratio should sum to be 1!\")\n","    total_index = np.arange(total_num)\n","    rng = np.random.default_rng()\n","    total_index = rng.permutation(total_index)\n","    knot_1 = int(total_num * train_ratio)\n","    knot_2 = int(total_num * valid_ratio) + knot_1\n","    train_index, valid_index, test_index = np.split(total_index, [knot_1, knot_2])\n","    return train_index, valid_index, test_index"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"fejegy__4TOk","executionInfo":{"status":"ok","timestamp":1629606395505,"user_tz":-60,"elapsed":6,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["path_train = \"/content/gdrive/MyDrive/Cola-Notebooks/FYP/YF/\"+\"new_FPC_train_index.csv\"\n","path_valid = \"/content/gdrive/MyDrive/Cola-Notebooks/FYP/YF/\"+\"new_FPC_valid_index.csv\"\n","path_test = \"/content/gdrive/MyDrive/Cola-Notebooks/FYP/YF/\"+\"new_FPC_test_index.csv\"\n","# saveIndex(path_train, path_valid, path_test,train_index, valid_index, test_index)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fu0q-tOE4U5n","executionInfo":{"status":"ok","timestamp":1629606396105,"user_tz":-60,"elapsed":605,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"ca8f3220-f109-4e7f-9d6e-b5f25c902a30"},"source":["# Load the train_index, valid_index and test_index\n","train_index,valid_index,test_index= getIndex(path_train,path_valid,path_test)\n","print(test_index)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[ 133.  490. 1480.  730.  481. 1382.  440.  750. 1502. 1451.  692. 1094.\n"," 1679.  510. 1241. 1101.  543. 1312. 1432. 1988. 1148. 1801. 1519.  367.\n"," 1858. 1043. 1175. 1218. 1479.  103. 1363.  800.  258. 1851.  267.  999.\n","  611. 1824.  318.  753. 1413.  727. 1273. 1358. 1090.  838.  250. 1763.\n"," 1038.  439. 1199.  334. 1848. 1924. 1013.  271.  936.  600. 1553.  423.\n"," 1467. 1658.  929. 1748.  783.  329.  303. 1067.  868.  374. 1102. 1843.\n","  683.  449.  855. 1142. 1393.  194. 1112.  636. 1617. 1910. 1722.  536.\n"," 1149. 1765.  468. 1922. 1703. 1311.  341.  110. 1258. 1257. 1711.   93.\n"," 1969.  396. 1259.  199.  962. 1704.  462. 1407.  634.  535. 1505.  537.\n","  612. 1707. 1565. 1963. 1955.    3. 1058. 1946.  372. 1653. 1077.  414.\n","  469.  680. 1430.  649.  215.  234. 1692.  653. 1455.  582. 1169. 1138.\n","  411.  518.  865. 1977. 1688.  822.  397. 1388. 1221.  239.  249. 1781.\n"," 1751.  915.  278. 1970.  907.  477. 1552.  703.  870.  916. 1650.  561.\n"," 1401.  129. 1123. 1804. 1871. 1527.  308.   94. 1911. 1425. 1574.   72.\n","  399. 1410. 1818.  926.  897. 1238. 1628.  498. 1066. 1908.   36.  550.\n"," 1010.  524.  996.  732. 1048. 1041. 1474. 1339. 1889. 1289. 1795.  869.\n"," 1935. 1837.  684.  380.  967. 1445. 1729.  160.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dGuU-LvdBV3_"},"source":["# Hierarchical autoencoder"]},{"cell_type":"markdown","metadata":{"id":"XII9bTSmCzIO"},"source":["## First subnetwork"]},{"cell_type":"markdown","metadata":{"id":"rMkDfCiOBcNa"},"source":["### load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PBNpFB2vC5gb","executionInfo":{"status":"ok","timestamp":1629606396106,"user_tz":-60,"elapsed":8,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"4d5f56e1-68aa-4b0a-aa99-04e0e4faadfb"},"source":["os.chdir('/content/gdrive/MyDrive/Cola-Notebooks/FYP/YF')\n","print(os.getcwd())\n","\n","# read in the data (1000 csv files)\n","nTrain = 1600\n","nValid = 200\n","nTest = 200\n","nTotal = nTrain + nValid + nTest\n","nNodes = 20550 # should really work this out\n","\n","\n","# The below method to load data is too slow. Therefore, we use load pt file\n","\n","# [:, :, 2] is speed, [:, :, 3] is u, [:, :, 4] is v\n","# (speed not really needed)\n","# [:, :, 0] and [:, :, 1] are the SFC orderings\n","\n","# training_data = np.zeros((nTrain,nNodes,5))\n","# for i in range(nTrain):\n","#     data = np.loadtxt('csv_data/data_' +str(int(train_index[i]))+ '.csv', delimiter=',')\n","#     training_data[i,:,:] = data\n","# training_data = np.array(training_data)\n","# print('size training data', training_data.shape)\n","\n","# valid_data = np.zeros((nValid,nNodes,5))\n","# for i in range(nValid):\n","#     data = np.loadtxt('csv_data/data_' +str(int(valid_index[i]))+ '.csv', delimiter=',')\n","#     valid_data[i,:,:] = data\n","# valid_data = np.array(valid_data)\n","# print('size validation data', valid_data.shape)\n","\n","# test_data = np.zeros((nTest,nNodes,5))\n","# for i in range(nTest):\n","#     data = np.loadtxt('csv_data/data_' +str(int(test_index[i]))+ '.csv', delimiter=',')\n","#     test_data[i,:,:] = data\n","# test_data = np.array(test_data)\n","# print('size test data', test_data.shape)\n","\n","# total_data = np.zeros((nTotal,nNodes,5))\n","# for i in range(len(train_index)):\n","#     total_data[int(train_index[i]),:,:] = training_data[i,:,:]\n","\n","# for i in range(len(valid_index)):\n","#     total_data[int(valid_index[i]),:,:] = valid_data[i,:,:]\n","\n","# for i in range(len(test_index)):\n","#     total_data[int(test_index[i]),:,:] = test_data[i,:,:]\n","# print('size total data', total_data.shape)\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/Cola-Notebooks/FYP/YF\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hmrrUVTO4Zl3","executionInfo":{"status":"ok","timestamp":1629606396106,"user_tz":-60,"elapsed":5,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["# Before we save the pt file, we must load the data according to the above method\n","# torch.save(training_data, '/content/gdrive/MyDrive/FPC_new_random_train.pt')\n","# torch.save(valid_data, '/content/gdrive/MyDrive/FPC_new_random_valid.pt')\n","# torch.save(test_data, '/content/gdrive/MyDrive/FPC_new_random_test.pt')\n","# torch.save(total_data, '/content/gdrive/MyDrive/FPC_new_random_total.pt')\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QiQ4U2G7d0Kp","executionInfo":{"status":"ok","timestamp":1629606466410,"user_tz":-60,"elapsed":70308,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"1e1f56ff-f002-4da7-867c-4a2087bf76a6"},"source":["# load the data, this method save the time\n","training_data = torch.load('/content/gdrive/MyDrive/FPC_new_random_train.pt')\n","valid_data = torch.load('/content/gdrive/MyDrive/FPC_new_random_valid.pt')\n","test_data = torch.load('/content/gdrive/MyDrive/FPC_new_random_test.pt')\n","total_data = torch.load('/content/gdrive/MyDrive/FPC_new_random_total.pt')\n","print(training_data.shape)\n","print(valid_data.shape)\n","print(test_data.shape)\n","print(total_data.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["(1600, 20550, 5)\n","(200, 20550, 5)\n","(200, 20550, 5)\n","(2000, 20550, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YXRsMOCwUxwU","executionInfo":{"status":"ok","timestamp":1629606470276,"user_tz":-60,"elapsed":3879,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["# rescale the data so that u and v data lies in the range [-1,1] (and speed in [0,1])\n","ma = np.max(training_data[:, :, 2])\n","mi = np.min(training_data[:, :, 2])\n","k = 1./(ma - mi)\n","b = 1 - k*ma #k*mi\n","training_data[:, :, 2] = k * training_data[:, :, 2] + b #- b\n","# this won't be used\n","\n","ma = np.max(training_data[:, :, 3])\n","mi = np.min(training_data[:, :, 3])\n","ku = 2./(ma - mi)\n","bu = 1 - ku*ma \n","training_data[:, :, 3] = ku * training_data[:, :, 3] + bu\n","valid_data[:, :, 3] = ku * valid_data[:, :, 3] + bu\n","test_data[:, :, 3] = ku * test_data[:, :, 3] + bu\n","total_data[:, :, 3] = ku * total_data[:, :, 3] + bu\n","\n","ma = np.max(training_data[:, :, 4])\n","mi = np.min(training_data[:, :, 4])\n","kv = 2./(ma - mi)\n","bv = 1 - kv*ma\n","training_data[:, :, 4] = kv * training_data[:, :, 4] + bv\n","valid_data[:, :, 4] = kv * valid_data[:, :, 4] + bv\n","test_data[:, :, 4] = kv * test_data[:, :, 4] + bv\n","total_data[:, :, 4] = kv * total_data[:, :, 4] + bv"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VGN-qendDFff"},"source":["### Network architetcure"]},{"cell_type":"code","metadata":{"id":"yrCzaq9PDJnx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629603440765,"user_tz":-60,"elapsed":575,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"991ebf54-7de7-4f6d-97f9-2abf3b3b5b41"},"source":["# SFC-CAE: one curve with nearest neighbour smoothing and compressing to 85 latent variables\n","print(\"compress to 85\")\n","Latent_num = 85\n","torch.manual_seed(42)\n","# Hyper-parameters\n","EPOCH = 2001\n","BATCH_SIZE = 16\n","LR = 0.0001\n","k = nNodes # number of nodes - this has to match training_data.shape[0]\n","print(training_data.shape) # nTrain by number of nodes by 5\n","\n","# Data Loader for easy mini-batch return in training\n","train_loader = Data.DataLoader(dataset = training_data, batch_size = BATCH_SIZE, shuffle = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["compress to 85\n","(1600, 20550, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"imijLVOsDN1h"},"source":["# Standard\n","class CNN_1(nn.Module):\n","    def __init__(self):\n","        super(CNN_1, self).__init__()\n","        self.encoder_h1 = nn.Sequential(\n","            # input shape (16,4,20550)  # The first 16 is the batch size\n","            nn.Tanh(),\n","            nn.Conv1d(4, 8, 16, 4, 9),\n","            # output shape (16, 8, 5139)\n","            nn.Tanh(),\n","            nn.Conv1d(8, 8, 16, 4, 9),\n","            # output shape (16, 8,1286)\n","            nn.Tanh(),\n","            nn.Conv1d(8, 16, 16, 4, 9),\n","            # output shape (16,16,323)\n","            nn.Tanh(),\n","            nn.Conv1d(16, 16, 16, 4, 9),\n","            # output shape (16, 16, 82)\n","            nn.Tanh(),\n","        )\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(16*82, 85),\n","            nn.Tanh(),\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(85, 16*82),\n","            nn.Tanh(),\n","        )\n","        self.decoder_h1 = nn.Sequential(\n","            # (16, 16, 82)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(16, 16, 17, 4, 9), # (16, 16, 323)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(16, 8, 16, 4, 9), # (16, 8, 1286)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(8, 8, 17, 4, 9), # (16, 8, 5139)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(8, 4, 16, 4, 9), # (16, 4, 20550)\n","            nn.Tanh(),\n","        )\n","\n","        # input sparse layers, initialize weight as 0.33, bias as 0\n","        self.weight1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight1_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight1_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias1 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight11 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight11_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight11_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias11 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight2 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight2_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight2_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias2 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight22 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight22_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight22_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias22 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight3 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight3_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight3_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias3 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight33 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight33_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight33_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias33 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight4 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight4_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight4_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias4 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight44 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight44_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight44_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias44 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        \n","        # output sparse layers, initialize weight as 0.083, bias as 0\n","        self.weight_out1 = torch.nn.Parameter(torch.FloatTensor(0.083 *torch.ones(k)),requires_grad = True) \n","        self.weight_out1_0 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True) \n","        self.weight_out1_1 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out11 = torch.nn.Parameter(torch.FloatTensor(0.083 *torch.ones(k)),requires_grad = True) \n","        self.weight_out11_0 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True) \n","        self.weight_out11_1 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out2 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out2_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out2_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out22 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out22_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out22_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out3 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out3_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out3_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out33 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out33_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out33_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out4 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out4_0= torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out4_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out44 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out44_0= torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out44_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.bias_out1 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.bias_out2 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","\n","\n","    def forward(self, x):\n","        # print(\"X_size\",x.size())\n","        # first curve\n","        ToSFC1 = x[:, :, 0] # The first column is the first SFC ordering\n","        ToSFC1Up = torch.zeros_like(ToSFC1)\n","        ToSFC1Down = torch.zeros_like(ToSFC1)\n","        ToSFC1Up[:-1] = ToSFC1[1:]\n","        ToSFC1Up[-1] = ToSFC1[-1]\n","        ToSFC1Down[1:] = ToSFC1[:-1]\n","        ToSFC1Down[0] = ToSFC1[0]\n","\n","        batch_num = ToSFC1.shape[0]\n","        x1 = x[:, :, 3:5] # The fourth column and fifth column are velocities u and v respectively\n","        #print(\"x1\", x1.shape) #        # (16, 20550, 2)\n","        x1_1d = torch.zeros((batch_num, 4, k)).to(device)\n","        # first input sparse layer, then transform to sfc order1\n","        for j in range(batch_num):\n","            x1_1d[j, 0, :] = x1[j, :, 0][ToSFC1[j].long()] * self.weight1 + \\\n","                             x1[j, :, 0][ToSFC1Up[j].long()] * self.weight1_0 + \\\n","                             x1[j, :, 0][ToSFC1Down[j].long()] * self.weight1_1 + self.bias1\n","        \n","            x1_1d[j, 1, :] = x1[j, :, 0][ToSFC1[j].long()] * self.weight11 + \\\n","                             x1[j, :, 0][ToSFC1Up[j].long()] * self.weight11_0 + \\\n","                             x1[j, :, 0][ToSFC1Down[j].long()] * self.weight11_1 + self.bias11\n","\n","            x1_1d[j, 2, :] = x1[j, :, 1][ToSFC1[j].long()] * self.weight2 + \\\n","                             x1[j, :, 1][ToSFC1Up[j].long()] * self.weight2_0 + \\\n","                             x1[j, :, 1][ToSFC1Down[j].long()] * self.weight2_1 + self.bias2\n","\n","            x1_1d[j, 3, :] = x1[j, :, 1][ToSFC1[j].long()] * self.weight22 + \\\n","                             x1[j, :, 1][ToSFC1Up[j].long()] * self.weight22_0 + \\\n","                             x1[j, :, 1][ToSFC1Down[j].long()] * self.weight22_1 + self.bias22\n","\n","        # first cnn encoder\n","        encoded_1 = self.encoder_h1(x1_1d.view(-1, 4, k)) #(16,4,20550)\n","        # print(\"encoded\", encoded_1.shape)\n","        # flatten and concatenate\n","        encoded_3 = encoded_1.view(-1,16*82)\n","        # print(\"Before FC\", encoded_3.shape)\n","        # fully connection\n","        encoded = self.fc1(encoded_3) # (b,64)\n","        # print(\"After encoder FC，the output of encoder\",encoded.shape)  \n","        decoded_3 = self.decoder_h1(self.fc2(encoded).view(-1, 16, 82))\n","        # print(\"The output of decoder: \", decoded_3.shape) \n","        BackSFC1 = torch.argsort(ToSFC1)\n","        BackSFC1Up = torch.argsort(ToSFC1Up)\n","        BackSFC1Down = torch.argsort(ToSFC1Down)\n","\n","        decoded_sp = torch.zeros((batch_num, k, 2)).to(device)\n","        # output sparse layer, resort according to sfc transform\n","        for j in range(batch_num):\n","            decoded_sp[j, :, 0] = decoded_3[j, 0, :][BackSFC1[j].long()]* self.weight_out1 + \\\n","                                  decoded_3[j, 0, :][BackSFC1Up[j].long()] * self.weight_out1_0 + \\\n","                                  decoded_3[j, 0, :][BackSFC1Down[j].long()] * self.weight_out1_1 + \\\n","                                  decoded_3[j, 1, :][BackSFC1[j].long()]* self.weight_out11 + \\\n","                                  decoded_3[j, 1, :][BackSFC1Up[j].long()] * self.weight_out11_0 + \\\n","                                  decoded_3[j, 1, :][BackSFC1Down[j].long()] * self.weight_out11_1 + self.bias_out1\n","\n","            decoded_sp[j, :, 1] = decoded_3[j, 2, :][BackSFC1[j].long()] * self.weight_out3 + \\\n","                                  decoded_3[j, 2, :][BackSFC1Up[j].long()] * self.weight_out3_0 + \\\n","                                  decoded_3[j, 2, :][BackSFC1Down[j].long()] * self.weight_out3_1 + \\\n","                                  decoded_3[j, 3, :][BackSFC1[j].long()] * self.weight_out33 + \\\n","                                  decoded_3[j, 3, :][BackSFC1Up[j].long()] * self.weight_out33_0 + \\\n","                                  decoded_3[j, 3, :][BackSFC1Down[j].long()] * self.weight_out33_1 + self.bias_out2 \n","        \n","        # resort 1D to 2D\n","        decoded = F.tanh(decoded_sp) # both are BATCH_SIZE by nNodes by 2\n","        return encoded, decoded"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SiZOP8Z3EqyY"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"21OskV8XEpON"},"source":["# The first network has been trained at SFC-CAE. Therefore, the mode we can load directly\n","# autoencoder = torch.load(\"./SFC_CAE/pkl/II_Eran2000_LV85_B16_n1600_L0.0001.pkl\")\n","\n","# pass training, validation and test data through the autoencoder\n","# t_predict_0 = time.time()\n","\n","# mode_1train, training_decoded = autoencoder.to(device)(torch.tensor(training_data).to(device))\n","# error_autoencoder = (training_decoded.cpu().detach().numpy() - training_data[:,:,3:5])\n","# print(\"MSE_err of training data\", (error_autoencoder**2).mean())\n","\n","# mode_1valid, valid_decoded = autoencoder.to(device)(torch.tensor(valid_data).to(device))\n","# error_autoencoder = (valid_decoded.cpu().detach().numpy() - valid_data[:, :, 3:5])\n","# print(\"Mse_err of validation data\", (error_autoencoder**2).mean())\n","\n","# mode_1test, test_decoded = autoencoder.to(device)(torch.tensor(test_data).to(device))\n","# error_autoencoder = (test_decoded.cpu().detach().numpy() - test_data[:, :, 3:5])\n","# print(\"Mse_err of test data\", (error_autoencoder**2).mean())\n","# t_predict_1 = time.time()\n","\n","\n","# total_decoded = getTotal_decoded(training_decoded,valid_decoded,test_decoded,train_index,valid_index,test_index)\n","# error_autoencoder = (total_decoded - total_data[:, :, 3:5])\n","# print(\"Mse_err of total data\", (error_autoencoder**2).mean())\n","\n","# print(mode_1train.shape)\n","# print(mode_1valid.shape)\n","# print(mode_1test.shape)\n","# print('Predict time:',t_predict_1-t_predict_0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cgJFouPFHjB"},"source":["### Save and  Plot loss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"53Q2G6wCAf6y","executionInfo":{"status":"ok","timestamp":1629603507545,"user_tz":-60,"elapsed":2172,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"48740752-ce32-4052-e171-88df255f1066"},"source":["pathName = \"./SFC_CAE/csv/II_Eran2000_LV85_B16_n1600_L0.0001.csv\"\n","name = \"SFC-CAE MSE loss of 85 compression variables\"\n","PlotMSELoss(pathName,name)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x504 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"6UAVja2o-o1d"},"source":["### Get mode"]},{"cell_type":"code","metadata":{"id":"0QY7ScXM39NU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629603523348,"user_tz":-60,"elapsed":579,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"70ef287d-55a4-4cba-9aa1-54dd0d55a8f3"},"source":["Latent_num = 85\n","torch.manual_seed(42)\n","BATCH_SIZE = 16\n","LR = 0.0001\n","nTrain = 1600\n","\n","path_train = \"./HAE/mode_new/II_mode1_LV\"+str(Latent_num)+\"_Eran\"+str(2000) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\"_train.csv\"\n","path_valid = \"./HAE/mode_new/II_mode1_LV\"+str(Latent_num)+\"_Eran\"+str(2000) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\"_valid.csv\"\n","path_test = \"./HAE/mode_new/II_mode1_LV\"+str(Latent_num)+\"_Eran\"+str(2000) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\"_test.csv\"\n","print(path_train)\n","# saveMode(path_train,path_valid,path_test,mode_1train,mode_1valid,mode_1test)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["./HAE/mode_new/II_mode1_LV85_Eran2000_B16_n1600_L0.0001_train.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iNJtPHty4p8J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629603550915,"user_tz":-60,"elapsed":16406,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"6341ba4e-78be-403b-9e7c-a1024d497eb8"},"source":["mode_1train,mode_1valid,mode_1test = getMode(path_train,path_valid,path_test)\n","mode_1train = torch.from_numpy(mode_1train).to(device)\n","mode_1valid = torch.from_numpy(mode_1valid).to(device)\n","mode_1test = torch.from_numpy(mode_1test).to(device)\n","\n","print(mode_1train.shape)\n","print(mode_1test.shape)\n","print(mode_1valid.shape)\n","print(mode_1valid)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1600, 85])\n","torch.Size([200, 85])\n","torch.Size([200, 85])\n","tensor([[-6.3244e-04,  3.0356e-02,  1.9735e-01,  ..., -5.4836e-02,\n","         -5.4846e-02, -9.0150e-02],\n","        [-1.4138e-01, -1.6923e-01,  1.4185e-01,  ..., -3.4630e-02,\n","         -1.5213e-01,  8.3272e-02],\n","        [ 2.5769e-01, -4.9603e-02, -1.9080e-02,  ...,  9.4185e-02,\n","          9.8093e-02, -3.8044e-02],\n","        ...,\n","        [-1.7728e-01, -1.0768e-01, -9.4376e-02,  ...,  7.4078e-02,\n","          1.3801e-01, -8.3884e-03],\n","        [ 2.0426e-01, -6.9100e-02, -1.2694e-01,  ...,  8.5133e-02,\n","          1.1796e-01, -2.1182e-01],\n","        [-1.2068e-01, -2.2452e-01,  1.8721e-04,  ...,  7.2473e-02,\n","          1.8349e-01, -9.8247e-02]], device='cuda:0', dtype=torch.float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C9FtxL40KqST"},"source":["## Second network"]},{"cell_type":"markdown","metadata":{"id":"-ERmaAI_Uh0j"},"source":["### Network architecture"]},{"cell_type":"code","metadata":{"id":"tquCvmgMUhco","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629603568045,"user_tz":-60,"elapsed":641,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"7e6e2deb-0cfc-470b-a791-d018cfbc1bd6"},"source":["# SFC-HAE: one curve with nearest neighbour smoothing and compressing to 170 latent variables\n","print(\"compress to 170\")\n","torch.manual_seed(42)\n","# Hyper-parameters\n","Latent_num = 170\n","EPOCH = 2001\n","BATCH_SIZE = 16\n","LR = 0.0001\n","k = nNodes # number of nodes - this has to match training_data.shape[0]\n","print(training_data.shape) # nTrain by number of nodes by 5\n","\n","# Combing the input data and the mode\n","train_set = TensorDataset(torch.from_numpy(training_data), mode_1train)\n","\n","# Data Loader for easy mini-batch return in training\n","train_loader = Data.DataLoader(dataset = train_set, batch_size =BATCH_SIZE , shuffle = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["compress to 170\n","(1600, 20550, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UdDcNiMXUnCB"},"source":["class CNN_2(nn.Module):\n","    def __init__(self):\n","        super(CNN_2, self).__init__()\n","        self.encoder_h1 = nn.Sequential(\n","            # input shape (16,4,20550)   # The first 16 is the batch size\n","            nn.Tanh(),\n","            nn.Conv1d(4, 16, 32, 4, 16),\n","            # output shape (16, 16, 5138)\n","            nn.Tanh(),\n","            nn.Conv1d(16, 16, 32, 4, 16),\n","            # output shape (16, 16,1285)\n","            nn.Tanh(),\n","            nn.Conv1d(16, 16, 32, 4, 16),\n","            # output shape (16,16,322)\n","            nn.Tanh(),\n","            nn.Conv1d(16, 16, 32, 4, 16),\n","            # output shape (16,16,81)\n","            nn.Tanh(),\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(1296, 85),\n","            nn.Tanh(),\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(85*2, 16*81),\n","            nn.Tanh(),\n","        )\n","        self.decoder_h1 = nn.Sequential(\n","            # (b, 16, 81)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(16, 16, 32, 4, 15), # (16, 16, 322)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(16, 16, 32, 4, 15), # (16, 16, 1286)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(16, 16, 32, 4, 16), # (16, 16, 5140)\n","            nn.Tanh(),\n","            nn.ConvTranspose1d(16, 4, 32, 4, 19), # (16, 4, 20550)\n","            nn.Tanh(),\n","        )\n","\n","        # input sparse layers, initialize weight as 0.33, bias as 0\n","        self.weight1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight1_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight1_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias1 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight11 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight11_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight11_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias11 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight2 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight2_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight2_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias2 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight22 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight22_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight22_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias22 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight3 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight3_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight3_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias3 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight33 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight33_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight33_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias33 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight4 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight4_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight4_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias4 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight44 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight44_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight44_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias44 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        \n","        # output sparse layers, initialize weight as 0.083, bias as 0\n","        self.weight_out1 = torch.nn.Parameter(torch.FloatTensor(0.083 *torch.ones(k)),requires_grad = True) \n","        self.weight_out1_0 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True) \n","        self.weight_out1_1 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out11 = torch.nn.Parameter(torch.FloatTensor(0.083 *torch.ones(k)),requires_grad = True) \n","        self.weight_out11_0 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True) \n","        self.weight_out11_1 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out2 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out2_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out2_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out22 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out22_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out22_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out3 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out3_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out3_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out33 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out33_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out33_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out4 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out4_0= torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out4_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out44 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out44_0= torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out44_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.bias_out1 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.bias_out2 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","\n","\n","    def forward(self, x, mode):\n","        # print(\"X_size\",x.size())\n","        # first curve\n","        ToSFC1 = x[:, :, 0] # # The first column is the first SFC ordering\n","        ToSFC1Up = torch.zeros_like(ToSFC1)\n","        ToSFC1Down = torch.zeros_like(ToSFC1)\n","        ToSFC1Up[:-1] = ToSFC1[1:]\n","        ToSFC1Up[-1] = ToSFC1[-1]\n","        ToSFC1Down[1:] = ToSFC1[:-1]\n","        ToSFC1Down[0] = ToSFC1[0]\n","\n","        batch_num = ToSFC1.shape[0]\n","        #print(\"ToSFC1\",ToSFC1.shape) # (16, 20550)\n","        x1 = x[:, :, 3:5] # The fourth column and fifth column are velocities u and v respectively\n","        #print(\"x1\", x1.shape) #        # (16, 20550, 2)\n","        x1_1d = torch.zeros((batch_num, 4, k)).to(device)\n","        # first input sparse layer, then transform to sfc order1\n","        for j in range(batch_num):\n","            x1_1d[j, 0, :] = x1[j, :, 0][ToSFC1[j].long()] * self.weight1 + \\\n","                             x1[j, :, 0][ToSFC1Up[j].long()] * self.weight1_0 + \\\n","                             x1[j, :, 0][ToSFC1Down[j].long()] * self.weight1_1 + self.bias1\n","        \n","            x1_1d[j, 1, :] = x1[j, :, 0][ToSFC1[j].long()] * self.weight11 + \\\n","                             x1[j, :, 0][ToSFC1Up[j].long()] * self.weight11_0 + \\\n","                             x1[j, :, 0][ToSFC1Down[j].long()] * self.weight11_1 + self.bias11\n","\n","            x1_1d[j, 2, :] = x1[j, :, 1][ToSFC1[j].long()] * self.weight2 + \\\n","                             x1[j, :, 1][ToSFC1Up[j].long()] * self.weight2_0 + \\\n","                             x1[j, :, 1][ToSFC1Down[j].long()] * self.weight2_1 + self.bias2\n","\n","            x1_1d[j, 3, :] = x1[j, :, 1][ToSFC1[j].long()] * self.weight22 + \\\n","                             x1[j, :, 1][ToSFC1Up[j].long()] * self.weight22_0 + \\\n","                             x1[j, :, 1][ToSFC1Down[j].long()] * self.weight22_1 + self.bias22\n","\n","        # first cnn encoder\n","        encoded_1 = self.encoder_h1(x1_1d.view(-1, 4, k)) #(16,4,20550)\n","        # print(\"encoded\", encoded_1.shape)\n","        # flatten and concatenate\n","        encoded_3 = encoded_1.view(-1,16*81)\n","        # print(\"Before FC\", encoded_3.shape)\n","        # fully connection\n","        encoded = self.fc1(encoded_3) # (b,128)\n","        # print(\"After encoder FC，the output of encoder\",encoded.shape)     \n","        encoded = torch.cat((encoded, mode),axis = 1)  # Combine the mode_1 to the x1\n","        \n","        # print(\"encoded_combine\",encoded.shape)\n","        decoded_3 = self.decoder_h1(self.fc2(encoded).view(-1, 16, 81))\n","        # print(\"The output of decoder: \", decoded_3.shape) # (16, 2, 20550)\n","        BackSFC1 = torch.argsort(ToSFC1)\n","        BackSFC1Up = torch.argsort(ToSFC1Up)\n","        BackSFC1Down = torch.argsort(ToSFC1Down)\n","\n","        # k = 20550\n","        # batch_num = ToSFC1.shape[0]\n","        decoded_sp = torch.zeros((batch_num, k, 2)).to(device)\n","        # output sparse layer, resort according to sfc transform\n","        for j in range(batch_num):\n","            decoded_sp[j, :, 0] = decoded_3[j, 0, :][BackSFC1[j].long()]* self.weight_out1 + \\\n","                                  decoded_3[j, 0, :][BackSFC1Up[j].long()] * self.weight_out1_0 + \\\n","                                  decoded_3[j, 0, :][BackSFC1Down[j].long()] * self.weight_out1_1 + \\\n","                                  decoded_3[j, 1, :][BackSFC1[j].long()]* self.weight_out11 + \\\n","                                  decoded_3[j, 1, :][BackSFC1Up[j].long()] * self.weight_out11_0 + \\\n","                                  decoded_3[j, 1, :][BackSFC1Down[j].long()] * self.weight_out11_1 + self.bias_out1\n","\n","            decoded_sp[j, :, 1] = decoded_3[j, 2, :][BackSFC1[j].long()] * self.weight_out3 + \\\n","                                  decoded_3[j, 2, :][BackSFC1Up[j].long()] * self.weight_out3_0 + \\\n","                                  decoded_3[j, 2, :][BackSFC1Down[j].long()] * self.weight_out3_1 + \\\n","                                  decoded_3[j, 3, :][BackSFC1[j].long()] * self.weight_out33 + \\\n","                                  decoded_3[j, 3, :][BackSFC1Up[j].long()] * self.weight_out33_0 + \\\n","                                  decoded_3[j, 3, :][BackSFC1Down[j].long()] * self.weight_out33_1 + self.bias_out2       \n","        # resort 1D to 2D\n","        decoded = F.tanh(decoded_sp) # both are BATCH_SIZE by nNodes by 2\n","        return encoded, decoded"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4UZZPRniaJD"},"source":["### Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIef2Uosd8PZ","executionInfo":{"status":"ok","timestamp":1628060522669,"user_tz":-60,"elapsed":23851923,"user":{"displayName":"杨钒","photoUrl":"","userId":"12928489296811262671"}},"outputId":"d22b3094-77b6-4e9b-85e2-ac5fd0f7d62b"},"source":["# train the autoencoder\n","t_train_0 = time.time()\n","autoencoder_2 = CNN_2().to(device)\n","optimizer = torch.optim.Adam(autoencoder_2.parameters(), lr=LR)\n","loss_func = nn.MSELoss()\n","\n","loss_list = []\n","loss_valid = []\n","epoch_list=[]\n","\n","for epoch in range(EPOCH):\n","    for x, mode in train_loader:\n","        detach_mode = mode.detach()\n","        b_y = x[:, :, 3:5].to(device)   # b_y= False\n","        b_x = x.to(device)    # b_x: False\n","      \n","        b_mode = detach_mode.to(device)  #b_mode = True  #The size is [16,128]\n","        \n","        # print(\"b_mode\",b_mode.requires_grad)\n","        encoded, decoded = autoencoder_2(b_x.float(),b_mode.float())   #decoded true\n","        # decoded.detach_()\n","        # decoded = decoded.detach()\n","      \n","        loss = loss_func(decoded, b_y.float()) # Loss: True       # mean square error\n","        optimizer.zero_grad()                  # clear gradients for this training step\n","        loss.backward()                     # backpropagation, compute gradients\n","        optimizer.step()                     # apply gradients\n","\n","    loss_list.append(loss)\n","\n","    encoded, decoded = autoencoder_2(torch.tensor(valid_data).to(device),mode_1valid.float().to(device))\n","    error_autoencoder_2 = (decoded.detach() - torch.tensor(valid_data[:,:, 3:5]).to(device))\n","    MSE_valid = (error_autoencoder_2**2).mean()\n","    loss_valid.append(MSE_valid)\n","    epoch_list.append(epoch)\n","    print('Epoch: ', epoch, '| train loss: %.6f' % loss.cpu().data.numpy(), '| valid loss: %.6f' % MSE_valid)\n","\n","    #save the weights every 500 epochs \n","    if (epoch%500 == 0):\n","        torch.save(autoencoder_2, \"./HAE/pkl/2_Eran\"+str(epoch) +\"_LV\"+str(Latent_num)+ \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\".pkl\")\n","        pathcsv= \"./HAE/csv/2_Eran\"+str(epoch)+\"_LV\"+str(Latent_num) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\".csv\"\n","        saveCsv(pathcsv,epoch+1)\n","\n","t_train_1 = time.time()\n","# torch.save(autoencoder_2, path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:  0 | train loss: 0.028633 | valid loss: 0.029399\n","Epoch:  1 | train loss: 0.025131 | valid loss: 0.024968\n","Epoch:  2 | train loss: 0.022414 | valid loss: 0.022807\n","Epoch:  3 | train loss: 0.021595 | valid loss: 0.021320\n","Epoch:  4 | train loss: 0.020481 | valid loss: 0.019986\n","Epoch:  5 | train loss: 0.019325 | valid loss: 0.018700\n","Epoch:  6 | train loss: 0.017704 | valid loss: 0.017507\n","Epoch:  7 | train loss: 0.016992 | valid loss: 0.016511\n","Epoch:  8 | train loss: 0.015910 | valid loss: 0.015551\n","Epoch:  9 | train loss: 0.014207 | valid loss: 0.014623\n","Epoch:  10 | train loss: 0.014063 | valid loss: 0.013800\n","Epoch:  11 | train loss: 0.012717 | valid loss: 0.012789\n","Epoch:  12 | train loss: 0.010645 | valid loss: 0.011061\n","Epoch:  13 | train loss: 0.009299 | valid loss: 0.009362\n","Epoch:  14 | train loss: 0.008325 | valid loss: 0.008220\n","Epoch:  15 | train loss: 0.007370 | valid loss: 0.007367\n","Epoch:  16 | train loss: 0.006822 | valid loss: 0.006682\n","Epoch:  17 | train loss: 0.005987 | valid loss: 0.006101\n","Epoch:  18 | train loss: 0.005911 | valid loss: 0.005636\n","Epoch:  19 | train loss: 0.004985 | valid loss: 0.005247\n","Epoch:  20 | train loss: 0.005052 | valid loss: 0.004908\n","Epoch:  21 | train loss: 0.004625 | valid loss: 0.004582\n","Epoch:  22 | train loss: 0.004828 | valid loss: 0.004273\n","Epoch:  23 | train loss: 0.003710 | valid loss: 0.004000\n","Epoch:  24 | train loss: 0.003560 | valid loss: 0.003749\n","Epoch:  25 | train loss: 0.003647 | valid loss: 0.003524\n","Epoch:  26 | train loss: 0.003107 | valid loss: 0.003316\n","Epoch:  27 | train loss: 0.003030 | valid loss: 0.003132\n","Epoch:  28 | train loss: 0.003025 | valid loss: 0.002964\n","Epoch:  29 | train loss: 0.002930 | valid loss: 0.002813\n","Epoch:  30 | train loss: 0.002314 | valid loss: 0.002674\n","Epoch:  31 | train loss: 0.002839 | valid loss: 0.002555\n","Epoch:  32 | train loss: 0.002088 | valid loss: 0.002436\n","Epoch:  33 | train loss: 0.002264 | valid loss: 0.002332\n","Epoch:  34 | train loss: 0.002367 | valid loss: 0.002247\n","Epoch:  35 | train loss: 0.002271 | valid loss: 0.002168\n","Epoch:  36 | train loss: 0.002061 | valid loss: 0.002092\n","Epoch:  37 | train loss: 0.001739 | valid loss: 0.002023\n","Epoch:  38 | train loss: 0.002008 | valid loss: 0.001964\n","Epoch:  39 | train loss: 0.002035 | valid loss: 0.001906\n","Epoch:  40 | train loss: 0.001958 | valid loss: 0.001851\n","Epoch:  41 | train loss: 0.001699 | valid loss: 0.001802\n","Epoch:  42 | train loss: 0.002110 | valid loss: 0.001752\n","Epoch:  43 | train loss: 0.002082 | valid loss: 0.001709\n","Epoch:  44 | train loss: 0.001768 | valid loss: 0.001667\n","Epoch:  45 | train loss: 0.001505 | valid loss: 0.001624\n","Epoch:  46 | train loss: 0.001418 | valid loss: 0.001583\n","Epoch:  47 | train loss: 0.001225 | valid loss: 0.001548\n","Epoch:  48 | train loss: 0.001583 | valid loss: 0.001513\n","Epoch:  49 | train loss: 0.001437 | valid loss: 0.001478\n","Epoch:  50 | train loss: 0.001295 | valid loss: 0.001445\n","Epoch:  51 | train loss: 0.001372 | valid loss: 0.001419\n","Epoch:  52 | train loss: 0.001191 | valid loss: 0.001390\n","Epoch:  53 | train loss: 0.001401 | valid loss: 0.001361\n","Epoch:  54 | train loss: 0.001120 | valid loss: 0.001335\n","Epoch:  55 | train loss: 0.001377 | valid loss: 0.001309\n","Epoch:  56 | train loss: 0.001425 | valid loss: 0.001285\n","Epoch:  57 | train loss: 0.001295 | valid loss: 0.001262\n","Epoch:  58 | train loss: 0.001051 | valid loss: 0.001240\n","Epoch:  59 | train loss: 0.001461 | valid loss: 0.001216\n","Epoch:  60 | train loss: 0.001306 | valid loss: 0.001196\n","Epoch:  61 | train loss: 0.001223 | valid loss: 0.001177\n","Epoch:  62 | train loss: 0.001009 | valid loss: 0.001158\n","Epoch:  63 | train loss: 0.001048 | valid loss: 0.001140\n","Epoch:  64 | train loss: 0.001377 | valid loss: 0.001123\n","Epoch:  65 | train loss: 0.001088 | valid loss: 0.001104\n","Epoch:  66 | train loss: 0.001091 | valid loss: 0.001089\n","Epoch:  67 | train loss: 0.000917 | valid loss: 0.001073\n","Epoch:  68 | train loss: 0.001131 | valid loss: 0.001058\n","Epoch:  69 | train loss: 0.001114 | valid loss: 0.001047\n","Epoch:  70 | train loss: 0.001032 | valid loss: 0.001030\n","Epoch:  71 | train loss: 0.000917 | valid loss: 0.001018\n","Epoch:  72 | train loss: 0.001105 | valid loss: 0.001004\n","Epoch:  73 | train loss: 0.001152 | valid loss: 0.000990\n","Epoch:  74 | train loss: 0.001068 | valid loss: 0.000980\n","Epoch:  75 | train loss: 0.001274 | valid loss: 0.000965\n","Epoch:  76 | train loss: 0.001320 | valid loss: 0.000956\n","Epoch:  77 | train loss: 0.000949 | valid loss: 0.000942\n","Epoch:  78 | train loss: 0.001127 | valid loss: 0.000932\n","Epoch:  79 | train loss: 0.000997 | valid loss: 0.000923\n","Epoch:  80 | train loss: 0.000901 | valid loss: 0.000911\n","Epoch:  81 | train loss: 0.000812 | valid loss: 0.000904\n","Epoch:  82 | train loss: 0.001111 | valid loss: 0.000894\n","Epoch:  83 | train loss: 0.000699 | valid loss: 0.000883\n","Epoch:  84 | train loss: 0.000916 | valid loss: 0.000875\n","Epoch:  85 | train loss: 0.001292 | valid loss: 0.000866\n","Epoch:  86 | train loss: 0.001022 | valid loss: 0.000857\n","Epoch:  87 | train loss: 0.000787 | valid loss: 0.000848\n","Epoch:  88 | train loss: 0.000786 | valid loss: 0.000842\n","Epoch:  89 | train loss: 0.000809 | valid loss: 0.000833\n","Epoch:  90 | train loss: 0.000799 | valid loss: 0.000825\n","Epoch:  91 | train loss: 0.000903 | valid loss: 0.000817\n","Epoch:  92 | train loss: 0.000792 | valid loss: 0.000809\n","Epoch:  93 | train loss: 0.000700 | valid loss: 0.000801\n","Epoch:  94 | train loss: 0.000738 | valid loss: 0.000793\n","Epoch:  95 | train loss: 0.000938 | valid loss: 0.000789\n","Epoch:  96 | train loss: 0.000671 | valid loss: 0.000784\n","Epoch:  97 | train loss: 0.000670 | valid loss: 0.000773\n","Epoch:  98 | train loss: 0.000976 | valid loss: 0.000767\n","Epoch:  99 | train loss: 0.000881 | valid loss: 0.000760\n","Epoch:  100 | train loss: 0.000724 | valid loss: 0.000755\n","Epoch:  101 | train loss: 0.000635 | valid loss: 0.000748\n","Epoch:  102 | train loss: 0.000719 | valid loss: 0.000740\n","Epoch:  103 | train loss: 0.000933 | valid loss: 0.000736\n","Epoch:  104 | train loss: 0.000638 | valid loss: 0.000729\n","Epoch:  105 | train loss: 0.000618 | valid loss: 0.000723\n","Epoch:  106 | train loss: 0.000767 | valid loss: 0.000716\n","Epoch:  107 | train loss: 0.000685 | valid loss: 0.000711\n","Epoch:  108 | train loss: 0.000823 | valid loss: 0.000706\n","Epoch:  109 | train loss: 0.000849 | valid loss: 0.000699\n","Epoch:  110 | train loss: 0.000568 | valid loss: 0.000695\n","Epoch:  111 | train loss: 0.000820 | valid loss: 0.000687\n","Epoch:  112 | train loss: 0.000573 | valid loss: 0.000684\n","Epoch:  113 | train loss: 0.000632 | valid loss: 0.000677\n","Epoch:  114 | train loss: 0.000769 | valid loss: 0.000672\n","Epoch:  115 | train loss: 0.000746 | valid loss: 0.000666\n","Epoch:  116 | train loss: 0.000753 | valid loss: 0.000663\n","Epoch:  117 | train loss: 0.000660 | valid loss: 0.000655\n","Epoch:  118 | train loss: 0.000542 | valid loss: 0.000653\n","Epoch:  119 | train loss: 0.000472 | valid loss: 0.000646\n","Epoch:  120 | train loss: 0.000706 | valid loss: 0.000642\n","Epoch:  121 | train loss: 0.000580 | valid loss: 0.000637\n","Epoch:  122 | train loss: 0.000685 | valid loss: 0.000631\n","Epoch:  123 | train loss: 0.000611 | valid loss: 0.000627\n","Epoch:  124 | train loss: 0.000619 | valid loss: 0.000623\n","Epoch:  125 | train loss: 0.000702 | valid loss: 0.000619\n","Epoch:  126 | train loss: 0.000583 | valid loss: 0.000615\n","Epoch:  127 | train loss: 0.000635 | valid loss: 0.000610\n","Epoch:  128 | train loss: 0.000498 | valid loss: 0.000607\n","Epoch:  129 | train loss: 0.000919 | valid loss: 0.000603\n","Epoch:  130 | train loss: 0.000593 | valid loss: 0.000599\n","Epoch:  131 | train loss: 0.000813 | valid loss: 0.000596\n","Epoch:  132 | train loss: 0.000559 | valid loss: 0.000591\n","Epoch:  133 | train loss: 0.000680 | valid loss: 0.000588\n","Epoch:  134 | train loss: 0.000507 | valid loss: 0.000586\n","Epoch:  135 | train loss: 0.000549 | valid loss: 0.000581\n","Epoch:  136 | train loss: 0.000644 | valid loss: 0.000579\n","Epoch:  137 | train loss: 0.000591 | valid loss: 0.000575\n","Epoch:  138 | train loss: 0.000622 | valid loss: 0.000572\n","Epoch:  139 | train loss: 0.000525 | valid loss: 0.000568\n","Epoch:  140 | train loss: 0.000675 | valid loss: 0.000564\n","Epoch:  141 | train loss: 0.000644 | valid loss: 0.000562\n","Epoch:  142 | train loss: 0.000534 | valid loss: 0.000558\n","Epoch:  143 | train loss: 0.000578 | valid loss: 0.000555\n","Epoch:  144 | train loss: 0.000648 | valid loss: 0.000555\n","Epoch:  145 | train loss: 0.000502 | valid loss: 0.000551\n","Epoch:  146 | train loss: 0.000474 | valid loss: 0.000547\n","Epoch:  147 | train loss: 0.000599 | valid loss: 0.000546\n","Epoch:  148 | train loss: 0.000705 | valid loss: 0.000541\n","Epoch:  149 | train loss: 0.000449 | valid loss: 0.000539\n","Epoch:  150 | train loss: 0.000661 | valid loss: 0.000536\n","Epoch:  151 | train loss: 0.000625 | valid loss: 0.000534\n","Epoch:  152 | train loss: 0.000572 | valid loss: 0.000533\n","Epoch:  153 | train loss: 0.000500 | valid loss: 0.000529\n","Epoch:  154 | train loss: 0.000593 | valid loss: 0.000527\n","Epoch:  155 | train loss: 0.000600 | valid loss: 0.000524\n","Epoch:  156 | train loss: 0.000530 | valid loss: 0.000522\n","Epoch:  157 | train loss: 0.000494 | valid loss: 0.000520\n","Epoch:  158 | train loss: 0.000489 | valid loss: 0.000518\n","Epoch:  159 | train loss: 0.000485 | valid loss: 0.000516\n","Epoch:  160 | train loss: 0.000595 | valid loss: 0.000516\n","Epoch:  161 | train loss: 0.000553 | valid loss: 0.000510\n","Epoch:  162 | train loss: 0.000440 | valid loss: 0.000509\n","Epoch:  163 | train loss: 0.000673 | valid loss: 0.000506\n","Epoch:  164 | train loss: 0.000427 | valid loss: 0.000505\n","Epoch:  165 | train loss: 0.000567 | valid loss: 0.000503\n","Epoch:  166 | train loss: 0.000464 | valid loss: 0.000500\n","Epoch:  167 | train loss: 0.000450 | valid loss: 0.000499\n","Epoch:  168 | train loss: 0.000386 | valid loss: 0.000496\n","Epoch:  169 | train loss: 0.000450 | valid loss: 0.000495\n","Epoch:  170 | train loss: 0.000462 | valid loss: 0.000492\n","Epoch:  171 | train loss: 0.000567 | valid loss: 0.000492\n","Epoch:  172 | train loss: 0.000621 | valid loss: 0.000490\n","Epoch:  173 | train loss: 0.000515 | valid loss: 0.000486\n","Epoch:  174 | train loss: 0.000440 | valid loss: 0.000485\n","Epoch:  175 | train loss: 0.000561 | valid loss: 0.000484\n","Epoch:  176 | train loss: 0.000467 | valid loss: 0.000482\n","Epoch:  177 | train loss: 0.000537 | valid loss: 0.000480\n","Epoch:  178 | train loss: 0.000498 | valid loss: 0.000479\n","Epoch:  179 | train loss: 0.000617 | valid loss: 0.000478\n","Epoch:  180 | train loss: 0.000530 | valid loss: 0.000475\n","Epoch:  181 | train loss: 0.000545 | valid loss: 0.000474\n","Epoch:  182 | train loss: 0.000438 | valid loss: 0.000472\n","Epoch:  183 | train loss: 0.000449 | valid loss: 0.000471\n","Epoch:  184 | train loss: 0.000514 | valid loss: 0.000468\n","Epoch:  185 | train loss: 0.000417 | valid loss: 0.000468\n","Epoch:  186 | train loss: 0.000600 | valid loss: 0.000466\n","Epoch:  187 | train loss: 0.000497 | valid loss: 0.000463\n","Epoch:  188 | train loss: 0.000514 | valid loss: 0.000462\n","Epoch:  189 | train loss: 0.000469 | valid loss: 0.000460\n","Epoch:  190 | train loss: 0.000577 | valid loss: 0.000460\n","Epoch:  191 | train loss: 0.000503 | valid loss: 0.000459\n","Epoch:  192 | train loss: 0.000487 | valid loss: 0.000456\n","Epoch:  193 | train loss: 0.000339 | valid loss: 0.000455\n","Epoch:  194 | train loss: 0.000541 | valid loss: 0.000455\n","Epoch:  195 | train loss: 0.000507 | valid loss: 0.000452\n","Epoch:  196 | train loss: 0.000369 | valid loss: 0.000451\n","Epoch:  197 | train loss: 0.000491 | valid loss: 0.000449\n","Epoch:  198 | train loss: 0.000429 | valid loss: 0.000448\n","Epoch:  199 | train loss: 0.000400 | valid loss: 0.000447\n","Epoch:  200 | train loss: 0.000520 | valid loss: 0.000444\n","Epoch:  201 | train loss: 0.000382 | valid loss: 0.000444\n","Epoch:  202 | train loss: 0.000457 | valid loss: 0.000443\n","Epoch:  203 | train loss: 0.000438 | valid loss: 0.000441\n","Epoch:  204 | train loss: 0.000607 | valid loss: 0.000440\n","Epoch:  205 | train loss: 0.000416 | valid loss: 0.000439\n","Epoch:  206 | train loss: 0.000754 | valid loss: 0.000442\n","Epoch:  207 | train loss: 0.000413 | valid loss: 0.000436\n","Epoch:  208 | train loss: 0.000636 | valid loss: 0.000439\n","Epoch:  209 | train loss: 0.000468 | valid loss: 0.000434\n","Epoch:  210 | train loss: 0.000336 | valid loss: 0.000436\n","Epoch:  211 | train loss: 0.000437 | valid loss: 0.000434\n","Epoch:  212 | train loss: 0.000497 | valid loss: 0.000431\n","Epoch:  213 | train loss: 0.000400 | valid loss: 0.000429\n","Epoch:  214 | train loss: 0.000441 | valid loss: 0.000428\n","Epoch:  215 | train loss: 0.000424 | valid loss: 0.000427\n","Epoch:  216 | train loss: 0.000396 | valid loss: 0.000426\n","Epoch:  217 | train loss: 0.000378 | valid loss: 0.000425\n","Epoch:  218 | train loss: 0.000441 | valid loss: 0.000424\n","Epoch:  219 | train loss: 0.000366 | valid loss: 0.000423\n","Epoch:  220 | train loss: 0.000415 | valid loss: 0.000424\n","Epoch:  221 | train loss: 0.000384 | valid loss: 0.000421\n","Epoch:  222 | train loss: 0.000346 | valid loss: 0.000422\n","Epoch:  223 | train loss: 0.000391 | valid loss: 0.000419\n","Epoch:  224 | train loss: 0.000531 | valid loss: 0.000419\n","Epoch:  225 | train loss: 0.000382 | valid loss: 0.000417\n","Epoch:  226 | train loss: 0.000442 | valid loss: 0.000416\n","Epoch:  227 | train loss: 0.000529 | valid loss: 0.000416\n","Epoch:  228 | train loss: 0.000404 | valid loss: 0.000414\n","Epoch:  229 | train loss: 0.000325 | valid loss: 0.000414\n","Epoch:  230 | train loss: 0.000427 | valid loss: 0.000413\n","Epoch:  231 | train loss: 0.000363 | valid loss: 0.000411\n","Epoch:  232 | train loss: 0.000427 | valid loss: 0.000412\n","Epoch:  233 | train loss: 0.000401 | valid loss: 0.000412\n","Epoch:  234 | train loss: 0.000464 | valid loss: 0.000411\n","Epoch:  235 | train loss: 0.000502 | valid loss: 0.000409\n","Epoch:  236 | train loss: 0.000463 | valid loss: 0.000407\n","Epoch:  237 | train loss: 0.000339 | valid loss: 0.000409\n","Epoch:  238 | train loss: 0.000358 | valid loss: 0.000407\n","Epoch:  239 | train loss: 0.000362 | valid loss: 0.000404\n","Epoch:  240 | train loss: 0.000504 | valid loss: 0.000404\n","Epoch:  241 | train loss: 0.000416 | valid loss: 0.000403\n","Epoch:  242 | train loss: 0.000391 | valid loss: 0.000405\n","Epoch:  243 | train loss: 0.000433 | valid loss: 0.000401\n","Epoch:  244 | train loss: 0.000449 | valid loss: 0.000400\n","Epoch:  245 | train loss: 0.000493 | valid loss: 0.000400\n","Epoch:  246 | train loss: 0.000485 | valid loss: 0.000400\n","Epoch:  247 | train loss: 0.000528 | valid loss: 0.000398\n","Epoch:  248 | train loss: 0.000538 | valid loss: 0.000398\n","Epoch:  249 | train loss: 0.000355 | valid loss: 0.000396\n","Epoch:  250 | train loss: 0.000521 | valid loss: 0.000397\n","Epoch:  251 | train loss: 0.000448 | valid loss: 0.000395\n","Epoch:  252 | train loss: 0.000308 | valid loss: 0.000400\n","Epoch:  253 | train loss: 0.000576 | valid loss: 0.000397\n","Epoch:  254 | train loss: 0.000483 | valid loss: 0.000397\n","Epoch:  255 | train loss: 0.000425 | valid loss: 0.000392\n","Epoch:  256 | train loss: 0.000411 | valid loss: 0.000392\n","Epoch:  257 | train loss: 0.000348 | valid loss: 0.000392\n","Epoch:  258 | train loss: 0.000497 | valid loss: 0.000391\n","Epoch:  259 | train loss: 0.000392 | valid loss: 0.000390\n","Epoch:  260 | train loss: 0.000439 | valid loss: 0.000388\n","Epoch:  261 | train loss: 0.000353 | valid loss: 0.000389\n","Epoch:  262 | train loss: 0.000329 | valid loss: 0.000388\n","Epoch:  263 | train loss: 0.000316 | valid loss: 0.000386\n","Epoch:  264 | train loss: 0.000457 | valid loss: 0.000388\n","Epoch:  265 | train loss: 0.000303 | valid loss: 0.000386\n","Epoch:  266 | train loss: 0.000413 | valid loss: 0.000384\n","Epoch:  267 | train loss: 0.000338 | valid loss: 0.000386\n","Epoch:  268 | train loss: 0.000528 | valid loss: 0.000388\n","Epoch:  269 | train loss: 0.000401 | valid loss: 0.000388\n","Epoch:  270 | train loss: 0.000337 | valid loss: 0.000383\n","Epoch:  271 | train loss: 0.000318 | valid loss: 0.000380\n","Epoch:  272 | train loss: 0.000305 | valid loss: 0.000381\n","Epoch:  273 | train loss: 0.000319 | valid loss: 0.000382\n","Epoch:  274 | train loss: 0.000480 | valid loss: 0.000382\n","Epoch:  275 | train loss: 0.000426 | valid loss: 0.000382\n","Epoch:  276 | train loss: 0.000334 | valid loss: 0.000380\n","Epoch:  277 | train loss: 0.000310 | valid loss: 0.000378\n","Epoch:  278 | train loss: 0.000417 | valid loss: 0.000377\n","Epoch:  279 | train loss: 0.000367 | valid loss: 0.000378\n","Epoch:  280 | train loss: 0.000343 | valid loss: 0.000375\n","Epoch:  281 | train loss: 0.000444 | valid loss: 0.000375\n","Epoch:  282 | train loss: 0.000302 | valid loss: 0.000374\n","Epoch:  283 | train loss: 0.000411 | valid loss: 0.000375\n","Epoch:  284 | train loss: 0.000284 | valid loss: 0.000373\n","Epoch:  285 | train loss: 0.000432 | valid loss: 0.000375\n","Epoch:  286 | train loss: 0.000312 | valid loss: 0.000374\n","Epoch:  287 | train loss: 0.000470 | valid loss: 0.000372\n","Epoch:  288 | train loss: 0.000464 | valid loss: 0.000373\n","Epoch:  289 | train loss: 0.000331 | valid loss: 0.000376\n","Epoch:  290 | train loss: 0.000523 | valid loss: 0.000371\n","Epoch:  291 | train loss: 0.000401 | valid loss: 0.000370\n","Epoch:  292 | train loss: 0.000390 | valid loss: 0.000368\n","Epoch:  293 | train loss: 0.000333 | valid loss: 0.000369\n","Epoch:  294 | train loss: 0.000316 | valid loss: 0.000368\n","Epoch:  295 | train loss: 0.000312 | valid loss: 0.000367\n","Epoch:  296 | train loss: 0.000380 | valid loss: 0.000366\n","Epoch:  297 | train loss: 0.000295 | valid loss: 0.000367\n","Epoch:  298 | train loss: 0.000415 | valid loss: 0.000366\n","Epoch:  299 | train loss: 0.000332 | valid loss: 0.000366\n","Epoch:  300 | train loss: 0.000382 | valid loss: 0.000365\n","Epoch:  301 | train loss: 0.000404 | valid loss: 0.000365\n","Epoch:  302 | train loss: 0.000343 | valid loss: 0.000371\n","Epoch:  303 | train loss: 0.000357 | valid loss: 0.000368\n","Epoch:  304 | train loss: 0.000423 | valid loss: 0.000363\n","Epoch:  305 | train loss: 0.000392 | valid loss: 0.000362\n","Epoch:  306 | train loss: 0.000295 | valid loss: 0.000362\n","Epoch:  307 | train loss: 0.000321 | valid loss: 0.000361\n","Epoch:  308 | train loss: 0.000405 | valid loss: 0.000361\n","Epoch:  309 | train loss: 0.000309 | valid loss: 0.000361\n","Epoch:  310 | train loss: 0.000385 | valid loss: 0.000361\n","Epoch:  311 | train loss: 0.000387 | valid loss: 0.000364\n","Epoch:  312 | train loss: 0.000471 | valid loss: 0.000360\n","Epoch:  313 | train loss: 0.000423 | valid loss: 0.000359\n","Epoch:  314 | train loss: 0.000410 | valid loss: 0.000359\n","Epoch:  315 | train loss: 0.000281 | valid loss: 0.000361\n","Epoch:  316 | train loss: 0.000343 | valid loss: 0.000358\n","Epoch:  317 | train loss: 0.000386 | valid loss: 0.000359\n","Epoch:  318 | train loss: 0.000306 | valid loss: 0.000356\n","Epoch:  319 | train loss: 0.000379 | valid loss: 0.000356\n","Epoch:  320 | train loss: 0.000396 | valid loss: 0.000356\n","Epoch:  321 | train loss: 0.000399 | valid loss: 0.000358\n","Epoch:  322 | train loss: 0.000381 | valid loss: 0.000356\n","Epoch:  323 | train loss: 0.000467 | valid loss: 0.000355\n","Epoch:  324 | train loss: 0.000422 | valid loss: 0.000353\n","Epoch:  325 | train loss: 0.000479 | valid loss: 0.000355\n","Epoch:  326 | train loss: 0.000273 | valid loss: 0.000355\n","Epoch:  327 | train loss: 0.000371 | valid loss: 0.000352\n","Epoch:  328 | train loss: 0.000419 | valid loss: 0.000352\n","Epoch:  329 | train loss: 0.000331 | valid loss: 0.000353\n","Epoch:  330 | train loss: 0.000378 | valid loss: 0.000353\n","Epoch:  331 | train loss: 0.000462 | valid loss: 0.000351\n","Epoch:  332 | train loss: 0.000320 | valid loss: 0.000350\n","Epoch:  333 | train loss: 0.000333 | valid loss: 0.000350\n","Epoch:  334 | train loss: 0.000324 | valid loss: 0.000349\n","Epoch:  335 | train loss: 0.000342 | valid loss: 0.000349\n","Epoch:  336 | train loss: 0.000431 | valid loss: 0.000351\n","Epoch:  337 | train loss: 0.000349 | valid loss: 0.000348\n","Epoch:  338 | train loss: 0.000412 | valid loss: 0.000354\n","Epoch:  339 | train loss: 0.000291 | valid loss: 0.000347\n","Epoch:  340 | train loss: 0.000267 | valid loss: 0.000348\n","Epoch:  341 | train loss: 0.000393 | valid loss: 0.000347\n","Epoch:  342 | train loss: 0.000441 | valid loss: 0.000347\n","Epoch:  343 | train loss: 0.000369 | valid loss: 0.000348\n","Epoch:  344 | train loss: 0.000312 | valid loss: 0.000349\n","Epoch:  345 | train loss: 0.000309 | valid loss: 0.000345\n","Epoch:  346 | train loss: 0.000326 | valid loss: 0.000346\n","Epoch:  347 | train loss: 0.000322 | valid loss: 0.000347\n","Epoch:  348 | train loss: 0.000485 | valid loss: 0.000344\n","Epoch:  349 | train loss: 0.000338 | valid loss: 0.000343\n","Epoch:  350 | train loss: 0.000385 | valid loss: 0.000343\n","Epoch:  351 | train loss: 0.000356 | valid loss: 0.000344\n","Epoch:  352 | train loss: 0.000396 | valid loss: 0.000343\n","Epoch:  353 | train loss: 0.000401 | valid loss: 0.000344\n","Epoch:  354 | train loss: 0.000298 | valid loss: 0.000346\n","Epoch:  355 | train loss: 0.000290 | valid loss: 0.000342\n","Epoch:  356 | train loss: 0.000279 | valid loss: 0.000341\n","Epoch:  357 | train loss: 0.000303 | valid loss: 0.000344\n","Epoch:  358 | train loss: 0.000373 | valid loss: 0.000341\n","Epoch:  359 | train loss: 0.000396 | valid loss: 0.000340\n","Epoch:  360 | train loss: 0.000321 | valid loss: 0.000340\n","Epoch:  361 | train loss: 0.000485 | valid loss: 0.000340\n","Epoch:  362 | train loss: 0.000295 | valid loss: 0.000340\n","Epoch:  363 | train loss: 0.000290 | valid loss: 0.000341\n","Epoch:  364 | train loss: 0.000325 | valid loss: 0.000339\n","Epoch:  365 | train loss: 0.000331 | valid loss: 0.000343\n","Epoch:  366 | train loss: 0.000323 | valid loss: 0.000340\n","Epoch:  367 | train loss: 0.000380 | valid loss: 0.000338\n","Epoch:  368 | train loss: 0.000306 | valid loss: 0.000338\n","Epoch:  369 | train loss: 0.000306 | valid loss: 0.000342\n","Epoch:  370 | train loss: 0.000314 | valid loss: 0.000336\n","Epoch:  371 | train loss: 0.000388 | valid loss: 0.000335\n","Epoch:  372 | train loss: 0.000429 | valid loss: 0.000336\n","Epoch:  373 | train loss: 0.000343 | valid loss: 0.000341\n","Epoch:  374 | train loss: 0.000322 | valid loss: 0.000335\n","Epoch:  375 | train loss: 0.000363 | valid loss: 0.000334\n","Epoch:  376 | train loss: 0.000327 | valid loss: 0.000335\n","Epoch:  377 | train loss: 0.000289 | valid loss: 0.000334\n","Epoch:  378 | train loss: 0.000283 | valid loss: 0.000335\n","Epoch:  379 | train loss: 0.000336 | valid loss: 0.000334\n","Epoch:  380 | train loss: 0.000315 | valid loss: 0.000334\n","Epoch:  381 | train loss: 0.000423 | valid loss: 0.000335\n","Epoch:  382 | train loss: 0.000296 | valid loss: 0.000336\n","Epoch:  383 | train loss: 0.000404 | valid loss: 0.000335\n","Epoch:  384 | train loss: 0.000281 | valid loss: 0.000332\n","Epoch:  385 | train loss: 0.000413 | valid loss: 0.000335\n","Epoch:  386 | train loss: 0.000385 | valid loss: 0.000336\n","Epoch:  387 | train loss: 0.000304 | valid loss: 0.000331\n","Epoch:  388 | train loss: 0.000318 | valid loss: 0.000334\n","Epoch:  389 | train loss: 0.000416 | valid loss: 0.000330\n","Epoch:  390 | train loss: 0.000379 | valid loss: 0.000331\n","Epoch:  391 | train loss: 0.000300 | valid loss: 0.000332\n","Epoch:  392 | train loss: 0.000452 | valid loss: 0.000329\n","Epoch:  393 | train loss: 0.000347 | valid loss: 0.000332\n","Epoch:  394 | train loss: 0.000340 | valid loss: 0.000330\n","Epoch:  395 | train loss: 0.000405 | valid loss: 0.000330\n","Epoch:  396 | train loss: 0.000345 | valid loss: 0.000330\n","Epoch:  397 | train loss: 0.000393 | valid loss: 0.000329\n","Epoch:  398 | train loss: 0.000452 | valid loss: 0.000328\n","Epoch:  399 | train loss: 0.000337 | valid loss: 0.000327\n","Epoch:  400 | train loss: 0.000357 | valid loss: 0.000329\n","Epoch:  401 | train loss: 0.000279 | valid loss: 0.000330\n","Epoch:  402 | train loss: 0.000293 | valid loss: 0.000327\n","Epoch:  403 | train loss: 0.000354 | valid loss: 0.000328\n","Epoch:  404 | train loss: 0.000346 | valid loss: 0.000329\n","Epoch:  405 | train loss: 0.000402 | valid loss: 0.000327\n","Epoch:  406 | train loss: 0.000346 | valid loss: 0.000326\n","Epoch:  407 | train loss: 0.000261 | valid loss: 0.000325\n","Epoch:  408 | train loss: 0.000312 | valid loss: 0.000325\n","Epoch:  409 | train loss: 0.000423 | valid loss: 0.000326\n","Epoch:  410 | train loss: 0.000579 | valid loss: 0.000333\n","Epoch:  411 | train loss: 0.000348 | valid loss: 0.000325\n","Epoch:  412 | train loss: 0.000274 | valid loss: 0.000325\n","Epoch:  413 | train loss: 0.000458 | valid loss: 0.000324\n","Epoch:  414 | train loss: 0.000307 | valid loss: 0.000324\n","Epoch:  415 | train loss: 0.000332 | valid loss: 0.000324\n","Epoch:  416 | train loss: 0.000451 | valid loss: 0.000323\n","Epoch:  417 | train loss: 0.000338 | valid loss: 0.000323\n","Epoch:  418 | train loss: 0.000305 | valid loss: 0.000323\n","Epoch:  419 | train loss: 0.000243 | valid loss: 0.000323\n","Epoch:  420 | train loss: 0.000317 | valid loss: 0.000324\n","Epoch:  421 | train loss: 0.000381 | valid loss: 0.000323\n","Epoch:  422 | train loss: 0.000413 | valid loss: 0.000323\n","Epoch:  423 | train loss: 0.000461 | valid loss: 0.000325\n","Epoch:  424 | train loss: 0.000265 | valid loss: 0.000324\n","Epoch:  425 | train loss: 0.000287 | valid loss: 0.000321\n","Epoch:  426 | train loss: 0.000414 | valid loss: 0.000322\n","Epoch:  427 | train loss: 0.000372 | valid loss: 0.000321\n","Epoch:  428 | train loss: 0.000278 | valid loss: 0.000324\n","Epoch:  429 | train loss: 0.000272 | valid loss: 0.000325\n","Epoch:  430 | train loss: 0.000286 | valid loss: 0.000325\n","Epoch:  431 | train loss: 0.000354 | valid loss: 0.000319\n","Epoch:  432 | train loss: 0.000305 | valid loss: 0.000320\n","Epoch:  433 | train loss: 0.000248 | valid loss: 0.000320\n","Epoch:  434 | train loss: 0.000346 | valid loss: 0.000318\n","Epoch:  435 | train loss: 0.000417 | valid loss: 0.000321\n","Epoch:  436 | train loss: 0.000292 | valid loss: 0.000318\n","Epoch:  437 | train loss: 0.000307 | valid loss: 0.000318\n","Epoch:  438 | train loss: 0.000333 | valid loss: 0.000325\n","Epoch:  439 | train loss: 0.000533 | valid loss: 0.000322\n","Epoch:  440 | train loss: 0.000309 | valid loss: 0.000318\n","Epoch:  441 | train loss: 0.000350 | valid loss: 0.000317\n","Epoch:  442 | train loss: 0.000370 | valid loss: 0.000319\n","Epoch:  443 | train loss: 0.000280 | valid loss: 0.000320\n","Epoch:  444 | train loss: 0.000298 | valid loss: 0.000318\n","Epoch:  445 | train loss: 0.000414 | valid loss: 0.000315\n","Epoch:  446 | train loss: 0.000278 | valid loss: 0.000317\n","Epoch:  447 | train loss: 0.000261 | valid loss: 0.000315\n","Epoch:  448 | train loss: 0.000313 | valid loss: 0.000316\n","Epoch:  449 | train loss: 0.000250 | valid loss: 0.000317\n","Epoch:  450 | train loss: 0.000264 | valid loss: 0.000319\n","Epoch:  451 | train loss: 0.000358 | valid loss: 0.000315\n","Epoch:  452 | train loss: 0.000366 | valid loss: 0.000315\n","Epoch:  453 | train loss: 0.000356 | valid loss: 0.000315\n","Epoch:  454 | train loss: 0.000342 | valid loss: 0.000315\n","Epoch:  455 | train loss: 0.000297 | valid loss: 0.000313\n","Epoch:  456 | train loss: 0.000287 | valid loss: 0.000313\n","Epoch:  457 | train loss: 0.000326 | valid loss: 0.000314\n","Epoch:  458 | train loss: 0.000318 | valid loss: 0.000315\n","Epoch:  459 | train loss: 0.000382 | valid loss: 0.000314\n","Epoch:  460 | train loss: 0.000271 | valid loss: 0.000314\n","Epoch:  461 | train loss: 0.000354 | valid loss: 0.000316\n","Epoch:  462 | train loss: 0.000302 | valid loss: 0.000313\n","Epoch:  463 | train loss: 0.000281 | valid loss: 0.000313\n","Epoch:  464 | train loss: 0.000268 | valid loss: 0.000312\n","Epoch:  465 | train loss: 0.000566 | valid loss: 0.000319\n","Epoch:  466 | train loss: 0.000244 | valid loss: 0.000313\n","Epoch:  467 | train loss: 0.000421 | valid loss: 0.000312\n","Epoch:  468 | train loss: 0.000315 | valid loss: 0.000313\n","Epoch:  469 | train loss: 0.000332 | valid loss: 0.000314\n","Epoch:  470 | train loss: 0.000304 | valid loss: 0.000312\n","Epoch:  471 | train loss: 0.000269 | valid loss: 0.000312\n","Epoch:  472 | train loss: 0.000320 | valid loss: 0.000313\n","Epoch:  473 | train loss: 0.000329 | valid loss: 0.000311\n","Epoch:  474 | train loss: 0.000323 | valid loss: 0.000313\n","Epoch:  475 | train loss: 0.000309 | valid loss: 0.000314\n","Epoch:  476 | train loss: 0.000248 | valid loss: 0.000311\n","Epoch:  477 | train loss: 0.000322 | valid loss: 0.000311\n","Epoch:  478 | train loss: 0.000269 | valid loss: 0.000311\n","Epoch:  479 | train loss: 0.000361 | valid loss: 0.000308\n","Epoch:  480 | train loss: 0.000293 | valid loss: 0.000312\n","Epoch:  481 | train loss: 0.000398 | valid loss: 0.000310\n","Epoch:  482 | train loss: 0.000323 | valid loss: 0.000310\n","Epoch:  483 | train loss: 0.000386 | valid loss: 0.000309\n","Epoch:  484 | train loss: 0.000279 | valid loss: 0.000314\n","Epoch:  485 | train loss: 0.000253 | valid loss: 0.000309\n","Epoch:  486 | train loss: 0.000443 | valid loss: 0.000309\n","Epoch:  487 | train loss: 0.000276 | valid loss: 0.000309\n","Epoch:  488 | train loss: 0.000309 | valid loss: 0.000308\n","Epoch:  489 | train loss: 0.000314 | valid loss: 0.000309\n","Epoch:  490 | train loss: 0.000239 | valid loss: 0.000310\n","Epoch:  491 | train loss: 0.000309 | valid loss: 0.000308\n","Epoch:  492 | train loss: 0.000367 | valid loss: 0.000309\n","Epoch:  493 | train loss: 0.000447 | valid loss: 0.000308\n","Epoch:  494 | train loss: 0.000401 | valid loss: 0.000307\n","Epoch:  495 | train loss: 0.000237 | valid loss: 0.000310\n","Epoch:  496 | train loss: 0.000346 | valid loss: 0.000308\n","Epoch:  497 | train loss: 0.000379 | valid loss: 0.000309\n","Epoch:  498 | train loss: 0.000311 | valid loss: 0.000308\n","Epoch:  499 | train loss: 0.000298 | valid loss: 0.000308\n","Epoch:  500 | train loss: 0.000355 | valid loss: 0.000307\n","Epoch:  501 | train loss: 0.000273 | valid loss: 0.000306\n","Epoch:  502 | train loss: 0.000237 | valid loss: 0.000307\n","Epoch:  503 | train loss: 0.000294 | valid loss: 0.000305\n","Epoch:  504 | train loss: 0.000287 | valid loss: 0.000305\n","Epoch:  505 | train loss: 0.000381 | valid loss: 0.000305\n","Epoch:  506 | train loss: 0.000284 | valid loss: 0.000305\n","Epoch:  507 | train loss: 0.000245 | valid loss: 0.000304\n","Epoch:  508 | train loss: 0.000314 | valid loss: 0.000305\n","Epoch:  509 | train loss: 0.000293 | valid loss: 0.000307\n","Epoch:  510 | train loss: 0.000491 | valid loss: 0.000305\n","Epoch:  511 | train loss: 0.000284 | valid loss: 0.000306\n","Epoch:  512 | train loss: 0.000289 | valid loss: 0.000310\n","Epoch:  513 | train loss: 0.000276 | valid loss: 0.000304\n","Epoch:  514 | train loss: 0.000310 | valid loss: 0.000308\n","Epoch:  515 | train loss: 0.000331 | valid loss: 0.000304\n","Epoch:  516 | train loss: 0.000362 | valid loss: 0.000304\n","Epoch:  517 | train loss: 0.000301 | valid loss: 0.000302\n","Epoch:  518 | train loss: 0.000311 | valid loss: 0.000302\n","Epoch:  519 | train loss: 0.000342 | valid loss: 0.000303\n","Epoch:  520 | train loss: 0.000319 | valid loss: 0.000305\n","Epoch:  521 | train loss: 0.000248 | valid loss: 0.000302\n","Epoch:  522 | train loss: 0.000277 | valid loss: 0.000302\n","Epoch:  523 | train loss: 0.000345 | valid loss: 0.000304\n","Epoch:  524 | train loss: 0.000308 | valid loss: 0.000302\n","Epoch:  525 | train loss: 0.000355 | valid loss: 0.000305\n","Epoch:  526 | train loss: 0.000324 | valid loss: 0.000305\n","Epoch:  527 | train loss: 0.000276 | valid loss: 0.000303\n","Epoch:  528 | train loss: 0.000241 | valid loss: 0.000302\n","Epoch:  529 | train loss: 0.000276 | valid loss: 0.000301\n","Epoch:  530 | train loss: 0.000225 | valid loss: 0.000301\n","Epoch:  531 | train loss: 0.000272 | valid loss: 0.000301\n","Epoch:  532 | train loss: 0.000375 | valid loss: 0.000304\n","Epoch:  533 | train loss: 0.000379 | valid loss: 0.000306\n","Epoch:  534 | train loss: 0.000324 | valid loss: 0.000301\n","Epoch:  535 | train loss: 0.000334 | valid loss: 0.000301\n","Epoch:  536 | train loss: 0.000288 | valid loss: 0.000308\n","Epoch:  537 | train loss: 0.000308 | valid loss: 0.000300\n","Epoch:  538 | train loss: 0.000262 | valid loss: 0.000300\n","Epoch:  539 | train loss: 0.000290 | valid loss: 0.000299\n","Epoch:  540 | train loss: 0.000294 | valid loss: 0.000298\n","Epoch:  541 | train loss: 0.000249 | valid loss: 0.000300\n","Epoch:  542 | train loss: 0.000343 | valid loss: 0.000300\n","Epoch:  543 | train loss: 0.000256 | valid loss: 0.000301\n","Epoch:  544 | train loss: 0.000319 | valid loss: 0.000300\n","Epoch:  545 | train loss: 0.000399 | valid loss: 0.000300\n","Epoch:  546 | train loss: 0.000261 | valid loss: 0.000298\n","Epoch:  547 | train loss: 0.000223 | valid loss: 0.000301\n","Epoch:  548 | train loss: 0.000345 | valid loss: 0.000298\n","Epoch:  549 | train loss: 0.000242 | valid loss: 0.000299\n","Epoch:  550 | train loss: 0.000254 | valid loss: 0.000298\n","Epoch:  551 | train loss: 0.000302 | valid loss: 0.000297\n","Epoch:  552 | train loss: 0.000273 | valid loss: 0.000304\n","Epoch:  553 | train loss: 0.000325 | valid loss: 0.000300\n","Epoch:  554 | train loss: 0.000331 | valid loss: 0.000296\n","Epoch:  555 | train loss: 0.000359 | valid loss: 0.000304\n","Epoch:  556 | train loss: 0.000323 | valid loss: 0.000297\n","Epoch:  557 | train loss: 0.000260 | valid loss: 0.000298\n","Epoch:  558 | train loss: 0.000258 | valid loss: 0.000299\n","Epoch:  559 | train loss: 0.000282 | valid loss: 0.000296\n","Epoch:  560 | train loss: 0.000316 | valid loss: 0.000299\n","Epoch:  561 | train loss: 0.000240 | valid loss: 0.000299\n","Epoch:  562 | train loss: 0.000411 | valid loss: 0.000296\n","Epoch:  563 | train loss: 0.000269 | valid loss: 0.000297\n","Epoch:  564 | train loss: 0.000358 | valid loss: 0.000297\n","Epoch:  565 | train loss: 0.000297 | valid loss: 0.000297\n","Epoch:  566 | train loss: 0.000287 | valid loss: 0.000296\n","Epoch:  567 | train loss: 0.000288 | valid loss: 0.000297\n","Epoch:  568 | train loss: 0.000347 | valid loss: 0.000295\n","Epoch:  569 | train loss: 0.000384 | valid loss: 0.000301\n","Epoch:  570 | train loss: 0.000319 | valid loss: 0.000297\n","Epoch:  571 | train loss: 0.000276 | valid loss: 0.000297\n","Epoch:  572 | train loss: 0.000340 | valid loss: 0.000295\n","Epoch:  573 | train loss: 0.000283 | valid loss: 0.000296\n","Epoch:  574 | train loss: 0.000308 | valid loss: 0.000295\n","Epoch:  575 | train loss: 0.000278 | valid loss: 0.000296\n","Epoch:  576 | train loss: 0.000302 | valid loss: 0.000295\n","Epoch:  577 | train loss: 0.000326 | valid loss: 0.000295\n","Epoch:  578 | train loss: 0.000288 | valid loss: 0.000294\n","Epoch:  579 | train loss: 0.000294 | valid loss: 0.000297\n","Epoch:  580 | train loss: 0.000242 | valid loss: 0.000294\n","Epoch:  581 | train loss: 0.000246 | valid loss: 0.000295\n","Epoch:  582 | train loss: 0.000272 | valid loss: 0.000296\n","Epoch:  583 | train loss: 0.000300 | valid loss: 0.000295\n","Epoch:  584 | train loss: 0.000236 | valid loss: 0.000293\n","Epoch:  585 | train loss: 0.000221 | valid loss: 0.000294\n","Epoch:  586 | train loss: 0.000279 | valid loss: 0.000295\n","Epoch:  587 | train loss: 0.000302 | valid loss: 0.000293\n","Epoch:  588 | train loss: 0.000286 | valid loss: 0.000295\n","Epoch:  589 | train loss: 0.000292 | valid loss: 0.000294\n","Epoch:  590 | train loss: 0.000305 | valid loss: 0.000293\n","Epoch:  591 | train loss: 0.000234 | valid loss: 0.000293\n","Epoch:  592 | train loss: 0.000317 | valid loss: 0.000293\n","Epoch:  593 | train loss: 0.000276 | valid loss: 0.000294\n","Epoch:  594 | train loss: 0.000276 | valid loss: 0.000293\n","Epoch:  595 | train loss: 0.000277 | valid loss: 0.000293\n","Epoch:  596 | train loss: 0.000260 | valid loss: 0.000293\n","Epoch:  597 | train loss: 0.000291 | valid loss: 0.000293\n","Epoch:  598 | train loss: 0.000291 | valid loss: 0.000293\n","Epoch:  599 | train loss: 0.000327 | valid loss: 0.000292\n","Epoch:  600 | train loss: 0.000291 | valid loss: 0.000292\n","Epoch:  601 | train loss: 0.000311 | valid loss: 0.000292\n","Epoch:  602 | train loss: 0.000350 | valid loss: 0.000298\n","Epoch:  603 | train loss: 0.000338 | valid loss: 0.000295\n","Epoch:  604 | train loss: 0.000310 | valid loss: 0.000291\n","Epoch:  605 | train loss: 0.000263 | valid loss: 0.000291\n","Epoch:  606 | train loss: 0.000325 | valid loss: 0.000291\n","Epoch:  607 | train loss: 0.000284 | valid loss: 0.000290\n","Epoch:  608 | train loss: 0.000259 | valid loss: 0.000293\n","Epoch:  609 | train loss: 0.000315 | valid loss: 0.000290\n","Epoch:  610 | train loss: 0.000308 | valid loss: 0.000292\n","Epoch:  611 | train loss: 0.000323 | valid loss: 0.000292\n","Epoch:  612 | train loss: 0.000305 | valid loss: 0.000293\n","Epoch:  613 | train loss: 0.000359 | valid loss: 0.000292\n","Epoch:  614 | train loss: 0.000260 | valid loss: 0.000292\n","Epoch:  615 | train loss: 0.000230 | valid loss: 0.000291\n","Epoch:  616 | train loss: 0.000338 | valid loss: 0.000291\n","Epoch:  617 | train loss: 0.000245 | valid loss: 0.000289\n","Epoch:  618 | train loss: 0.000282 | valid loss: 0.000290\n","Epoch:  619 | train loss: 0.000299 | valid loss: 0.000289\n","Epoch:  620 | train loss: 0.000312 | valid loss: 0.000289\n","Epoch:  621 | train loss: 0.000300 | valid loss: 0.000290\n","Epoch:  622 | train loss: 0.000339 | valid loss: 0.000290\n","Epoch:  623 | train loss: 0.000323 | valid loss: 0.000290\n","Epoch:  624 | train loss: 0.000312 | valid loss: 0.000294\n","Epoch:  625 | train loss: 0.000269 | valid loss: 0.000290\n","Epoch:  626 | train loss: 0.000272 | valid loss: 0.000290\n","Epoch:  627 | train loss: 0.000320 | valid loss: 0.000290\n","Epoch:  628 | train loss: 0.000244 | valid loss: 0.000289\n","Epoch:  629 | train loss: 0.000316 | valid loss: 0.000288\n","Epoch:  630 | train loss: 0.000250 | valid loss: 0.000289\n","Epoch:  631 | train loss: 0.000222 | valid loss: 0.000288\n","Epoch:  632 | train loss: 0.000257 | valid loss: 0.000288\n","Epoch:  633 | train loss: 0.000290 | valid loss: 0.000288\n","Epoch:  634 | train loss: 0.000269 | valid loss: 0.000290\n","Epoch:  635 | train loss: 0.000314 | valid loss: 0.000290\n","Epoch:  636 | train loss: 0.000347 | valid loss: 0.000288\n","Epoch:  637 | train loss: 0.000237 | valid loss: 0.000289\n","Epoch:  638 | train loss: 0.000298 | valid loss: 0.000289\n","Epoch:  639 | train loss: 0.000371 | valid loss: 0.000289\n","Epoch:  640 | train loss: 0.000282 | valid loss: 0.000287\n","Epoch:  641 | train loss: 0.000352 | valid loss: 0.000287\n","Epoch:  642 | train loss: 0.000298 | valid loss: 0.000287\n","Epoch:  643 | train loss: 0.000229 | valid loss: 0.000286\n","Epoch:  644 | train loss: 0.000227 | valid loss: 0.000287\n","Epoch:  645 | train loss: 0.000228 | valid loss: 0.000290\n","Epoch:  646 | train loss: 0.000258 | valid loss: 0.000289\n","Epoch:  647 | train loss: 0.000293 | valid loss: 0.000287\n","Epoch:  648 | train loss: 0.000280 | valid loss: 0.000286\n","Epoch:  649 | train loss: 0.000289 | valid loss: 0.000285\n","Epoch:  650 | train loss: 0.000344 | valid loss: 0.000286\n","Epoch:  651 | train loss: 0.000364 | valid loss: 0.000287\n","Epoch:  652 | train loss: 0.000267 | valid loss: 0.000288\n","Epoch:  653 | train loss: 0.000272 | valid loss: 0.000288\n","Epoch:  654 | train loss: 0.000454 | valid loss: 0.000293\n","Epoch:  655 | train loss: 0.000228 | valid loss: 0.000287\n","Epoch:  656 | train loss: 0.000328 | valid loss: 0.000285\n","Epoch:  657 | train loss: 0.000266 | valid loss: 0.000287\n","Epoch:  658 | train loss: 0.000261 | valid loss: 0.000288\n","Epoch:  659 | train loss: 0.000351 | valid loss: 0.000285\n","Epoch:  660 | train loss: 0.000337 | valid loss: 0.000287\n","Epoch:  661 | train loss: 0.000278 | valid loss: 0.000285\n","Epoch:  662 | train loss: 0.000320 | valid loss: 0.000284\n","Epoch:  663 | train loss: 0.000290 | valid loss: 0.000286\n","Epoch:  664 | train loss: 0.000310 | valid loss: 0.000284\n","Epoch:  665 | train loss: 0.000349 | valid loss: 0.000284\n","Epoch:  666 | train loss: 0.000299 | valid loss: 0.000286\n","Epoch:  667 | train loss: 0.000227 | valid loss: 0.000285\n","Epoch:  668 | train loss: 0.000287 | valid loss: 0.000284\n","Epoch:  669 | train loss: 0.000350 | valid loss: 0.000285\n","Epoch:  670 | train loss: 0.000233 | valid loss: 0.000285\n","Epoch:  671 | train loss: 0.000281 | valid loss: 0.000285\n","Epoch:  672 | train loss: 0.000267 | valid loss: 0.000285\n","Epoch:  673 | train loss: 0.000347 | valid loss: 0.000287\n","Epoch:  674 | train loss: 0.000342 | valid loss: 0.000285\n","Epoch:  675 | train loss: 0.000248 | valid loss: 0.000283\n","Epoch:  676 | train loss: 0.000278 | valid loss: 0.000282\n","Epoch:  677 | train loss: 0.000267 | valid loss: 0.000283\n","Epoch:  678 | train loss: 0.000322 | valid loss: 0.000283\n","Epoch:  679 | train loss: 0.000282 | valid loss: 0.000283\n","Epoch:  680 | train loss: 0.000296 | valid loss: 0.000284\n","Epoch:  681 | train loss: 0.000374 | valid loss: 0.000283\n","Epoch:  682 | train loss: 0.000269 | valid loss: 0.000284\n","Epoch:  683 | train loss: 0.000309 | valid loss: 0.000284\n","Epoch:  684 | train loss: 0.000302 | valid loss: 0.000284\n","Epoch:  685 | train loss: 0.000354 | valid loss: 0.000282\n","Epoch:  686 | train loss: 0.000276 | valid loss: 0.000291\n","Epoch:  687 | train loss: 0.000331 | valid loss: 0.000282\n","Epoch:  688 | train loss: 0.000290 | valid loss: 0.000283\n","Epoch:  689 | train loss: 0.000320 | valid loss: 0.000283\n","Epoch:  690 | train loss: 0.000262 | valid loss: 0.000282\n","Epoch:  691 | train loss: 0.000330 | valid loss: 0.000283\n","Epoch:  692 | train loss: 0.000281 | valid loss: 0.000282\n","Epoch:  693 | train loss: 0.000231 | valid loss: 0.000281\n","Epoch:  694 | train loss: 0.000305 | valid loss: 0.000284\n","Epoch:  695 | train loss: 0.000294 | valid loss: 0.000283\n","Epoch:  696 | train loss: 0.000363 | valid loss: 0.000284\n","Epoch:  697 | train loss: 0.000328 | valid loss: 0.000282\n","Epoch:  698 | train loss: 0.000296 | valid loss: 0.000281\n","Epoch:  699 | train loss: 0.000232 | valid loss: 0.000282\n","Epoch:  700 | train loss: 0.000241 | valid loss: 0.000283\n","Epoch:  701 | train loss: 0.000299 | valid loss: 0.000281\n","Epoch:  702 | train loss: 0.000237 | valid loss: 0.000281\n","Epoch:  703 | train loss: 0.000315 | valid loss: 0.000282\n","Epoch:  704 | train loss: 0.000328 | valid loss: 0.000283\n","Epoch:  705 | train loss: 0.000289 | valid loss: 0.000281\n","Epoch:  706 | train loss: 0.000301 | valid loss: 0.000282\n","Epoch:  707 | train loss: 0.000286 | valid loss: 0.000280\n","Epoch:  708 | train loss: 0.000250 | valid loss: 0.000281\n","Epoch:  709 | train loss: 0.000247 | valid loss: 0.000281\n","Epoch:  710 | train loss: 0.000347 | valid loss: 0.000282\n","Epoch:  711 | train loss: 0.000272 | valid loss: 0.000283\n","Epoch:  712 | train loss: 0.000261 | valid loss: 0.000282\n","Epoch:  713 | train loss: 0.000316 | valid loss: 0.000280\n","Epoch:  714 | train loss: 0.000561 | valid loss: 0.000284\n","Epoch:  715 | train loss: 0.000276 | valid loss: 0.000280\n","Epoch:  716 | train loss: 0.000251 | valid loss: 0.000281\n","Epoch:  717 | train loss: 0.000231 | valid loss: 0.000281\n","Epoch:  718 | train loss: 0.000306 | valid loss: 0.000281\n","Epoch:  719 | train loss: 0.000264 | valid loss: 0.000279\n","Epoch:  720 | train loss: 0.000276 | valid loss: 0.000279\n","Epoch:  721 | train loss: 0.000327 | valid loss: 0.000284\n","Epoch:  722 | train loss: 0.000327 | valid loss: 0.000279\n","Epoch:  723 | train loss: 0.000346 | valid loss: 0.000280\n","Epoch:  724 | train loss: 0.000359 | valid loss: 0.000281\n","Epoch:  725 | train loss: 0.000252 | valid loss: 0.000279\n","Epoch:  726 | train loss: 0.000226 | valid loss: 0.000279\n","Epoch:  727 | train loss: 0.000307 | valid loss: 0.000279\n","Epoch:  728 | train loss: 0.000306 | valid loss: 0.000281\n","Epoch:  729 | train loss: 0.000355 | valid loss: 0.000282\n","Epoch:  730 | train loss: 0.000305 | valid loss: 0.000279\n","Epoch:  731 | train loss: 0.000333 | valid loss: 0.000279\n","Epoch:  732 | train loss: 0.000361 | valid loss: 0.000279\n","Epoch:  733 | train loss: 0.000308 | valid loss: 0.000279\n","Epoch:  734 | train loss: 0.000319 | valid loss: 0.000280\n","Epoch:  735 | train loss: 0.000292 | valid loss: 0.000281\n","Epoch:  736 | train loss: 0.000262 | valid loss: 0.000280\n","Epoch:  737 | train loss: 0.000270 | valid loss: 0.000279\n","Epoch:  738 | train loss: 0.000304 | valid loss: 0.000277\n","Epoch:  739 | train loss: 0.000335 | valid loss: 0.000278\n","Epoch:  740 | train loss: 0.000273 | valid loss: 0.000279\n","Epoch:  741 | train loss: 0.000261 | valid loss: 0.000279\n","Epoch:  742 | train loss: 0.000261 | valid loss: 0.000277\n","Epoch:  743 | train loss: 0.000230 | valid loss: 0.000278\n","Epoch:  744 | train loss: 0.000255 | valid loss: 0.000278\n","Epoch:  745 | train loss: 0.000229 | valid loss: 0.000278\n","Epoch:  746 | train loss: 0.000268 | valid loss: 0.000281\n","Epoch:  747 | train loss: 0.000300 | valid loss: 0.000278\n","Epoch:  748 | train loss: 0.000288 | valid loss: 0.000278\n","Epoch:  749 | train loss: 0.000258 | valid loss: 0.000278\n","Epoch:  750 | train loss: 0.000300 | valid loss: 0.000281\n","Epoch:  751 | train loss: 0.000286 | valid loss: 0.000277\n","Epoch:  752 | train loss: 0.000317 | valid loss: 0.000276\n","Epoch:  753 | train loss: 0.000300 | valid loss: 0.000277\n","Epoch:  754 | train loss: 0.000306 | valid loss: 0.000278\n","Epoch:  755 | train loss: 0.000276 | valid loss: 0.000278\n","Epoch:  756 | train loss: 0.000346 | valid loss: 0.000278\n","Epoch:  757 | train loss: 0.000276 | valid loss: 0.000277\n","Epoch:  758 | train loss: 0.000345 | valid loss: 0.000277\n","Epoch:  759 | train loss: 0.000351 | valid loss: 0.000277\n","Epoch:  760 | train loss: 0.000284 | valid loss: 0.000277\n","Epoch:  761 | train loss: 0.000241 | valid loss: 0.000276\n","Epoch:  762 | train loss: 0.000296 | valid loss: 0.000277\n","Epoch:  763 | train loss: 0.000314 | valid loss: 0.000276\n","Epoch:  764 | train loss: 0.000302 | valid loss: 0.000278\n","Epoch:  765 | train loss: 0.000264 | valid loss: 0.000276\n","Epoch:  766 | train loss: 0.000257 | valid loss: 0.000275\n","Epoch:  767 | train loss: 0.000293 | valid loss: 0.000285\n","Epoch:  768 | train loss: 0.000221 | valid loss: 0.000276\n","Epoch:  769 | train loss: 0.000315 | valid loss: 0.000276\n","Epoch:  770 | train loss: 0.000283 | valid loss: 0.000275\n","Epoch:  771 | train loss: 0.000323 | valid loss: 0.000276\n","Epoch:  772 | train loss: 0.000281 | valid loss: 0.000277\n","Epoch:  773 | train loss: 0.000229 | valid loss: 0.000276\n","Epoch:  774 | train loss: 0.000277 | valid loss: 0.000276\n","Epoch:  775 | train loss: 0.000250 | valid loss: 0.000276\n","Epoch:  776 | train loss: 0.000279 | valid loss: 0.000276\n","Epoch:  777 | train loss: 0.000253 | valid loss: 0.000275\n","Epoch:  778 | train loss: 0.000256 | valid loss: 0.000276\n","Epoch:  779 | train loss: 0.000249 | valid loss: 0.000279\n","Epoch:  780 | train loss: 0.000220 | valid loss: 0.000275\n","Epoch:  781 | train loss: 0.000330 | valid loss: 0.000276\n","Epoch:  782 | train loss: 0.000269 | valid loss: 0.000274\n","Epoch:  783 | train loss: 0.000250 | valid loss: 0.000274\n","Epoch:  784 | train loss: 0.000354 | valid loss: 0.000279\n","Epoch:  785 | train loss: 0.000222 | valid loss: 0.000278\n","Epoch:  786 | train loss: 0.000280 | valid loss: 0.000274\n","Epoch:  787 | train loss: 0.000400 | valid loss: 0.000276\n","Epoch:  788 | train loss: 0.000258 | valid loss: 0.000275\n","Epoch:  789 | train loss: 0.000218 | valid loss: 0.000274\n","Epoch:  790 | train loss: 0.000210 | valid loss: 0.000275\n","Epoch:  791 | train loss: 0.000328 | valid loss: 0.000275\n","Epoch:  792 | train loss: 0.000249 | valid loss: 0.000275\n","Epoch:  793 | train loss: 0.000276 | valid loss: 0.000277\n","Epoch:  794 | train loss: 0.000284 | valid loss: 0.000278\n","Epoch:  795 | train loss: 0.000292 | valid loss: 0.000275\n","Epoch:  796 | train loss: 0.000243 | valid loss: 0.000273\n","Epoch:  797 | train loss: 0.000265 | valid loss: 0.000274\n","Epoch:  798 | train loss: 0.000296 | valid loss: 0.000279\n","Epoch:  799 | train loss: 0.000258 | valid loss: 0.000274\n","Epoch:  800 | train loss: 0.000339 | valid loss: 0.000274\n","Epoch:  801 | train loss: 0.000263 | valid loss: 0.000273\n","Epoch:  802 | train loss: 0.000245 | valid loss: 0.000275\n","Epoch:  803 | train loss: 0.000267 | valid loss: 0.000273\n","Epoch:  804 | train loss: 0.000269 | valid loss: 0.000274\n","Epoch:  805 | train loss: 0.000231 | valid loss: 0.000275\n","Epoch:  806 | train loss: 0.000286 | valid loss: 0.000276\n","Epoch:  807 | train loss: 0.000262 | valid loss: 0.000276\n","Epoch:  808 | train loss: 0.000251 | valid loss: 0.000272\n","Epoch:  809 | train loss: 0.000280 | valid loss: 0.000276\n","Epoch:  810 | train loss: 0.000239 | valid loss: 0.000274\n","Epoch:  811 | train loss: 0.000244 | valid loss: 0.000273\n","Epoch:  812 | train loss: 0.000319 | valid loss: 0.000272\n","Epoch:  813 | train loss: 0.000330 | valid loss: 0.000273\n","Epoch:  814 | train loss: 0.000215 | valid loss: 0.000272\n","Epoch:  815 | train loss: 0.000241 | valid loss: 0.000274\n","Epoch:  816 | train loss: 0.000260 | valid loss: 0.000273\n","Epoch:  817 | train loss: 0.000240 | valid loss: 0.000273\n","Epoch:  818 | train loss: 0.000263 | valid loss: 0.000276\n","Epoch:  819 | train loss: 0.000242 | valid loss: 0.000273\n","Epoch:  820 | train loss: 0.000309 | valid loss: 0.000272\n","Epoch:  821 | train loss: 0.000260 | valid loss: 0.000273\n","Epoch:  822 | train loss: 0.000307 | valid loss: 0.000274\n","Epoch:  823 | train loss: 0.000283 | valid loss: 0.000272\n","Epoch:  824 | train loss: 0.000280 | valid loss: 0.000272\n","Epoch:  825 | train loss: 0.000229 | valid loss: 0.000276\n","Epoch:  826 | train loss: 0.000329 | valid loss: 0.000271\n","Epoch:  827 | train loss: 0.000239 | valid loss: 0.000272\n","Epoch:  828 | train loss: 0.000331 | valid loss: 0.000271\n","Epoch:  829 | train loss: 0.000269 | valid loss: 0.000275\n","Epoch:  830 | train loss: 0.000289 | valid loss: 0.000273\n","Epoch:  831 | train loss: 0.000262 | valid loss: 0.000272\n","Epoch:  832 | train loss: 0.000269 | valid loss: 0.000271\n","Epoch:  833 | train loss: 0.000322 | valid loss: 0.000272\n","Epoch:  834 | train loss: 0.000354 | valid loss: 0.000274\n","Epoch:  835 | train loss: 0.000231 | valid loss: 0.000271\n","Epoch:  836 | train loss: 0.000286 | valid loss: 0.000272\n","Epoch:  837 | train loss: 0.000314 | valid loss: 0.000273\n","Epoch:  838 | train loss: 0.000247 | valid loss: 0.000272\n","Epoch:  839 | train loss: 0.000232 | valid loss: 0.000271\n","Epoch:  840 | train loss: 0.000224 | valid loss: 0.000272\n","Epoch:  841 | train loss: 0.000286 | valid loss: 0.000272\n","Epoch:  842 | train loss: 0.000293 | valid loss: 0.000271\n","Epoch:  843 | train loss: 0.000238 | valid loss: 0.000271\n","Epoch:  844 | train loss: 0.000278 | valid loss: 0.000271\n","Epoch:  845 | train loss: 0.000331 | valid loss: 0.000271\n","Epoch:  846 | train loss: 0.000305 | valid loss: 0.000274\n","Epoch:  847 | train loss: 0.000262 | valid loss: 0.000272\n","Epoch:  848 | train loss: 0.000364 | valid loss: 0.000270\n","Epoch:  849 | train loss: 0.000274 | valid loss: 0.000271\n","Epoch:  850 | train loss: 0.000275 | valid loss: 0.000270\n","Epoch:  851 | train loss: 0.000279 | valid loss: 0.000271\n","Epoch:  852 | train loss: 0.000268 | valid loss: 0.000269\n","Epoch:  853 | train loss: 0.000245 | valid loss: 0.000274\n","Epoch:  854 | train loss: 0.000244 | valid loss: 0.000270\n","Epoch:  855 | train loss: 0.000296 | valid loss: 0.000271\n","Epoch:  856 | train loss: 0.000210 | valid loss: 0.000269\n","Epoch:  857 | train loss: 0.000337 | valid loss: 0.000269\n","Epoch:  858 | train loss: 0.000276 | valid loss: 0.000271\n","Epoch:  859 | train loss: 0.000235 | valid loss: 0.000270\n","Epoch:  860 | train loss: 0.000231 | valid loss: 0.000271\n","Epoch:  861 | train loss: 0.000213 | valid loss: 0.000271\n","Epoch:  862 | train loss: 0.000248 | valid loss: 0.000271\n","Epoch:  863 | train loss: 0.000285 | valid loss: 0.000276\n","Epoch:  864 | train loss: 0.000286 | valid loss: 0.000274\n","Epoch:  865 | train loss: 0.000250 | valid loss: 0.000270\n","Epoch:  866 | train loss: 0.000330 | valid loss: 0.000270\n","Epoch:  867 | train loss: 0.000273 | valid loss: 0.000270\n","Epoch:  868 | train loss: 0.000230 | valid loss: 0.000271\n","Epoch:  869 | train loss: 0.000216 | valid loss: 0.000269\n","Epoch:  870 | train loss: 0.000236 | valid loss: 0.000269\n","Epoch:  871 | train loss: 0.000283 | valid loss: 0.000270\n","Epoch:  872 | train loss: 0.000281 | valid loss: 0.000268\n","Epoch:  873 | train loss: 0.000301 | valid loss: 0.000271\n","Epoch:  874 | train loss: 0.000200 | valid loss: 0.000271\n","Epoch:  875 | train loss: 0.000214 | valid loss: 0.000271\n","Epoch:  876 | train loss: 0.000223 | valid loss: 0.000269\n","Epoch:  877 | train loss: 0.000271 | valid loss: 0.000269\n","Epoch:  878 | train loss: 0.000264 | valid loss: 0.000269\n","Epoch:  879 | train loss: 0.000280 | valid loss: 0.000269\n","Epoch:  880 | train loss: 0.000328 | valid loss: 0.000271\n","Epoch:  881 | train loss: 0.000339 | valid loss: 0.000274\n","Epoch:  882 | train loss: 0.000362 | valid loss: 0.000268\n","Epoch:  883 | train loss: 0.000283 | valid loss: 0.000281\n","Epoch:  884 | train loss: 0.000286 | valid loss: 0.000267\n","Epoch:  885 | train loss: 0.000298 | valid loss: 0.000267\n","Epoch:  886 | train loss: 0.000332 | valid loss: 0.000268\n","Epoch:  887 | train loss: 0.000317 | valid loss: 0.000267\n","Epoch:  888 | train loss: 0.000269 | valid loss: 0.000267\n","Epoch:  889 | train loss: 0.000323 | valid loss: 0.000269\n","Epoch:  890 | train loss: 0.000349 | valid loss: 0.000268\n","Epoch:  891 | train loss: 0.000314 | valid loss: 0.000268\n","Epoch:  892 | train loss: 0.000279 | valid loss: 0.000270\n","Epoch:  893 | train loss: 0.000221 | valid loss: 0.000267\n","Epoch:  894 | train loss: 0.000278 | valid loss: 0.000268\n","Epoch:  895 | train loss: 0.000260 | valid loss: 0.000271\n","Epoch:  896 | train loss: 0.000238 | valid loss: 0.000269\n","Epoch:  897 | train loss: 0.000338 | valid loss: 0.000268\n","Epoch:  898 | train loss: 0.000250 | valid loss: 0.000267\n","Epoch:  899 | train loss: 0.000246 | valid loss: 0.000270\n","Epoch:  900 | train loss: 0.000271 | valid loss: 0.000267\n","Epoch:  901 | train loss: 0.000294 | valid loss: 0.000268\n","Epoch:  902 | train loss: 0.000271 | valid loss: 0.000266\n","Epoch:  903 | train loss: 0.000242 | valid loss: 0.000268\n","Epoch:  904 | train loss: 0.000326 | valid loss: 0.000267\n","Epoch:  905 | train loss: 0.000268 | valid loss: 0.000269\n","Epoch:  906 | train loss: 0.000309 | valid loss: 0.000268\n","Epoch:  907 | train loss: 0.000208 | valid loss: 0.000268\n","Epoch:  908 | train loss: 0.000315 | valid loss: 0.000269\n","Epoch:  909 | train loss: 0.000311 | valid loss: 0.000267\n","Epoch:  910 | train loss: 0.000274 | valid loss: 0.000267\n","Epoch:  911 | train loss: 0.000245 | valid loss: 0.000267\n","Epoch:  912 | train loss: 0.000235 | valid loss: 0.000270\n","Epoch:  913 | train loss: 0.000238 | valid loss: 0.000270\n","Epoch:  914 | train loss: 0.000350 | valid loss: 0.000268\n","Epoch:  915 | train loss: 0.000199 | valid loss: 0.000266\n","Epoch:  916 | train loss: 0.000301 | valid loss: 0.000268\n","Epoch:  917 | train loss: 0.000290 | valid loss: 0.000267\n","Epoch:  918 | train loss: 0.000284 | valid loss: 0.000269\n","Epoch:  919 | train loss: 0.000270 | valid loss: 0.000270\n","Epoch:  920 | train loss: 0.000206 | valid loss: 0.000266\n","Epoch:  921 | train loss: 0.000291 | valid loss: 0.000268\n","Epoch:  922 | train loss: 0.000283 | valid loss: 0.000267\n","Epoch:  923 | train loss: 0.000281 | valid loss: 0.000266\n","Epoch:  924 | train loss: 0.000282 | valid loss: 0.000267\n","Epoch:  925 | train loss: 0.000325 | valid loss: 0.000266\n","Epoch:  926 | train loss: 0.000281 | valid loss: 0.000266\n","Epoch:  927 | train loss: 0.000239 | valid loss: 0.000268\n","Epoch:  928 | train loss: 0.000247 | valid loss: 0.000268\n","Epoch:  929 | train loss: 0.000210 | valid loss: 0.000266\n","Epoch:  930 | train loss: 0.000335 | valid loss: 0.000266\n","Epoch:  931 | train loss: 0.000205 | valid loss: 0.000266\n","Epoch:  932 | train loss: 0.000275 | valid loss: 0.000268\n","Epoch:  933 | train loss: 0.000305 | valid loss: 0.000266\n","Epoch:  934 | train loss: 0.000263 | valid loss: 0.000270\n","Epoch:  935 | train loss: 0.000273 | valid loss: 0.000268\n","Epoch:  936 | train loss: 0.000231 | valid loss: 0.000266\n","Epoch:  937 | train loss: 0.000267 | valid loss: 0.000265\n","Epoch:  938 | train loss: 0.000317 | valid loss: 0.000267\n","Epoch:  939 | train loss: 0.000309 | valid loss: 0.000266\n","Epoch:  940 | train loss: 0.000331 | valid loss: 0.000265\n","Epoch:  941 | train loss: 0.000279 | valid loss: 0.000267\n","Epoch:  942 | train loss: 0.000291 | valid loss: 0.000266\n","Epoch:  943 | train loss: 0.000245 | valid loss: 0.000266\n","Epoch:  944 | train loss: 0.000212 | valid loss: 0.000267\n","Epoch:  945 | train loss: 0.000212 | valid loss: 0.000266\n","Epoch:  946 | train loss: 0.000240 | valid loss: 0.000264\n","Epoch:  947 | train loss: 0.000253 | valid loss: 0.000265\n","Epoch:  948 | train loss: 0.000266 | valid loss: 0.000265\n","Epoch:  949 | train loss: 0.000305 | valid loss: 0.000266\n","Epoch:  950 | train loss: 0.000281 | valid loss: 0.000265\n","Epoch:  951 | train loss: 0.000262 | valid loss: 0.000266\n","Epoch:  952 | train loss: 0.000300 | valid loss: 0.000267\n","Epoch:  953 | train loss: 0.000261 | valid loss: 0.000272\n","Epoch:  954 | train loss: 0.000243 | valid loss: 0.000265\n","Epoch:  955 | train loss: 0.000291 | valid loss: 0.000266\n","Epoch:  956 | train loss: 0.000244 | valid loss: 0.000268\n","Epoch:  957 | train loss: 0.000246 | valid loss: 0.000265\n","Epoch:  958 | train loss: 0.000238 | valid loss: 0.000267\n","Epoch:  959 | train loss: 0.000239 | valid loss: 0.000265\n","Epoch:  960 | train loss: 0.000309 | valid loss: 0.000265\n","Epoch:  961 | train loss: 0.000304 | valid loss: 0.000264\n","Epoch:  962 | train loss: 0.000317 | valid loss: 0.000265\n","Epoch:  963 | train loss: 0.000229 | valid loss: 0.000266\n","Epoch:  964 | train loss: 0.000292 | valid loss: 0.000264\n","Epoch:  965 | train loss: 0.000229 | valid loss: 0.000265\n","Epoch:  966 | train loss: 0.000299 | valid loss: 0.000265\n","Epoch:  967 | train loss: 0.000317 | valid loss: 0.000267\n","Epoch:  968 | train loss: 0.000286 | valid loss: 0.000263\n","Epoch:  969 | train loss: 0.000274 | valid loss: 0.000268\n","Epoch:  970 | train loss: 0.000301 | valid loss: 0.000266\n","Epoch:  971 | train loss: 0.000216 | valid loss: 0.000266\n","Epoch:  972 | train loss: 0.000211 | valid loss: 0.000264\n","Epoch:  973 | train loss: 0.000293 | valid loss: 0.000264\n","Epoch:  974 | train loss: 0.000286 | valid loss: 0.000264\n","Epoch:  975 | train loss: 0.000233 | valid loss: 0.000263\n","Epoch:  976 | train loss: 0.000360 | valid loss: 0.000264\n","Epoch:  977 | train loss: 0.000460 | valid loss: 0.000267\n","Epoch:  978 | train loss: 0.000333 | valid loss: 0.000264\n","Epoch:  979 | train loss: 0.000281 | valid loss: 0.000263\n","Epoch:  980 | train loss: 0.000239 | valid loss: 0.000266\n","Epoch:  981 | train loss: 0.000224 | valid loss: 0.000264\n","Epoch:  982 | train loss: 0.000267 | valid loss: 0.000263\n","Epoch:  983 | train loss: 0.000202 | valid loss: 0.000265\n","Epoch:  984 | train loss: 0.000258 | valid loss: 0.000264\n","Epoch:  985 | train loss: 0.000238 | valid loss: 0.000264\n","Epoch:  986 | train loss: 0.000344 | valid loss: 0.000262\n","Epoch:  987 | train loss: 0.000241 | valid loss: 0.000263\n","Epoch:  988 | train loss: 0.000263 | valid loss: 0.000262\n","Epoch:  989 | train loss: 0.000369 | valid loss: 0.000264\n","Epoch:  990 | train loss: 0.000300 | valid loss: 0.000263\n","Epoch:  991 | train loss: 0.000239 | valid loss: 0.000263\n","Epoch:  992 | train loss: 0.000280 | valid loss: 0.000266\n","Epoch:  993 | train loss: 0.000218 | valid loss: 0.000263\n","Epoch:  994 | train loss: 0.000355 | valid loss: 0.000263\n","Epoch:  995 | train loss: 0.000241 | valid loss: 0.000264\n","Epoch:  996 | train loss: 0.000343 | valid loss: 0.000263\n","Epoch:  997 | train loss: 0.000260 | valid loss: 0.000263\n","Epoch:  998 | train loss: 0.000296 | valid loss: 0.000265\n","Epoch:  999 | train loss: 0.000210 | valid loss: 0.000262\n","Epoch:  1000 | train loss: 0.000280 | valid loss: 0.000262\n","Epoch:  1001 | train loss: 0.000297 | valid loss: 0.000262\n","Epoch:  1002 | train loss: 0.000267 | valid loss: 0.000262\n","Epoch:  1003 | train loss: 0.000239 | valid loss: 0.000265\n","Epoch:  1004 | train loss: 0.000485 | valid loss: 0.000265\n","Epoch:  1005 | train loss: 0.000206 | valid loss: 0.000262\n","Epoch:  1006 | train loss: 0.000259 | valid loss: 0.000261\n","Epoch:  1007 | train loss: 0.000256 | valid loss: 0.000262\n","Epoch:  1008 | train loss: 0.000262 | valid loss: 0.000265\n","Epoch:  1009 | train loss: 0.000191 | valid loss: 0.000263\n","Epoch:  1010 | train loss: 0.000343 | valid loss: 0.000265\n","Epoch:  1011 | train loss: 0.000253 | valid loss: 0.000262\n","Epoch:  1012 | train loss: 0.000209 | valid loss: 0.000261\n","Epoch:  1013 | train loss: 0.000219 | valid loss: 0.000262\n","Epoch:  1014 | train loss: 0.000301 | valid loss: 0.000266\n","Epoch:  1015 | train loss: 0.000274 | valid loss: 0.000262\n","Epoch:  1016 | train loss: 0.000244 | valid loss: 0.000262\n","Epoch:  1017 | train loss: 0.000241 | valid loss: 0.000263\n","Epoch:  1018 | train loss: 0.000260 | valid loss: 0.000262\n","Epoch:  1019 | train loss: 0.000294 | valid loss: 0.000261\n","Epoch:  1020 | train loss: 0.000195 | valid loss: 0.000262\n","Epoch:  1021 | train loss: 0.000237 | valid loss: 0.000263\n","Epoch:  1022 | train loss: 0.000202 | valid loss: 0.000263\n","Epoch:  1023 | train loss: 0.000306 | valid loss: 0.000263\n","Epoch:  1024 | train loss: 0.000296 | valid loss: 0.000265\n","Epoch:  1025 | train loss: 0.000258 | valid loss: 0.000262\n","Epoch:  1026 | train loss: 0.000258 | valid loss: 0.000263\n","Epoch:  1027 | train loss: 0.000267 | valid loss: 0.000264\n","Epoch:  1028 | train loss: 0.000205 | valid loss: 0.000262\n","Epoch:  1029 | train loss: 0.000319 | valid loss: 0.000261\n","Epoch:  1030 | train loss: 0.000243 | valid loss: 0.000263\n","Epoch:  1031 | train loss: 0.000385 | valid loss: 0.000262\n","Epoch:  1032 | train loss: 0.000260 | valid loss: 0.000263\n","Epoch:  1033 | train loss: 0.000285 | valid loss: 0.000263\n","Epoch:  1034 | train loss: 0.000201 | valid loss: 0.000260\n","Epoch:  1035 | train loss: 0.000297 | valid loss: 0.000262\n","Epoch:  1036 | train loss: 0.000248 | valid loss: 0.000262\n","Epoch:  1037 | train loss: 0.000266 | valid loss: 0.000263\n","Epoch:  1038 | train loss: 0.000282 | valid loss: 0.000262\n","Epoch:  1039 | train loss: 0.000236 | valid loss: 0.000260\n","Epoch:  1040 | train loss: 0.000274 | valid loss: 0.000260\n","Epoch:  1041 | train loss: 0.000205 | valid loss: 0.000262\n","Epoch:  1042 | train loss: 0.000220 | valid loss: 0.000260\n","Epoch:  1043 | train loss: 0.000264 | valid loss: 0.000261\n","Epoch:  1044 | train loss: 0.000271 | valid loss: 0.000259\n","Epoch:  1045 | train loss: 0.000214 | valid loss: 0.000262\n","Epoch:  1046 | train loss: 0.000264 | valid loss: 0.000260\n","Epoch:  1047 | train loss: 0.000244 | valid loss: 0.000259\n","Epoch:  1048 | train loss: 0.000367 | valid loss: 0.000261\n","Epoch:  1049 | train loss: 0.000237 | valid loss: 0.000260\n","Epoch:  1050 | train loss: 0.000225 | valid loss: 0.000261\n","Epoch:  1051 | train loss: 0.000277 | valid loss: 0.000260\n","Epoch:  1052 | train loss: 0.000332 | valid loss: 0.000260\n","Epoch:  1053 | train loss: 0.000254 | valid loss: 0.000259\n","Epoch:  1054 | train loss: 0.000200 | valid loss: 0.000260\n","Epoch:  1055 | train loss: 0.000213 | valid loss: 0.000262\n","Epoch:  1056 | train loss: 0.000324 | valid loss: 0.000262\n","Epoch:  1057 | train loss: 0.000297 | valid loss: 0.000259\n","Epoch:  1058 | train loss: 0.000220 | valid loss: 0.000263\n","Epoch:  1059 | train loss: 0.000281 | valid loss: 0.000260\n","Epoch:  1060 | train loss: 0.000218 | valid loss: 0.000261\n","Epoch:  1061 | train loss: 0.000260 | valid loss: 0.000259\n","Epoch:  1062 | train loss: 0.000306 | valid loss: 0.000262\n","Epoch:  1063 | train loss: 0.000272 | valid loss: 0.000259\n","Epoch:  1064 | train loss: 0.000263 | valid loss: 0.000259\n","Epoch:  1065 | train loss: 0.000245 | valid loss: 0.000258\n","Epoch:  1066 | train loss: 0.000322 | valid loss: 0.000261\n","Epoch:  1067 | train loss: 0.000204 | valid loss: 0.000263\n","Epoch:  1068 | train loss: 0.000223 | valid loss: 0.000261\n","Epoch:  1069 | train loss: 0.000297 | valid loss: 0.000259\n","Epoch:  1070 | train loss: 0.000205 | valid loss: 0.000260\n","Epoch:  1071 | train loss: 0.000263 | valid loss: 0.000261\n","Epoch:  1072 | train loss: 0.000266 | valid loss: 0.000258\n","Epoch:  1073 | train loss: 0.000221 | valid loss: 0.000260\n","Epoch:  1074 | train loss: 0.000213 | valid loss: 0.000260\n","Epoch:  1075 | train loss: 0.000300 | valid loss: 0.000259\n","Epoch:  1076 | train loss: 0.000220 | valid loss: 0.000260\n","Epoch:  1077 | train loss: 0.000220 | valid loss: 0.000261\n","Epoch:  1078 | train loss: 0.000254 | valid loss: 0.000260\n","Epoch:  1079 | train loss: 0.000329 | valid loss: 0.000262\n","Epoch:  1080 | train loss: 0.000311 | valid loss: 0.000263\n","Epoch:  1081 | train loss: 0.000275 | valid loss: 0.000258\n","Epoch:  1082 | train loss: 0.000274 | valid loss: 0.000261\n","Epoch:  1083 | train loss: 0.000238 | valid loss: 0.000260\n","Epoch:  1084 | train loss: 0.000260 | valid loss: 0.000260\n","Epoch:  1085 | train loss: 0.000240 | valid loss: 0.000259\n","Epoch:  1086 | train loss: 0.000232 | valid loss: 0.000265\n","Epoch:  1087 | train loss: 0.000258 | valid loss: 0.000260\n","Epoch:  1088 | train loss: 0.000289 | valid loss: 0.000260\n","Epoch:  1089 | train loss: 0.000261 | valid loss: 0.000259\n","Epoch:  1090 | train loss: 0.000342 | valid loss: 0.000260\n","Epoch:  1091 | train loss: 0.000245 | valid loss: 0.000258\n","Epoch:  1092 | train loss: 0.000258 | valid loss: 0.000258\n","Epoch:  1093 | train loss: 0.000337 | valid loss: 0.000260\n","Epoch:  1094 | train loss: 0.000215 | valid loss: 0.000258\n","Epoch:  1095 | train loss: 0.000290 | valid loss: 0.000258\n","Epoch:  1096 | train loss: 0.000297 | valid loss: 0.000258\n","Epoch:  1097 | train loss: 0.000249 | valid loss: 0.000259\n","Epoch:  1098 | train loss: 0.000244 | valid loss: 0.000260\n","Epoch:  1099 | train loss: 0.000248 | valid loss: 0.000259\n","Epoch:  1100 | train loss: 0.000282 | valid loss: 0.000259\n","Epoch:  1101 | train loss: 0.000276 | valid loss: 0.000258\n","Epoch:  1102 | train loss: 0.000278 | valid loss: 0.000257\n","Epoch:  1103 | train loss: 0.000229 | valid loss: 0.000259\n","Epoch:  1104 | train loss: 0.000266 | valid loss: 0.000257\n","Epoch:  1105 | train loss: 0.000212 | valid loss: 0.000259\n","Epoch:  1106 | train loss: 0.000325 | valid loss: 0.000259\n","Epoch:  1107 | train loss: 0.000220 | valid loss: 0.000259\n","Epoch:  1108 | train loss: 0.000287 | valid loss: 0.000258\n","Epoch:  1109 | train loss: 0.000303 | valid loss: 0.000264\n","Epoch:  1110 | train loss: 0.000228 | valid loss: 0.000258\n","Epoch:  1111 | train loss: 0.000290 | valid loss: 0.000260\n","Epoch:  1112 | train loss: 0.000206 | valid loss: 0.000258\n","Epoch:  1113 | train loss: 0.000199 | valid loss: 0.000258\n","Epoch:  1114 | train loss: 0.000243 | valid loss: 0.000258\n","Epoch:  1115 | train loss: 0.000323 | valid loss: 0.000257\n","Epoch:  1116 | train loss: 0.000265 | valid loss: 0.000258\n","Epoch:  1117 | train loss: 0.000252 | valid loss: 0.000257\n","Epoch:  1118 | train loss: 0.000265 | valid loss: 0.000257\n","Epoch:  1119 | train loss: 0.000246 | valid loss: 0.000258\n","Epoch:  1120 | train loss: 0.000246 | valid loss: 0.000258\n","Epoch:  1121 | train loss: 0.000265 | valid loss: 0.000256\n","Epoch:  1122 | train loss: 0.000248 | valid loss: 0.000258\n","Epoch:  1123 | train loss: 0.000242 | valid loss: 0.000256\n","Epoch:  1124 | train loss: 0.000278 | valid loss: 0.000258\n","Epoch:  1125 | train loss: 0.000243 | valid loss: 0.000259\n","Epoch:  1126 | train loss: 0.000190 | valid loss: 0.000258\n","Epoch:  1127 | train loss: 0.000353 | valid loss: 0.000257\n","Epoch:  1128 | train loss: 0.000235 | valid loss: 0.000257\n","Epoch:  1129 | train loss: 0.000330 | valid loss: 0.000259\n","Epoch:  1130 | train loss: 0.000220 | valid loss: 0.000257\n","Epoch:  1131 | train loss: 0.000294 | valid loss: 0.000257\n","Epoch:  1132 | train loss: 0.000265 | valid loss: 0.000256\n","Epoch:  1133 | train loss: 0.000317 | valid loss: 0.000258\n","Epoch:  1134 | train loss: 0.000212 | valid loss: 0.000259\n","Epoch:  1135 | train loss: 0.000275 | valid loss: 0.000256\n","Epoch:  1136 | train loss: 0.000302 | valid loss: 0.000258\n","Epoch:  1137 | train loss: 0.000257 | valid loss: 0.000259\n","Epoch:  1138 | train loss: 0.000213 | valid loss: 0.000257\n","Epoch:  1139 | train loss: 0.000275 | valid loss: 0.000258\n","Epoch:  1140 | train loss: 0.000361 | valid loss: 0.000256\n","Epoch:  1141 | train loss: 0.000272 | valid loss: 0.000256\n","Epoch:  1142 | train loss: 0.000283 | valid loss: 0.000255\n","Epoch:  1143 | train loss: 0.000222 | valid loss: 0.000257\n","Epoch:  1144 | train loss: 0.000249 | valid loss: 0.000260\n","Epoch:  1145 | train loss: 0.000250 | valid loss: 0.000257\n","Epoch:  1146 | train loss: 0.000252 | valid loss: 0.000256\n","Epoch:  1147 | train loss: 0.000300 | valid loss: 0.000258\n","Epoch:  1148 | train loss: 0.000291 | valid loss: 0.000260\n","Epoch:  1149 | train loss: 0.000245 | valid loss: 0.000255\n","Epoch:  1150 | train loss: 0.000331 | valid loss: 0.000257\n","Epoch:  1151 | train loss: 0.000286 | valid loss: 0.000256\n","Epoch:  1152 | train loss: 0.000210 | valid loss: 0.000255\n","Epoch:  1153 | train loss: 0.000219 | valid loss: 0.000256\n","Epoch:  1154 | train loss: 0.000293 | valid loss: 0.000258\n","Epoch:  1155 | train loss: 0.000275 | valid loss: 0.000258\n","Epoch:  1156 | train loss: 0.000285 | valid loss: 0.000256\n","Epoch:  1157 | train loss: 0.000257 | valid loss: 0.000255\n","Epoch:  1158 | train loss: 0.000204 | valid loss: 0.000258\n","Epoch:  1159 | train loss: 0.000248 | valid loss: 0.000256\n","Epoch:  1160 | train loss: 0.000275 | valid loss: 0.000256\n","Epoch:  1161 | train loss: 0.000287 | valid loss: 0.000258\n","Epoch:  1162 | train loss: 0.000262 | valid loss: 0.000256\n","Epoch:  1163 | train loss: 0.000268 | valid loss: 0.000258\n","Epoch:  1164 | train loss: 0.000202 | valid loss: 0.000256\n","Epoch:  1165 | train loss: 0.000270 | valid loss: 0.000255\n","Epoch:  1166 | train loss: 0.000250 | valid loss: 0.000256\n","Epoch:  1167 | train loss: 0.000271 | valid loss: 0.000255\n","Epoch:  1168 | train loss: 0.000272 | valid loss: 0.000256\n","Epoch:  1169 | train loss: 0.000268 | valid loss: 0.000256\n","Epoch:  1170 | train loss: 0.000279 | valid loss: 0.000256\n","Epoch:  1171 | train loss: 0.000207 | valid loss: 0.000256\n","Epoch:  1172 | train loss: 0.000224 | valid loss: 0.000257\n","Epoch:  1173 | train loss: 0.000245 | valid loss: 0.000256\n","Epoch:  1174 | train loss: 0.000277 | valid loss: 0.000254\n","Epoch:  1175 | train loss: 0.000279 | valid loss: 0.000257\n","Epoch:  1176 | train loss: 0.000243 | valid loss: 0.000256\n","Epoch:  1177 | train loss: 0.000190 | valid loss: 0.000255\n","Epoch:  1178 | train loss: 0.000249 | valid loss: 0.000255\n","Epoch:  1179 | train loss: 0.000253 | valid loss: 0.000257\n","Epoch:  1180 | train loss: 0.000245 | valid loss: 0.000256\n","Epoch:  1181 | train loss: 0.000308 | valid loss: 0.000255\n","Epoch:  1182 | train loss: 0.000312 | valid loss: 0.000256\n","Epoch:  1183 | train loss: 0.000219 | valid loss: 0.000255\n","Epoch:  1184 | train loss: 0.000254 | valid loss: 0.000257\n","Epoch:  1185 | train loss: 0.000338 | valid loss: 0.000258\n","Epoch:  1186 | train loss: 0.000197 | valid loss: 0.000256\n","Epoch:  1187 | train loss: 0.000267 | valid loss: 0.000255\n","Epoch:  1188 | train loss: 0.000249 | valid loss: 0.000254\n","Epoch:  1189 | train loss: 0.000229 | valid loss: 0.000255\n","Epoch:  1190 | train loss: 0.000508 | valid loss: 0.000258\n","Epoch:  1191 | train loss: 0.000250 | valid loss: 0.000256\n","Epoch:  1192 | train loss: 0.000320 | valid loss: 0.000255\n","Epoch:  1193 | train loss: 0.000334 | valid loss: 0.000254\n","Epoch:  1194 | train loss: 0.000197 | valid loss: 0.000253\n","Epoch:  1195 | train loss: 0.000196 | valid loss: 0.000258\n","Epoch:  1196 | train loss: 0.000305 | valid loss: 0.000257\n","Epoch:  1197 | train loss: 0.000352 | valid loss: 0.000257\n","Epoch:  1198 | train loss: 0.000235 | valid loss: 0.000253\n","Epoch:  1199 | train loss: 0.000295 | valid loss: 0.000258\n","Epoch:  1200 | train loss: 0.000342 | valid loss: 0.000255\n","Epoch:  1201 | train loss: 0.000265 | valid loss: 0.000256\n","Epoch:  1202 | train loss: 0.000263 | valid loss: 0.000255\n","Epoch:  1203 | train loss: 0.000195 | valid loss: 0.000255\n","Epoch:  1204 | train loss: 0.000223 | valid loss: 0.000255\n","Epoch:  1205 | train loss: 0.000241 | valid loss: 0.000253\n","Epoch:  1206 | train loss: 0.000256 | valid loss: 0.000256\n","Epoch:  1207 | train loss: 0.000233 | valid loss: 0.000254\n","Epoch:  1208 | train loss: 0.000326 | valid loss: 0.000256\n","Epoch:  1209 | train loss: 0.000279 | valid loss: 0.000255\n","Epoch:  1210 | train loss: 0.000287 | valid loss: 0.000257\n","Epoch:  1211 | train loss: 0.000233 | valid loss: 0.000254\n","Epoch:  1212 | train loss: 0.000228 | valid loss: 0.000255\n","Epoch:  1213 | train loss: 0.000200 | valid loss: 0.000254\n","Epoch:  1214 | train loss: 0.000292 | valid loss: 0.000255\n","Epoch:  1215 | train loss: 0.000303 | valid loss: 0.000256\n","Epoch:  1216 | train loss: 0.000227 | valid loss: 0.000255\n","Epoch:  1217 | train loss: 0.000221 | valid loss: 0.000256\n","Epoch:  1218 | train loss: 0.000220 | valid loss: 0.000256\n","Epoch:  1219 | train loss: 0.000276 | valid loss: 0.000256\n","Epoch:  1220 | train loss: 0.000285 | valid loss: 0.000254\n","Epoch:  1221 | train loss: 0.000209 | valid loss: 0.000253\n","Epoch:  1222 | train loss: 0.000198 | valid loss: 0.000254\n","Epoch:  1223 | train loss: 0.000285 | valid loss: 0.000263\n","Epoch:  1224 | train loss: 0.000233 | valid loss: 0.000253\n","Epoch:  1225 | train loss: 0.000223 | valid loss: 0.000254\n","Epoch:  1226 | train loss: 0.000240 | valid loss: 0.000254\n","Epoch:  1227 | train loss: 0.000311 | valid loss: 0.000254\n","Epoch:  1228 | train loss: 0.000225 | valid loss: 0.000254\n","Epoch:  1229 | train loss: 0.000230 | valid loss: 0.000254\n","Epoch:  1230 | train loss: 0.000195 | valid loss: 0.000253\n","Epoch:  1231 | train loss: 0.000243 | valid loss: 0.000255\n","Epoch:  1232 | train loss: 0.000261 | valid loss: 0.000254\n","Epoch:  1233 | train loss: 0.000265 | valid loss: 0.000253\n","Epoch:  1234 | train loss: 0.000190 | valid loss: 0.000253\n","Epoch:  1235 | train loss: 0.000227 | valid loss: 0.000253\n","Epoch:  1236 | train loss: 0.000214 | valid loss: 0.000255\n","Epoch:  1237 | train loss: 0.000215 | valid loss: 0.000254\n","Epoch:  1238 | train loss: 0.000279 | valid loss: 0.000254\n","Epoch:  1239 | train loss: 0.000252 | valid loss: 0.000253\n","Epoch:  1240 | train loss: 0.000225 | valid loss: 0.000254\n","Epoch:  1241 | train loss: 0.000212 | valid loss: 0.000253\n","Epoch:  1242 | train loss: 0.000270 | valid loss: 0.000253\n","Epoch:  1243 | train loss: 0.000187 | valid loss: 0.000253\n","Epoch:  1244 | train loss: 0.000199 | valid loss: 0.000253\n","Epoch:  1245 | train loss: 0.000211 | valid loss: 0.000253\n","Epoch:  1246 | train loss: 0.000235 | valid loss: 0.000253\n","Epoch:  1247 | train loss: 0.000309 | valid loss: 0.000255\n","Epoch:  1248 | train loss: 0.000234 | valid loss: 0.000253\n","Epoch:  1249 | train loss: 0.000277 | valid loss: 0.000253\n","Epoch:  1250 | train loss: 0.000314 | valid loss: 0.000252\n","Epoch:  1251 | train loss: 0.000254 | valid loss: 0.000256\n","Epoch:  1252 | train loss: 0.000262 | valid loss: 0.000255\n","Epoch:  1253 | train loss: 0.000271 | valid loss: 0.000255\n","Epoch:  1254 | train loss: 0.000269 | valid loss: 0.000256\n","Epoch:  1255 | train loss: 0.000208 | valid loss: 0.000255\n","Epoch:  1256 | train loss: 0.000221 | valid loss: 0.000252\n","Epoch:  1257 | train loss: 0.000272 | valid loss: 0.000252\n","Epoch:  1258 | train loss: 0.000332 | valid loss: 0.000260\n","Epoch:  1259 | train loss: 0.000195 | valid loss: 0.000254\n","Epoch:  1260 | train loss: 0.000227 | valid loss: 0.000254\n","Epoch:  1261 | train loss: 0.000211 | valid loss: 0.000253\n","Epoch:  1262 | train loss: 0.000257 | valid loss: 0.000252\n","Epoch:  1263 | train loss: 0.000213 | valid loss: 0.000251\n","Epoch:  1264 | train loss: 0.000220 | valid loss: 0.000252\n","Epoch:  1265 | train loss: 0.000231 | valid loss: 0.000252\n","Epoch:  1266 | train loss: 0.000241 | valid loss: 0.000252\n","Epoch:  1267 | train loss: 0.000231 | valid loss: 0.000252\n","Epoch:  1268 | train loss: 0.000270 | valid loss: 0.000255\n","Epoch:  1269 | train loss: 0.000210 | valid loss: 0.000255\n","Epoch:  1270 | train loss: 0.000300 | valid loss: 0.000254\n","Epoch:  1271 | train loss: 0.000282 | valid loss: 0.000256\n","Epoch:  1272 | train loss: 0.000191 | valid loss: 0.000252\n","Epoch:  1273 | train loss: 0.000324 | valid loss: 0.000254\n","Epoch:  1274 | train loss: 0.000263 | valid loss: 0.000252\n","Epoch:  1275 | train loss: 0.000248 | valid loss: 0.000253\n","Epoch:  1276 | train loss: 0.000214 | valid loss: 0.000251\n","Epoch:  1277 | train loss: 0.000255 | valid loss: 0.000253\n","Epoch:  1278 | train loss: 0.000227 | valid loss: 0.000252\n","Epoch:  1279 | train loss: 0.000227 | valid loss: 0.000253\n","Epoch:  1280 | train loss: 0.000269 | valid loss: 0.000251\n","Epoch:  1281 | train loss: 0.000250 | valid loss: 0.000253\n","Epoch:  1282 | train loss: 0.000269 | valid loss: 0.000252\n","Epoch:  1283 | train loss: 0.000263 | valid loss: 0.000252\n","Epoch:  1284 | train loss: 0.000283 | valid loss: 0.000256\n","Epoch:  1285 | train loss: 0.000228 | valid loss: 0.000266\n","Epoch:  1286 | train loss: 0.000244 | valid loss: 0.000251\n","Epoch:  1287 | train loss: 0.000243 | valid loss: 0.000250\n","Epoch:  1288 | train loss: 0.000280 | valid loss: 0.000252\n","Epoch:  1289 | train loss: 0.000275 | valid loss: 0.000251\n","Epoch:  1290 | train loss: 0.000195 | valid loss: 0.000251\n","Epoch:  1291 | train loss: 0.000279 | valid loss: 0.000255\n","Epoch:  1292 | train loss: 0.000203 | valid loss: 0.000252\n","Epoch:  1293 | train loss: 0.000210 | valid loss: 0.000252\n","Epoch:  1294 | train loss: 0.000322 | valid loss: 0.000253\n","Epoch:  1295 | train loss: 0.000201 | valid loss: 0.000253\n","Epoch:  1296 | train loss: 0.000251 | valid loss: 0.000251\n","Epoch:  1297 | train loss: 0.000300 | valid loss: 0.000251\n","Epoch:  1298 | train loss: 0.000269 | valid loss: 0.000254\n","Epoch:  1299 | train loss: 0.000265 | valid loss: 0.000252\n","Epoch:  1300 | train loss: 0.000338 | valid loss: 0.000253\n","Epoch:  1301 | train loss: 0.000360 | valid loss: 0.000252\n","Epoch:  1302 | train loss: 0.000236 | valid loss: 0.000251\n","Epoch:  1303 | train loss: 0.000279 | valid loss: 0.000252\n","Epoch:  1304 | train loss: 0.000243 | valid loss: 0.000251\n","Epoch:  1305 | train loss: 0.000220 | valid loss: 0.000250\n","Epoch:  1306 | train loss: 0.000285 | valid loss: 0.000251\n","Epoch:  1307 | train loss: 0.000530 | valid loss: 0.000255\n","Epoch:  1308 | train loss: 0.000242 | valid loss: 0.000252\n","Epoch:  1309 | train loss: 0.000348 | valid loss: 0.000251\n","Epoch:  1310 | train loss: 0.000264 | valid loss: 0.000253\n","Epoch:  1311 | train loss: 0.000293 | valid loss: 0.000251\n","Epoch:  1312 | train loss: 0.000218 | valid loss: 0.000252\n","Epoch:  1313 | train loss: 0.000274 | valid loss: 0.000252\n","Epoch:  1314 | train loss: 0.000198 | valid loss: 0.000251\n","Epoch:  1315 | train loss: 0.000252 | valid loss: 0.000251\n","Epoch:  1316 | train loss: 0.000252 | valid loss: 0.000251\n","Epoch:  1317 | train loss: 0.000445 | valid loss: 0.000253\n","Epoch:  1318 | train loss: 0.000306 | valid loss: 0.000251\n","Epoch:  1319 | train loss: 0.000292 | valid loss: 0.000252\n","Epoch:  1320 | train loss: 0.000230 | valid loss: 0.000252\n","Epoch:  1321 | train loss: 0.000289 | valid loss: 0.000252\n","Epoch:  1322 | train loss: 0.000248 | valid loss: 0.000255\n","Epoch:  1323 | train loss: 0.000295 | valid loss: 0.000254\n","Epoch:  1324 | train loss: 0.000513 | valid loss: 0.000252\n","Epoch:  1325 | train loss: 0.000206 | valid loss: 0.000253\n","Epoch:  1326 | train loss: 0.000288 | valid loss: 0.000252\n","Epoch:  1327 | train loss: 0.000197 | valid loss: 0.000250\n","Epoch:  1328 | train loss: 0.000184 | valid loss: 0.000250\n","Epoch:  1329 | train loss: 0.000244 | valid loss: 0.000249\n","Epoch:  1330 | train loss: 0.000257 | valid loss: 0.000251\n","Epoch:  1331 | train loss: 0.000221 | valid loss: 0.000249\n","Epoch:  1332 | train loss: 0.000257 | valid loss: 0.000252\n","Epoch:  1333 | train loss: 0.000263 | valid loss: 0.000251\n","Epoch:  1334 | train loss: 0.000342 | valid loss: 0.000253\n","Epoch:  1335 | train loss: 0.000208 | valid loss: 0.000249\n","Epoch:  1336 | train loss: 0.000271 | valid loss: 0.000259\n","Epoch:  1337 | train loss: 0.000235 | valid loss: 0.000251\n","Epoch:  1338 | train loss: 0.000209 | valid loss: 0.000249\n","Epoch:  1339 | train loss: 0.000238 | valid loss: 0.000252\n","Epoch:  1340 | train loss: 0.000265 | valid loss: 0.000252\n","Epoch:  1341 | train loss: 0.000279 | valid loss: 0.000251\n","Epoch:  1342 | train loss: 0.000273 | valid loss: 0.000249\n","Epoch:  1343 | train loss: 0.000262 | valid loss: 0.000252\n","Epoch:  1344 | train loss: 0.000245 | valid loss: 0.000255\n","Epoch:  1345 | train loss: 0.000216 | valid loss: 0.000251\n","Epoch:  1346 | train loss: 0.000190 | valid loss: 0.000249\n","Epoch:  1347 | train loss: 0.000203 | valid loss: 0.000251\n","Epoch:  1348 | train loss: 0.000241 | valid loss: 0.000249\n","Epoch:  1349 | train loss: 0.000207 | valid loss: 0.000251\n","Epoch:  1350 | train loss: 0.000192 | valid loss: 0.000250\n","Epoch:  1351 | train loss: 0.000261 | valid loss: 0.000252\n","Epoch:  1352 | train loss: 0.000246 | valid loss: 0.000251\n","Epoch:  1353 | train loss: 0.000270 | valid loss: 0.000251\n","Epoch:  1354 | train loss: 0.000228 | valid loss: 0.000249\n","Epoch:  1355 | train loss: 0.000216 | valid loss: 0.000248\n","Epoch:  1356 | train loss: 0.000218 | valid loss: 0.000250\n","Epoch:  1357 | train loss: 0.000297 | valid loss: 0.000249\n","Epoch:  1358 | train loss: 0.000215 | valid loss: 0.000250\n","Epoch:  1359 | train loss: 0.000221 | valid loss: 0.000250\n","Epoch:  1360 | train loss: 0.000277 | valid loss: 0.000250\n","Epoch:  1361 | train loss: 0.000268 | valid loss: 0.000252\n","Epoch:  1362 | train loss: 0.000232 | valid loss: 0.000249\n","Epoch:  1363 | train loss: 0.000233 | valid loss: 0.000250\n","Epoch:  1364 | train loss: 0.000256 | valid loss: 0.000252\n","Epoch:  1365 | train loss: 0.000265 | valid loss: 0.000249\n","Epoch:  1366 | train loss: 0.000283 | valid loss: 0.000249\n","Epoch:  1367 | train loss: 0.000288 | valid loss: 0.000249\n","Epoch:  1368 | train loss: 0.000228 | valid loss: 0.000253\n","Epoch:  1369 | train loss: 0.000191 | valid loss: 0.000249\n","Epoch:  1370 | train loss: 0.000331 | valid loss: 0.000249\n","Epoch:  1371 | train loss: 0.000243 | valid loss: 0.000249\n","Epoch:  1372 | train loss: 0.000215 | valid loss: 0.000250\n","Epoch:  1373 | train loss: 0.000287 | valid loss: 0.000251\n","Epoch:  1374 | train loss: 0.000232 | valid loss: 0.000250\n","Epoch:  1375 | train loss: 0.000261 | valid loss: 0.000254\n","Epoch:  1376 | train loss: 0.000289 | valid loss: 0.000249\n","Epoch:  1377 | train loss: 0.000247 | valid loss: 0.000248\n","Epoch:  1378 | train loss: 0.000328 | valid loss: 0.000249\n","Epoch:  1379 | train loss: 0.000256 | valid loss: 0.000251\n","Epoch:  1380 | train loss: 0.000245 | valid loss: 0.000250\n","Epoch:  1381 | train loss: 0.000333 | valid loss: 0.000248\n","Epoch:  1382 | train loss: 0.000249 | valid loss: 0.000250\n","Epoch:  1383 | train loss: 0.000273 | valid loss: 0.000249\n","Epoch:  1384 | train loss: 0.000193 | valid loss: 0.000249\n","Epoch:  1385 | train loss: 0.000209 | valid loss: 0.000249\n","Epoch:  1386 | train loss: 0.000259 | valid loss: 0.000249\n","Epoch:  1387 | train loss: 0.000332 | valid loss: 0.000249\n","Epoch:  1388 | train loss: 0.000284 | valid loss: 0.000249\n","Epoch:  1389 | train loss: 0.000274 | valid loss: 0.000253\n","Epoch:  1390 | train loss: 0.000231 | valid loss: 0.000250\n","Epoch:  1391 | train loss: 0.000265 | valid loss: 0.000249\n","Epoch:  1392 | train loss: 0.000221 | valid loss: 0.000250\n","Epoch:  1393 | train loss: 0.000239 | valid loss: 0.000248\n","Epoch:  1394 | train loss: 0.000220 | valid loss: 0.000250\n","Epoch:  1395 | train loss: 0.000249 | valid loss: 0.000251\n","Epoch:  1396 | train loss: 0.000244 | valid loss: 0.000249\n","Epoch:  1397 | train loss: 0.000206 | valid loss: 0.000248\n","Epoch:  1398 | train loss: 0.000258 | valid loss: 0.000248\n","Epoch:  1399 | train loss: 0.000238 | valid loss: 0.000251\n","Epoch:  1400 | train loss: 0.000248 | valid loss: 0.000251\n","Epoch:  1401 | train loss: 0.000197 | valid loss: 0.000247\n","Epoch:  1402 | train loss: 0.000278 | valid loss: 0.000248\n","Epoch:  1403 | train loss: 0.000261 | valid loss: 0.000247\n","Epoch:  1404 | train loss: 0.000200 | valid loss: 0.000250\n","Epoch:  1405 | train loss: 0.000259 | valid loss: 0.000247\n","Epoch:  1406 | train loss: 0.000236 | valid loss: 0.000248\n","Epoch:  1407 | train loss: 0.000305 | valid loss: 0.000247\n","Epoch:  1408 | train loss: 0.000260 | valid loss: 0.000249\n","Epoch:  1409 | train loss: 0.000193 | valid loss: 0.000253\n","Epoch:  1410 | train loss: 0.000240 | valid loss: 0.000249\n","Epoch:  1411 | train loss: 0.000315 | valid loss: 0.000248\n","Epoch:  1412 | train loss: 0.000301 | valid loss: 0.000247\n","Epoch:  1413 | train loss: 0.000301 | valid loss: 0.000247\n","Epoch:  1414 | train loss: 0.000182 | valid loss: 0.000249\n","Epoch:  1415 | train loss: 0.000298 | valid loss: 0.000248\n","Epoch:  1416 | train loss: 0.000257 | valid loss: 0.000251\n","Epoch:  1417 | train loss: 0.000299 | valid loss: 0.000249\n","Epoch:  1418 | train loss: 0.000310 | valid loss: 0.000252\n","Epoch:  1419 | train loss: 0.000215 | valid loss: 0.000251\n","Epoch:  1420 | train loss: 0.000272 | valid loss: 0.000247\n","Epoch:  1421 | train loss: 0.000250 | valid loss: 0.000249\n","Epoch:  1422 | train loss: 0.000262 | valid loss: 0.000247\n","Epoch:  1423 | train loss: 0.000271 | valid loss: 0.000249\n","Epoch:  1424 | train loss: 0.000252 | valid loss: 0.000248\n","Epoch:  1425 | train loss: 0.000211 | valid loss: 0.000249\n","Epoch:  1426 | train loss: 0.000230 | valid loss: 0.000250\n","Epoch:  1427 | train loss: 0.000277 | valid loss: 0.000249\n","Epoch:  1428 | train loss: 0.000294 | valid loss: 0.000246\n","Epoch:  1429 | train loss: 0.000280 | valid loss: 0.000246\n","Epoch:  1430 | train loss: 0.000223 | valid loss: 0.000247\n","Epoch:  1431 | train loss: 0.000313 | valid loss: 0.000248\n","Epoch:  1432 | train loss: 0.000291 | valid loss: 0.000246\n","Epoch:  1433 | train loss: 0.000290 | valid loss: 0.000249\n","Epoch:  1434 | train loss: 0.000317 | valid loss: 0.000247\n","Epoch:  1435 | train loss: 0.000257 | valid loss: 0.000248\n","Epoch:  1436 | train loss: 0.000367 | valid loss: 0.000248\n","Epoch:  1437 | train loss: 0.000267 | valid loss: 0.000250\n","Epoch:  1438 | train loss: 0.000238 | valid loss: 0.000246\n","Epoch:  1439 | train loss: 0.000274 | valid loss: 0.000247\n","Epoch:  1440 | train loss: 0.000186 | valid loss: 0.000247\n","Epoch:  1441 | train loss: 0.000259 | valid loss: 0.000247\n","Epoch:  1442 | train loss: 0.000200 | valid loss: 0.000247\n","Epoch:  1443 | train loss: 0.000233 | valid loss: 0.000247\n","Epoch:  1444 | train loss: 0.000258 | valid loss: 0.000250\n","Epoch:  1445 | train loss: 0.000216 | valid loss: 0.000247\n","Epoch:  1446 | train loss: 0.000369 | valid loss: 0.000247\n","Epoch:  1447 | train loss: 0.000239 | valid loss: 0.000247\n","Epoch:  1448 | train loss: 0.000287 | valid loss: 0.000247\n","Epoch:  1449 | train loss: 0.000199 | valid loss: 0.000249\n","Epoch:  1450 | train loss: 0.000228 | valid loss: 0.000250\n","Epoch:  1451 | train loss: 0.000184 | valid loss: 0.000247\n","Epoch:  1452 | train loss: 0.000212 | valid loss: 0.000247\n","Epoch:  1453 | train loss: 0.000229 | valid loss: 0.000247\n","Epoch:  1454 | train loss: 0.000281 | valid loss: 0.000247\n","Epoch:  1455 | train loss: 0.000318 | valid loss: 0.000246\n","Epoch:  1456 | train loss: 0.000218 | valid loss: 0.000249\n","Epoch:  1457 | train loss: 0.000244 | valid loss: 0.000249\n","Epoch:  1458 | train loss: 0.000237 | valid loss: 0.000247\n","Epoch:  1459 | train loss: 0.000250 | valid loss: 0.000247\n","Epoch:  1460 | train loss: 0.000258 | valid loss: 0.000246\n","Epoch:  1461 | train loss: 0.000257 | valid loss: 0.000249\n","Epoch:  1462 | train loss: 0.000305 | valid loss: 0.000246\n","Epoch:  1463 | train loss: 0.000281 | valid loss: 0.000247\n","Epoch:  1464 | train loss: 0.000285 | valid loss: 0.000246\n","Epoch:  1465 | train loss: 0.000219 | valid loss: 0.000246\n","Epoch:  1466 | train loss: 0.000261 | valid loss: 0.000249\n","Epoch:  1467 | train loss: 0.000195 | valid loss: 0.000247\n","Epoch:  1468 | train loss: 0.000316 | valid loss: 0.000247\n","Epoch:  1469 | train loss: 0.000334 | valid loss: 0.000246\n","Epoch:  1470 | train loss: 0.000207 | valid loss: 0.000246\n","Epoch:  1471 | train loss: 0.000300 | valid loss: 0.000246\n","Epoch:  1472 | train loss: 0.000270 | valid loss: 0.000246\n","Epoch:  1473 | train loss: 0.000283 | valid loss: 0.000246\n","Epoch:  1474 | train loss: 0.000245 | valid loss: 0.000247\n","Epoch:  1475 | train loss: 0.000280 | valid loss: 0.000247\n","Epoch:  1476 | train loss: 0.000295 | valid loss: 0.000248\n","Epoch:  1477 | train loss: 0.000238 | valid loss: 0.000252\n","Epoch:  1478 | train loss: 0.000292 | valid loss: 0.000248\n","Epoch:  1479 | train loss: 0.000205 | valid loss: 0.000246\n","Epoch:  1480 | train loss: 0.000210 | valid loss: 0.000246\n","Epoch:  1481 | train loss: 0.000282 | valid loss: 0.000246\n","Epoch:  1482 | train loss: 0.000251 | valid loss: 0.000247\n","Epoch:  1483 | train loss: 0.000189 | valid loss: 0.000246\n","Epoch:  1484 | train loss: 0.000195 | valid loss: 0.000247\n","Epoch:  1485 | train loss: 0.000284 | valid loss: 0.000248\n","Epoch:  1486 | train loss: 0.000224 | valid loss: 0.000249\n","Epoch:  1487 | train loss: 0.000261 | valid loss: 0.000246\n","Epoch:  1488 | train loss: 0.000272 | valid loss: 0.000245\n","Epoch:  1489 | train loss: 0.000219 | valid loss: 0.000248\n","Epoch:  1490 | train loss: 0.000231 | valid loss: 0.000247\n","Epoch:  1491 | train loss: 0.000274 | valid loss: 0.000250\n","Epoch:  1492 | train loss: 0.000284 | valid loss: 0.000245\n","Epoch:  1493 | train loss: 0.000295 | valid loss: 0.000247\n","Epoch:  1494 | train loss: 0.000234 | valid loss: 0.000245\n","Epoch:  1495 | train loss: 0.000302 | valid loss: 0.000245\n","Epoch:  1496 | train loss: 0.000330 | valid loss: 0.000247\n","Epoch:  1497 | train loss: 0.000343 | valid loss: 0.000246\n","Epoch:  1498 | train loss: 0.000207 | valid loss: 0.000246\n","Epoch:  1499 | train loss: 0.000263 | valid loss: 0.000245\n","Epoch:  1500 | train loss: 0.000261 | valid loss: 0.000244\n","Epoch:  1501 | train loss: 0.000293 | valid loss: 0.000246\n","Epoch:  1502 | train loss: 0.000212 | valid loss: 0.000245\n","Epoch:  1503 | train loss: 0.000247 | valid loss: 0.000245\n","Epoch:  1504 | train loss: 0.000251 | valid loss: 0.000248\n","Epoch:  1505 | train loss: 0.000196 | valid loss: 0.000245\n","Epoch:  1506 | train loss: 0.000299 | valid loss: 0.000248\n","Epoch:  1507 | train loss: 0.000243 | valid loss: 0.000248\n","Epoch:  1508 | train loss: 0.000292 | valid loss: 0.000245\n","Epoch:  1509 | train loss: 0.000195 | valid loss: 0.000245\n","Epoch:  1510 | train loss: 0.000258 | valid loss: 0.000245\n","Epoch:  1511 | train loss: 0.000247 | valid loss: 0.000245\n","Epoch:  1512 | train loss: 0.000270 | valid loss: 0.000245\n","Epoch:  1513 | train loss: 0.000281 | valid loss: 0.000251\n","Epoch:  1514 | train loss: 0.000188 | valid loss: 0.000246\n","Epoch:  1515 | train loss: 0.000325 | valid loss: 0.000249\n","Epoch:  1516 | train loss: 0.000246 | valid loss: 0.000254\n","Epoch:  1517 | train loss: 0.000239 | valid loss: 0.000244\n","Epoch:  1518 | train loss: 0.000257 | valid loss: 0.000244\n","Epoch:  1519 | train loss: 0.000264 | valid loss: 0.000245\n","Epoch:  1520 | train loss: 0.000194 | valid loss: 0.000244\n","Epoch:  1521 | train loss: 0.000239 | valid loss: 0.000245\n","Epoch:  1522 | train loss: 0.000257 | valid loss: 0.000246\n","Epoch:  1523 | train loss: 0.000240 | valid loss: 0.000247\n","Epoch:  1524 | train loss: 0.000230 | valid loss: 0.000245\n","Epoch:  1525 | train loss: 0.000235 | valid loss: 0.000244\n","Epoch:  1526 | train loss: 0.000267 | valid loss: 0.000246\n","Epoch:  1527 | train loss: 0.000184 | valid loss: 0.000245\n","Epoch:  1528 | train loss: 0.000223 | valid loss: 0.000247\n","Epoch:  1529 | train loss: 0.000227 | valid loss: 0.000244\n","Epoch:  1530 | train loss: 0.000214 | valid loss: 0.000245\n","Epoch:  1531 | train loss: 0.000255 | valid loss: 0.000247\n","Epoch:  1532 | train loss: 0.000262 | valid loss: 0.000247\n","Epoch:  1533 | train loss: 0.000188 | valid loss: 0.000246\n","Epoch:  1534 | train loss: 0.000214 | valid loss: 0.000245\n","Epoch:  1535 | train loss: 0.000279 | valid loss: 0.000245\n","Epoch:  1536 | train loss: 0.000259 | valid loss: 0.000244\n","Epoch:  1537 | train loss: 0.000182 | valid loss: 0.000245\n","Epoch:  1538 | train loss: 0.000261 | valid loss: 0.000245\n","Epoch:  1539 | train loss: 0.000372 | valid loss: 0.000245\n","Epoch:  1540 | train loss: 0.000255 | valid loss: 0.000246\n","Epoch:  1541 | train loss: 0.000239 | valid loss: 0.000249\n","Epoch:  1542 | train loss: 0.000289 | valid loss: 0.000244\n","Epoch:  1543 | train loss: 0.000246 | valid loss: 0.000244\n","Epoch:  1544 | train loss: 0.000251 | valid loss: 0.000244\n","Epoch:  1545 | train loss: 0.000275 | valid loss: 0.000245\n","Epoch:  1546 | train loss: 0.000287 | valid loss: 0.000248\n","Epoch:  1547 | train loss: 0.000356 | valid loss: 0.000248\n","Epoch:  1548 | train loss: 0.000214 | valid loss: 0.000246\n","Epoch:  1549 | train loss: 0.000262 | valid loss: 0.000244\n","Epoch:  1550 | train loss: 0.000279 | valid loss: 0.000248\n","Epoch:  1551 | train loss: 0.000201 | valid loss: 0.000244\n","Epoch:  1552 | train loss: 0.000220 | valid loss: 0.000244\n","Epoch:  1553 | train loss: 0.000247 | valid loss: 0.000243\n","Epoch:  1554 | train loss: 0.000280 | valid loss: 0.000246\n","Epoch:  1555 | train loss: 0.000260 | valid loss: 0.000248\n","Epoch:  1556 | train loss: 0.000221 | valid loss: 0.000252\n","Epoch:  1557 | train loss: 0.000217 | valid loss: 0.000250\n","Epoch:  1558 | train loss: 0.000229 | valid loss: 0.000245\n","Epoch:  1559 | train loss: 0.000256 | valid loss: 0.000248\n","Epoch:  1560 | train loss: 0.000232 | valid loss: 0.000245\n","Epoch:  1561 | train loss: 0.000216 | valid loss: 0.000243\n","Epoch:  1562 | train loss: 0.000326 | valid loss: 0.000245\n","Epoch:  1563 | train loss: 0.000266 | valid loss: 0.000243\n","Epoch:  1564 | train loss: 0.000192 | valid loss: 0.000245\n","Epoch:  1565 | train loss: 0.000224 | valid loss: 0.000244\n","Epoch:  1566 | train loss: 0.000182 | valid loss: 0.000243\n","Epoch:  1567 | train loss: 0.000233 | valid loss: 0.000251\n","Epoch:  1568 | train loss: 0.000213 | valid loss: 0.000244\n","Epoch:  1569 | train loss: 0.000221 | valid loss: 0.000244\n","Epoch:  1570 | train loss: 0.000264 | valid loss: 0.000243\n","Epoch:  1571 | train loss: 0.000284 | valid loss: 0.000244\n","Epoch:  1572 | train loss: 0.000279 | valid loss: 0.000244\n","Epoch:  1573 | train loss: 0.000198 | valid loss: 0.000245\n","Epoch:  1574 | train loss: 0.000183 | valid loss: 0.000244\n","Epoch:  1575 | train loss: 0.000198 | valid loss: 0.000245\n","Epoch:  1576 | train loss: 0.000281 | valid loss: 0.000243\n","Epoch:  1577 | train loss: 0.000285 | valid loss: 0.000244\n","Epoch:  1578 | train loss: 0.000265 | valid loss: 0.000245\n","Epoch:  1579 | train loss: 0.000234 | valid loss: 0.000248\n","Epoch:  1580 | train loss: 0.000211 | valid loss: 0.000244\n","Epoch:  1581 | train loss: 0.000209 | valid loss: 0.000245\n","Epoch:  1582 | train loss: 0.000262 | valid loss: 0.000244\n","Epoch:  1583 | train loss: 0.000297 | valid loss: 0.000243\n","Epoch:  1584 | train loss: 0.000281 | valid loss: 0.000244\n","Epoch:  1585 | train loss: 0.000217 | valid loss: 0.000243\n","Epoch:  1586 | train loss: 0.000207 | valid loss: 0.000246\n","Epoch:  1587 | train loss: 0.000256 | valid loss: 0.000243\n","Epoch:  1588 | train loss: 0.000220 | valid loss: 0.000244\n","Epoch:  1589 | train loss: 0.000250 | valid loss: 0.000247\n","Epoch:  1590 | train loss: 0.000278 | valid loss: 0.000244\n","Epoch:  1591 | train loss: 0.000247 | valid loss: 0.000245\n","Epoch:  1592 | train loss: 0.000235 | valid loss: 0.000243\n","Epoch:  1593 | train loss: 0.000242 | valid loss: 0.000246\n","Epoch:  1594 | train loss: 0.000220 | valid loss: 0.000245\n","Epoch:  1595 | train loss: 0.000283 | valid loss: 0.000245\n","Epoch:  1596 | train loss: 0.000193 | valid loss: 0.000242\n","Epoch:  1597 | train loss: 0.000178 | valid loss: 0.000243\n","Epoch:  1598 | train loss: 0.000225 | valid loss: 0.000242\n","Epoch:  1599 | train loss: 0.000268 | valid loss: 0.000243\n","Epoch:  1600 | train loss: 0.000567 | valid loss: 0.000245\n","Epoch:  1601 | train loss: 0.000245 | valid loss: 0.000244\n","Epoch:  1602 | train loss: 0.000251 | valid loss: 0.000245\n","Epoch:  1603 | train loss: 0.000254 | valid loss: 0.000243\n","Epoch:  1604 | train loss: 0.000193 | valid loss: 0.000244\n","Epoch:  1605 | train loss: 0.000212 | valid loss: 0.000250\n","Epoch:  1606 | train loss: 0.000185 | valid loss: 0.000243\n","Epoch:  1607 | train loss: 0.000215 | valid loss: 0.000242\n","Epoch:  1608 | train loss: 0.000263 | valid loss: 0.000247\n","Epoch:  1609 | train loss: 0.000283 | valid loss: 0.000246\n","Epoch:  1610 | train loss: 0.000192 | valid loss: 0.000242\n","Epoch:  1611 | train loss: 0.000224 | valid loss: 0.000242\n","Epoch:  1612 | train loss: 0.000249 | valid loss: 0.000244\n","Epoch:  1613 | train loss: 0.000294 | valid loss: 0.000242\n","Epoch:  1614 | train loss: 0.000250 | valid loss: 0.000244\n","Epoch:  1615 | train loss: 0.000247 | valid loss: 0.000243\n","Epoch:  1616 | train loss: 0.000265 | valid loss: 0.000243\n","Epoch:  1617 | train loss: 0.000193 | valid loss: 0.000242\n","Epoch:  1618 | train loss: 0.000245 | valid loss: 0.000245\n","Epoch:  1619 | train loss: 0.000180 | valid loss: 0.000242\n","Epoch:  1620 | train loss: 0.000244 | valid loss: 0.000242\n","Epoch:  1621 | train loss: 0.000212 | valid loss: 0.000245\n","Epoch:  1622 | train loss: 0.000179 | valid loss: 0.000242\n","Epoch:  1623 | train loss: 0.000235 | valid loss: 0.000243\n","Epoch:  1624 | train loss: 0.000231 | valid loss: 0.000243\n","Epoch:  1625 | train loss: 0.000279 | valid loss: 0.000244\n","Epoch:  1626 | train loss: 0.000294 | valid loss: 0.000243\n","Epoch:  1627 | train loss: 0.000245 | valid loss: 0.000243\n","Epoch:  1628 | train loss: 0.000191 | valid loss: 0.000243\n","Epoch:  1629 | train loss: 0.000359 | valid loss: 0.000244\n","Epoch:  1630 | train loss: 0.000274 | valid loss: 0.000242\n","Epoch:  1631 | train loss: 0.000214 | valid loss: 0.000243\n","Epoch:  1632 | train loss: 0.000187 | valid loss: 0.000241\n","Epoch:  1633 | train loss: 0.000270 | valid loss: 0.000249\n","Epoch:  1634 | train loss: 0.000322 | valid loss: 0.000243\n","Epoch:  1635 | train loss: 0.000301 | valid loss: 0.000242\n","Epoch:  1636 | train loss: 0.000217 | valid loss: 0.000242\n","Epoch:  1637 | train loss: 0.000264 | valid loss: 0.000243\n","Epoch:  1638 | train loss: 0.000254 | valid loss: 0.000242\n","Epoch:  1639 | train loss: 0.000285 | valid loss: 0.000242\n","Epoch:  1640 | train loss: 0.000244 | valid loss: 0.000244\n","Epoch:  1641 | train loss: 0.000344 | valid loss: 0.000245\n","Epoch:  1642 | train loss: 0.000280 | valid loss: 0.000242\n","Epoch:  1643 | train loss: 0.000184 | valid loss: 0.000242\n","Epoch:  1644 | train loss: 0.000199 | valid loss: 0.000242\n","Epoch:  1645 | train loss: 0.000246 | valid loss: 0.000247\n","Epoch:  1646 | train loss: 0.000265 | valid loss: 0.000243\n","Epoch:  1647 | train loss: 0.000255 | valid loss: 0.000241\n","Epoch:  1648 | train loss: 0.000209 | valid loss: 0.000247\n","Epoch:  1649 | train loss: 0.000246 | valid loss: 0.000241\n","Epoch:  1650 | train loss: 0.000266 | valid loss: 0.000244\n","Epoch:  1651 | train loss: 0.000282 | valid loss: 0.000242\n","Epoch:  1652 | train loss: 0.000276 | valid loss: 0.000242\n","Epoch:  1653 | train loss: 0.000273 | valid loss: 0.000243\n","Epoch:  1654 | train loss: 0.000284 | valid loss: 0.000242\n","Epoch:  1655 | train loss: 0.000236 | valid loss: 0.000242\n","Epoch:  1656 | train loss: 0.000246 | valid loss: 0.000242\n","Epoch:  1657 | train loss: 0.000263 | valid loss: 0.000243\n","Epoch:  1658 | train loss: 0.000335 | valid loss: 0.000241\n","Epoch:  1659 | train loss: 0.000191 | valid loss: 0.000241\n","Epoch:  1660 | train loss: 0.000250 | valid loss: 0.000241\n","Epoch:  1661 | train loss: 0.000321 | valid loss: 0.000242\n","Epoch:  1662 | train loss: 0.000280 | valid loss: 0.000245\n","Epoch:  1663 | train loss: 0.000222 | valid loss: 0.000244\n","Epoch:  1664 | train loss: 0.000226 | valid loss: 0.000241\n","Epoch:  1665 | train loss: 0.000321 | valid loss: 0.000242\n","Epoch:  1666 | train loss: 0.000252 | valid loss: 0.000242\n","Epoch:  1667 | train loss: 0.000253 | valid loss: 0.000243\n","Epoch:  1668 | train loss: 0.000241 | valid loss: 0.000242\n","Epoch:  1669 | train loss: 0.000279 | valid loss: 0.000246\n","Epoch:  1670 | train loss: 0.000278 | valid loss: 0.000244\n","Epoch:  1671 | train loss: 0.000230 | valid loss: 0.000241\n","Epoch:  1672 | train loss: 0.000266 | valid loss: 0.000243\n","Epoch:  1673 | train loss: 0.000241 | valid loss: 0.000242\n","Epoch:  1674 | train loss: 0.000182 | valid loss: 0.000241\n","Epoch:  1675 | train loss: 0.000250 | valid loss: 0.000242\n","Epoch:  1676 | train loss: 0.000188 | valid loss: 0.000241\n","Epoch:  1677 | train loss: 0.000265 | valid loss: 0.000245\n","Epoch:  1678 | train loss: 0.000290 | valid loss: 0.000243\n","Epoch:  1679 | train loss: 0.000277 | valid loss: 0.000243\n","Epoch:  1680 | train loss: 0.000326 | valid loss: 0.000241\n","Epoch:  1681 | train loss: 0.000255 | valid loss: 0.000243\n","Epoch:  1682 | train loss: 0.000285 | valid loss: 0.000249\n","Epoch:  1683 | train loss: 0.000226 | valid loss: 0.000242\n","Epoch:  1684 | train loss: 0.000477 | valid loss: 0.000245\n","Epoch:  1685 | train loss: 0.000290 | valid loss: 0.000241\n","Epoch:  1686 | train loss: 0.000296 | valid loss: 0.000244\n","Epoch:  1687 | train loss: 0.000230 | valid loss: 0.000244\n","Epoch:  1688 | train loss: 0.000246 | valid loss: 0.000241\n","Epoch:  1689 | train loss: 0.000252 | valid loss: 0.000241\n","Epoch:  1690 | train loss: 0.000266 | valid loss: 0.000244\n","Epoch:  1691 | train loss: 0.000273 | valid loss: 0.000243\n","Epoch:  1692 | train loss: 0.000272 | valid loss: 0.000242\n","Epoch:  1693 | train loss: 0.000249 | valid loss: 0.000242\n","Epoch:  1694 | train loss: 0.000233 | valid loss: 0.000242\n","Epoch:  1695 | train loss: 0.000231 | valid loss: 0.000242\n","Epoch:  1696 | train loss: 0.000271 | valid loss: 0.000243\n","Epoch:  1697 | train loss: 0.000250 | valid loss: 0.000242\n","Epoch:  1698 | train loss: 0.000185 | valid loss: 0.000241\n","Epoch:  1699 | train loss: 0.000274 | valid loss: 0.000240\n","Epoch:  1700 | train loss: 0.000241 | valid loss: 0.000240\n","Epoch:  1701 | train loss: 0.000472 | valid loss: 0.000243\n","Epoch:  1702 | train loss: 0.000217 | valid loss: 0.000242\n","Epoch:  1703 | train loss: 0.000276 | valid loss: 0.000241\n","Epoch:  1704 | train loss: 0.000234 | valid loss: 0.000240\n","Epoch:  1705 | train loss: 0.000249 | valid loss: 0.000241\n","Epoch:  1706 | train loss: 0.000284 | valid loss: 0.000242\n","Epoch:  1707 | train loss: 0.000258 | valid loss: 0.000241\n","Epoch:  1708 | train loss: 0.000249 | valid loss: 0.000242\n","Epoch:  1709 | train loss: 0.000243 | valid loss: 0.000240\n","Epoch:  1710 | train loss: 0.000205 | valid loss: 0.000242\n","Epoch:  1711 | train loss: 0.000223 | valid loss: 0.000243\n","Epoch:  1712 | train loss: 0.000214 | valid loss: 0.000244\n","Epoch:  1713 | train loss: 0.000291 | valid loss: 0.000244\n","Epoch:  1714 | train loss: 0.000229 | valid loss: 0.000241\n","Epoch:  1715 | train loss: 0.000247 | valid loss: 0.000241\n","Epoch:  1716 | train loss: 0.000229 | valid loss: 0.000242\n","Epoch:  1717 | train loss: 0.000217 | valid loss: 0.000240\n","Epoch:  1718 | train loss: 0.000246 | valid loss: 0.000240\n","Epoch:  1719 | train loss: 0.000245 | valid loss: 0.000241\n","Epoch:  1720 | train loss: 0.000298 | valid loss: 0.000241\n","Epoch:  1721 | train loss: 0.000355 | valid loss: 0.000243\n","Epoch:  1722 | train loss: 0.000178 | valid loss: 0.000240\n","Epoch:  1723 | train loss: 0.000239 | valid loss: 0.000243\n","Epoch:  1724 | train loss: 0.000248 | valid loss: 0.000240\n","Epoch:  1725 | train loss: 0.000274 | valid loss: 0.000242\n","Epoch:  1726 | train loss: 0.000216 | valid loss: 0.000240\n","Epoch:  1727 | train loss: 0.000260 | valid loss: 0.000241\n","Epoch:  1728 | train loss: 0.000234 | valid loss: 0.000241\n","Epoch:  1729 | train loss: 0.000200 | valid loss: 0.000241\n","Epoch:  1730 | train loss: 0.000201 | valid loss: 0.000242\n","Epoch:  1731 | train loss: 0.000274 | valid loss: 0.000240\n","Epoch:  1732 | train loss: 0.000204 | valid loss: 0.000246\n","Epoch:  1733 | train loss: 0.000225 | valid loss: 0.000241\n","Epoch:  1734 | train loss: 0.000259 | valid loss: 0.000240\n","Epoch:  1735 | train loss: 0.000257 | valid loss: 0.000240\n","Epoch:  1736 | train loss: 0.000180 | valid loss: 0.000240\n","Epoch:  1737 | train loss: 0.000228 | valid loss: 0.000241\n","Epoch:  1738 | train loss: 0.000231 | valid loss: 0.000240\n","Epoch:  1739 | train loss: 0.000244 | valid loss: 0.000242\n","Epoch:  1740 | train loss: 0.000227 | valid loss: 0.000240\n","Epoch:  1741 | train loss: 0.000257 | valid loss: 0.000241\n","Epoch:  1742 | train loss: 0.000213 | valid loss: 0.000240\n","Epoch:  1743 | train loss: 0.000188 | valid loss: 0.000241\n","Epoch:  1744 | train loss: 0.000263 | valid loss: 0.000241\n","Epoch:  1745 | train loss: 0.000311 | valid loss: 0.000240\n","Epoch:  1746 | train loss: 0.000199 | valid loss: 0.000240\n","Epoch:  1747 | train loss: 0.000275 | valid loss: 0.000242\n","Epoch:  1748 | train loss: 0.000254 | valid loss: 0.000243\n","Epoch:  1749 | train loss: 0.000249 | valid loss: 0.000241\n","Epoch:  1750 | train loss: 0.000242 | valid loss: 0.000239\n","Epoch:  1751 | train loss: 0.000264 | valid loss: 0.000242\n","Epoch:  1752 | train loss: 0.000300 | valid loss: 0.000241\n","Epoch:  1753 | train loss: 0.000234 | valid loss: 0.000240\n","Epoch:  1754 | train loss: 0.000260 | valid loss: 0.000241\n","Epoch:  1755 | train loss: 0.000234 | valid loss: 0.000240\n","Epoch:  1756 | train loss: 0.000286 | valid loss: 0.000244\n","Epoch:  1757 | train loss: 0.000303 | valid loss: 0.000241\n","Epoch:  1758 | train loss: 0.000254 | valid loss: 0.000241\n","Epoch:  1759 | train loss: 0.000246 | valid loss: 0.000240\n","Epoch:  1760 | train loss: 0.000274 | valid loss: 0.000245\n","Epoch:  1761 | train loss: 0.000253 | valid loss: 0.000240\n","Epoch:  1762 | train loss: 0.000297 | valid loss: 0.000240\n","Epoch:  1763 | train loss: 0.000191 | valid loss: 0.000239\n","Epoch:  1764 | train loss: 0.000277 | valid loss: 0.000241\n","Epoch:  1765 | train loss: 0.000253 | valid loss: 0.000245\n","Epoch:  1766 | train loss: 0.000199 | valid loss: 0.000239\n","Epoch:  1767 | train loss: 0.000277 | valid loss: 0.000240\n","Epoch:  1768 | train loss: 0.000244 | valid loss: 0.000240\n","Epoch:  1769 | train loss: 0.000240 | valid loss: 0.000243\n","Epoch:  1770 | train loss: 0.000247 | valid loss: 0.000242\n","Epoch:  1771 | train loss: 0.000231 | valid loss: 0.000242\n","Epoch:  1772 | train loss: 0.000201 | valid loss: 0.000239\n","Epoch:  1773 | train loss: 0.000268 | valid loss: 0.000239\n","Epoch:  1774 | train loss: 0.000214 | valid loss: 0.000238\n","Epoch:  1775 | train loss: 0.000274 | valid loss: 0.000243\n","Epoch:  1776 | train loss: 0.000240 | valid loss: 0.000240\n","Epoch:  1777 | train loss: 0.000233 | valid loss: 0.000240\n","Epoch:  1778 | train loss: 0.000245 | valid loss: 0.000241\n","Epoch:  1779 | train loss: 0.000245 | valid loss: 0.000242\n","Epoch:  1780 | train loss: 0.000249 | valid loss: 0.000239\n","Epoch:  1781 | train loss: 0.000186 | valid loss: 0.000239\n","Epoch:  1782 | train loss: 0.000278 | valid loss: 0.000239\n","Epoch:  1783 | train loss: 0.000172 | valid loss: 0.000240\n","Epoch:  1784 | train loss: 0.000215 | valid loss: 0.000242\n","Epoch:  1785 | train loss: 0.000227 | valid loss: 0.000240\n","Epoch:  1786 | train loss: 0.000185 | valid loss: 0.000239\n","Epoch:  1787 | train loss: 0.000184 | valid loss: 0.000241\n","Epoch:  1788 | train loss: 0.000294 | valid loss: 0.000240\n","Epoch:  1789 | train loss: 0.000244 | valid loss: 0.000242\n","Epoch:  1790 | train loss: 0.000291 | valid loss: 0.000246\n","Epoch:  1791 | train loss: 0.000297 | valid loss: 0.000238\n","Epoch:  1792 | train loss: 0.000310 | valid loss: 0.000239\n","Epoch:  1793 | train loss: 0.000210 | valid loss: 0.000238\n","Epoch:  1794 | train loss: 0.000220 | valid loss: 0.000239\n","Epoch:  1795 | train loss: 0.000249 | valid loss: 0.000239\n","Epoch:  1796 | train loss: 0.000267 | valid loss: 0.000239\n","Epoch:  1797 | train loss: 0.000190 | valid loss: 0.000240\n","Epoch:  1798 | train loss: 0.000210 | valid loss: 0.000239\n","Epoch:  1799 | train loss: 0.000241 | valid loss: 0.000240\n","Epoch:  1800 | train loss: 0.000201 | valid loss: 0.000239\n","Epoch:  1801 | train loss: 0.000280 | valid loss: 0.000247\n","Epoch:  1802 | train loss: 0.000223 | valid loss: 0.000242\n","Epoch:  1803 | train loss: 0.000192 | valid loss: 0.000239\n","Epoch:  1804 | train loss: 0.000274 | valid loss: 0.000240\n","Epoch:  1805 | train loss: 0.000223 | valid loss: 0.000239\n","Epoch:  1806 | train loss: 0.000201 | valid loss: 0.000239\n","Epoch:  1807 | train loss: 0.000237 | valid loss: 0.000238\n","Epoch:  1808 | train loss: 0.000226 | valid loss: 0.000239\n","Epoch:  1809 | train loss: 0.000230 | valid loss: 0.000239\n","Epoch:  1810 | train loss: 0.000272 | valid loss: 0.000240\n","Epoch:  1811 | train loss: 0.000208 | valid loss: 0.000238\n","Epoch:  1812 | train loss: 0.000283 | valid loss: 0.000239\n","Epoch:  1813 | train loss: 0.000234 | valid loss: 0.000239\n","Epoch:  1814 | train loss: 0.000241 | valid loss: 0.000238\n","Epoch:  1815 | train loss: 0.000238 | valid loss: 0.000240\n","Epoch:  1816 | train loss: 0.000241 | valid loss: 0.000239\n","Epoch:  1817 | train loss: 0.000203 | valid loss: 0.000240\n","Epoch:  1818 | train loss: 0.000317 | valid loss: 0.000240\n","Epoch:  1819 | train loss: 0.000199 | valid loss: 0.000241\n","Epoch:  1820 | train loss: 0.000325 | valid loss: 0.000240\n","Epoch:  1821 | train loss: 0.000257 | valid loss: 0.000238\n","Epoch:  1822 | train loss: 0.000260 | valid loss: 0.000242\n","Epoch:  1823 | train loss: 0.000324 | valid loss: 0.000238\n","Epoch:  1824 | train loss: 0.000310 | valid loss: 0.000238\n","Epoch:  1825 | train loss: 0.000269 | valid loss: 0.000238\n","Epoch:  1826 | train loss: 0.000261 | valid loss: 0.000239\n","Epoch:  1827 | train loss: 0.000207 | valid loss: 0.000238\n","Epoch:  1828 | train loss: 0.000213 | valid loss: 0.000239\n","Epoch:  1829 | train loss: 0.000230 | valid loss: 0.000240\n","Epoch:  1830 | train loss: 0.000274 | valid loss: 0.000239\n","Epoch:  1831 | train loss: 0.000200 | valid loss: 0.000240\n","Epoch:  1832 | train loss: 0.000194 | valid loss: 0.000239\n","Epoch:  1833 | train loss: 0.000321 | valid loss: 0.000240\n","Epoch:  1834 | train loss: 0.000216 | valid loss: 0.000238\n","Epoch:  1835 | train loss: 0.000237 | valid loss: 0.000238\n","Epoch:  1836 | train loss: 0.000221 | valid loss: 0.000238\n","Epoch:  1837 | train loss: 0.000285 | valid loss: 0.000240\n","Epoch:  1838 | train loss: 0.000292 | valid loss: 0.000240\n","Epoch:  1839 | train loss: 0.000273 | valid loss: 0.000238\n","Epoch:  1840 | train loss: 0.000200 | valid loss: 0.000240\n","Epoch:  1841 | train loss: 0.000303 | valid loss: 0.000238\n","Epoch:  1842 | train loss: 0.000273 | valid loss: 0.000240\n","Epoch:  1843 | train loss: 0.000250 | valid loss: 0.000242\n","Epoch:  1844 | train loss: 0.000202 | valid loss: 0.000240\n","Epoch:  1845 | train loss: 0.000244 | valid loss: 0.000241\n","Epoch:  1846 | train loss: 0.000222 | valid loss: 0.000240\n","Epoch:  1847 | train loss: 0.000319 | valid loss: 0.000239\n","Epoch:  1848 | train loss: 0.000223 | valid loss: 0.000239\n","Epoch:  1849 | train loss: 0.000245 | valid loss: 0.000237\n","Epoch:  1850 | train loss: 0.000266 | valid loss: 0.000239\n","Epoch:  1851 | train loss: 0.000256 | valid loss: 0.000238\n","Epoch:  1852 | train loss: 0.000184 | valid loss: 0.000238\n","Epoch:  1853 | train loss: 0.000240 | valid loss: 0.000238\n","Epoch:  1854 | train loss: 0.000190 | valid loss: 0.000237\n","Epoch:  1855 | train loss: 0.000289 | valid loss: 0.000238\n","Epoch:  1856 | train loss: 0.000229 | valid loss: 0.000237\n","Epoch:  1857 | train loss: 0.000252 | valid loss: 0.000240\n","Epoch:  1858 | train loss: 0.000223 | valid loss: 0.000237\n","Epoch:  1859 | train loss: 0.000255 | valid loss: 0.000240\n","Epoch:  1860 | train loss: 0.000252 | valid loss: 0.000238\n","Epoch:  1861 | train loss: 0.000224 | valid loss: 0.000239\n","Epoch:  1862 | train loss: 0.000240 | valid loss: 0.000240\n","Epoch:  1863 | train loss: 0.000202 | valid loss: 0.000238\n","Epoch:  1864 | train loss: 0.000231 | valid loss: 0.000237\n","Epoch:  1865 | train loss: 0.000254 | valid loss: 0.000238\n","Epoch:  1866 | train loss: 0.000235 | valid loss: 0.000240\n","Epoch:  1867 | train loss: 0.000234 | valid loss: 0.000239\n","Epoch:  1868 | train loss: 0.000272 | valid loss: 0.000237\n","Epoch:  1869 | train loss: 0.000303 | valid loss: 0.000238\n","Epoch:  1870 | train loss: 0.000240 | valid loss: 0.000238\n","Epoch:  1871 | train loss: 0.000253 | valid loss: 0.000239\n","Epoch:  1872 | train loss: 0.000209 | valid loss: 0.000238\n","Epoch:  1873 | train loss: 0.000205 | valid loss: 0.000238\n","Epoch:  1874 | train loss: 0.000233 | valid loss: 0.000238\n","Epoch:  1875 | train loss: 0.000242 | valid loss: 0.000237\n","Epoch:  1876 | train loss: 0.000204 | valid loss: 0.000239\n","Epoch:  1877 | train loss: 0.000257 | valid loss: 0.000237\n","Epoch:  1878 | train loss: 0.000244 | valid loss: 0.000237\n","Epoch:  1879 | train loss: 0.000227 | valid loss: 0.000237\n","Epoch:  1880 | train loss: 0.000269 | valid loss: 0.000238\n","Epoch:  1881 | train loss: 0.000204 | valid loss: 0.000239\n","Epoch:  1882 | train loss: 0.000244 | valid loss: 0.000238\n","Epoch:  1883 | train loss: 0.000226 | valid loss: 0.000238\n","Epoch:  1884 | train loss: 0.000290 | valid loss: 0.000240\n","Epoch:  1885 | train loss: 0.000255 | valid loss: 0.000238\n","Epoch:  1886 | train loss: 0.000207 | valid loss: 0.000241\n","Epoch:  1887 | train loss: 0.000202 | valid loss: 0.000241\n","Epoch:  1888 | train loss: 0.000233 | valid loss: 0.000239\n","Epoch:  1889 | train loss: 0.000191 | valid loss: 0.000236\n","Epoch:  1890 | train loss: 0.000221 | valid loss: 0.000238\n","Epoch:  1891 | train loss: 0.000311 | valid loss: 0.000238\n","Epoch:  1892 | train loss: 0.000266 | valid loss: 0.000241\n","Epoch:  1893 | train loss: 0.000205 | valid loss: 0.000237\n","Epoch:  1894 | train loss: 0.000289 | valid loss: 0.000239\n","Epoch:  1895 | train loss: 0.000261 | valid loss: 0.000242\n","Epoch:  1896 | train loss: 0.000254 | valid loss: 0.000240\n","Epoch:  1897 | train loss: 0.000249 | valid loss: 0.000237\n","Epoch:  1898 | train loss: 0.000313 | valid loss: 0.000239\n","Epoch:  1899 | train loss: 0.000226 | valid loss: 0.000237\n","Epoch:  1900 | train loss: 0.000297 | valid loss: 0.000238\n","Epoch:  1901 | train loss: 0.000228 | valid loss: 0.000238\n","Epoch:  1902 | train loss: 0.000219 | valid loss: 0.000238\n","Epoch:  1903 | train loss: 0.000175 | valid loss: 0.000237\n","Epoch:  1904 | train loss: 0.000248 | valid loss: 0.000241\n","Epoch:  1905 | train loss: 0.000219 | valid loss: 0.000238\n","Epoch:  1906 | train loss: 0.000479 | valid loss: 0.000239\n","Epoch:  1907 | train loss: 0.000244 | valid loss: 0.000238\n","Epoch:  1908 | train loss: 0.000206 | valid loss: 0.000236\n","Epoch:  1909 | train loss: 0.000251 | valid loss: 0.000237\n","Epoch:  1910 | train loss: 0.000272 | valid loss: 0.000236\n","Epoch:  1911 | train loss: 0.000222 | valid loss: 0.000236\n","Epoch:  1912 | train loss: 0.000279 | valid loss: 0.000238\n","Epoch:  1913 | train loss: 0.000284 | valid loss: 0.000238\n","Epoch:  1914 | train loss: 0.000235 | valid loss: 0.000240\n","Epoch:  1915 | train loss: 0.000254 | valid loss: 0.000242\n","Epoch:  1916 | train loss: 0.000347 | valid loss: 0.000237\n","Epoch:  1917 | train loss: 0.000223 | valid loss: 0.000236\n","Epoch:  1918 | train loss: 0.000255 | valid loss: 0.000236\n","Epoch:  1919 | train loss: 0.000204 | valid loss: 0.000237\n","Epoch:  1920 | train loss: 0.000225 | valid loss: 0.000237\n","Epoch:  1921 | train loss: 0.000249 | valid loss: 0.000241\n","Epoch:  1922 | train loss: 0.000284 | valid loss: 0.000239\n","Epoch:  1923 | train loss: 0.000205 | valid loss: 0.000237\n","Epoch:  1924 | train loss: 0.000283 | valid loss: 0.000238\n","Epoch:  1925 | train loss: 0.000189 | valid loss: 0.000242\n","Epoch:  1926 | train loss: 0.000226 | valid loss: 0.000238\n","Epoch:  1927 | train loss: 0.000219 | valid loss: 0.000237\n","Epoch:  1928 | train loss: 0.000203 | valid loss: 0.000236\n","Epoch:  1929 | train loss: 0.000191 | valid loss: 0.000238\n","Epoch:  1930 | train loss: 0.000282 | valid loss: 0.000236\n","Epoch:  1931 | train loss: 0.000181 | valid loss: 0.000239\n","Epoch:  1932 | train loss: 0.000296 | valid loss: 0.000237\n","Epoch:  1933 | train loss: 0.000208 | valid loss: 0.000238\n","Epoch:  1934 | train loss: 0.000226 | valid loss: 0.000237\n","Epoch:  1935 | train loss: 0.000258 | valid loss: 0.000236\n","Epoch:  1936 | train loss: 0.000250 | valid loss: 0.000235\n","Epoch:  1937 | train loss: 0.000190 | valid loss: 0.000236\n","Epoch:  1938 | train loss: 0.000191 | valid loss: 0.000239\n","Epoch:  1939 | train loss: 0.000248 | valid loss: 0.000241\n","Epoch:  1940 | train loss: 0.000227 | valid loss: 0.000239\n","Epoch:  1941 | train loss: 0.000257 | valid loss: 0.000238\n","Epoch:  1942 | train loss: 0.000204 | valid loss: 0.000237\n","Epoch:  1943 | train loss: 0.000207 | valid loss: 0.000235\n","Epoch:  1944 | train loss: 0.000209 | valid loss: 0.000237\n","Epoch:  1945 | train loss: 0.000247 | valid loss: 0.000236\n","Epoch:  1946 | train loss: 0.000238 | valid loss: 0.000236\n","Epoch:  1947 | train loss: 0.000214 | valid loss: 0.000241\n","Epoch:  1948 | train loss: 0.000238 | valid loss: 0.000236\n","Epoch:  1949 | train loss: 0.000244 | valid loss: 0.000238\n","Epoch:  1950 | train loss: 0.000241 | valid loss: 0.000238\n","Epoch:  1951 | train loss: 0.000202 | valid loss: 0.000237\n","Epoch:  1952 | train loss: 0.000206 | valid loss: 0.000236\n","Epoch:  1953 | train loss: 0.000190 | valid loss: 0.000236\n","Epoch:  1954 | train loss: 0.000188 | valid loss: 0.000236\n","Epoch:  1955 | train loss: 0.000198 | valid loss: 0.000238\n","Epoch:  1956 | train loss: 0.000248 | valid loss: 0.000237\n","Epoch:  1957 | train loss: 0.000238 | valid loss: 0.000236\n","Epoch:  1958 | train loss: 0.000215 | valid loss: 0.000236\n","Epoch:  1959 | train loss: 0.000275 | valid loss: 0.000235\n","Epoch:  1960 | train loss: 0.000247 | valid loss: 0.000236\n","Epoch:  1961 | train loss: 0.000239 | valid loss: 0.000237\n","Epoch:  1962 | train loss: 0.000242 | valid loss: 0.000238\n","Epoch:  1963 | train loss: 0.000267 | valid loss: 0.000237\n","Epoch:  1964 | train loss: 0.000241 | valid loss: 0.000236\n","Epoch:  1965 | train loss: 0.000268 | valid loss: 0.000236\n","Epoch:  1966 | train loss: 0.000219 | valid loss: 0.000237\n","Epoch:  1967 | train loss: 0.000314 | valid loss: 0.000238\n","Epoch:  1968 | train loss: 0.000266 | valid loss: 0.000239\n","Epoch:  1969 | train loss: 0.000231 | valid loss: 0.000238\n","Epoch:  1970 | train loss: 0.000200 | valid loss: 0.000236\n","Epoch:  1971 | train loss: 0.000180 | valid loss: 0.000237\n","Epoch:  1972 | train loss: 0.000200 | valid loss: 0.000238\n","Epoch:  1973 | train loss: 0.000204 | valid loss: 0.000237\n","Epoch:  1974 | train loss: 0.000213 | valid loss: 0.000235\n","Epoch:  1975 | train loss: 0.000248 | valid loss: 0.000238\n","Epoch:  1976 | train loss: 0.000281 | valid loss: 0.000238\n","Epoch:  1977 | train loss: 0.000294 | valid loss: 0.000236\n","Epoch:  1978 | train loss: 0.000253 | valid loss: 0.000238\n","Epoch:  1979 | train loss: 0.000236 | valid loss: 0.000236\n","Epoch:  1980 | train loss: 0.000228 | valid loss: 0.000235\n","Epoch:  1981 | train loss: 0.000251 | valid loss: 0.000244\n","Epoch:  1982 | train loss: 0.000226 | valid loss: 0.000237\n","Epoch:  1983 | train loss: 0.000278 | valid loss: 0.000235\n","Epoch:  1984 | train loss: 0.000242 | valid loss: 0.000235\n","Epoch:  1985 | train loss: 0.000225 | valid loss: 0.000236\n","Epoch:  1986 | train loss: 0.000264 | valid loss: 0.000235\n","Epoch:  1987 | train loss: 0.000227 | valid loss: 0.000239\n","Epoch:  1988 | train loss: 0.000212 | valid loss: 0.000237\n","Epoch:  1989 | train loss: 0.000190 | valid loss: 0.000237\n","Epoch:  1990 | train loss: 0.000208 | valid loss: 0.000236\n","Epoch:  1991 | train loss: 0.000256 | valid loss: 0.000235\n","Epoch:  1992 | train loss: 0.000227 | valid loss: 0.000235\n","Epoch:  1993 | train loss: 0.000229 | valid loss: 0.000240\n","Epoch:  1994 | train loss: 0.000319 | valid loss: 0.000235\n","Epoch:  1995 | train loss: 0.000258 | valid loss: 0.000238\n","Epoch:  1996 | train loss: 0.000182 | valid loss: 0.000235\n","Epoch:  1997 | train loss: 0.000225 | valid loss: 0.000237\n","Epoch:  1998 | train loss: 0.000174 | valid loss: 0.000237\n","Epoch:  1999 | train loss: 0.000362 | valid loss: 0.000236\n","Epoch:  2000 | train loss: 0.000228 | valid loss: 0.000236\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRrk0nwejFUz","executionInfo":{"status":"ok","timestamp":1628060525368,"user_tz":-60,"elapsed":3,"user":{"displayName":"杨钒","photoUrl":"","userId":"12928489296811262671"}},"outputId":"cf34bbd0-7c6c-4fae-aa46-5120d60acf85"},"source":["print(t_train_1-t_train_0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["23849.998926639557\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IKqsMAc0-q_e"},"source":["### Save loss and plot"]},{"cell_type":"code","metadata":{"id":"r5b_let1kFGs","colab":{"base_uri":"https://localhost:8080/","height":431},"executionInfo":{"status":"error","timestamp":1629603763996,"user_tz":-60,"elapsed":591,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"f575f97d-e5b6-4713-9b75-60d1b6870034"},"source":["pathName = \"./HAE/csv/2_Eran2000_LV170_B16_n1600_L0.0001.csv\"\n","name = \"SFC-HAE MSE loss of 170 compression variables\"\n","PlotMSELoss(pathName,name)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-1426a80de295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpathName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./HAE/csv/2_Eran2000_LV170_B16_n1600_L0.0001.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"HCAE MSE loss of 170 compression variables\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mPlotMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-71489a728af4>\u001b[0m in \u001b[0;36mPlotMSELoss\u001b[0;34m(pathName, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPlotMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './HAE/csv/2_Eran2000_LV170_B16_n1600_L0.0001.csv'"]}]},{"cell_type":"code","metadata":{"id":"2daz7wP_79YN"},"source":["autoencoder_2 = torch.load(\"./HAE/pkl/2_Eran2000_LV170_B16_n1600_L0.0001.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l40y8mXMBiT_"},"source":["### Calculate MSE"]},{"cell_type":"code","metadata":{"id":"NC0CNUzo-s_m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629604031994,"user_tz":-60,"elapsed":6319,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"a4249697-559d-44b7-a6f0-e0830c2b3aa1"},"source":["# pass training, validation and test data through the autoencoder\n","t_predict_0 = time.time()\n","\n","mode_2train, training_decoded_2 = autoencoder_2.to(device)(torch.tensor(training_data).to(device),mode_1train.float().to(device))\n","error_autoencoder = (training_decoded_2.cpu().detach().numpy() - training_data[:,:,3:5])\n","print(\"MSE_err of training data\", (error_autoencoder**2).mean())\n","\n","mode_2valid, valid_decoded_2 = autoencoder_2.to(device)(torch.tensor(valid_data).to(device),mode_1valid.float().to(device))\n","error_autoencoder = (valid_decoded_2.cpu().detach().numpy() - valid_data[:, :, 3:5])\n","print(\"Mse_err of validation data\", (error_autoencoder**2).mean())\n","\n","mode_2test, test_decoded_2 = autoencoder_2.to(device)(torch.tensor(test_data).to(device),mode_1test.float().to(device))\n","error_autoencoder = (test_decoded_2.cpu().detach().numpy() - test_data[:, :, 3:5])\n","print(\"Mse_err of test data\", (error_autoencoder**2).mean())\n","\n","# total_decoded2 = getTotal_decoded(training_decoded_2,valid_decoded_2,test_decoded_2,train_index,valid_index,test_index)\n","# error_autoencoder = (total_decoded2 - total_data[:, :, 3:5])\n","# print(\"Mse_err of total data\", (error_autoencoder**2).mean())\n","\n","print(mode_2train.shape)\n","print(mode_2valid.shape)\n","print(mode_2test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["MSE_err of training data 0.00024124998088607434\n","Mse_err of validation data 0.00023550184804158695\n","Mse_err of test data 0.00023022214807142663\n","torch.Size([1600, 170])\n","torch.Size([200, 170])\n","torch.Size([200, 170])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SAiYFCbM_PF5"},"source":["### Get mode"]},{"cell_type":"code","metadata":{"id":"HNEM09BXOZzE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629606470276,"user_tz":-60,"elapsed":5,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"74cb387a-ace5-4978-fda7-e02117087ae0"},"source":["Latent_num = 170\n","torch.manual_seed(42)\n","BATCH_SIZE = 16\n","LR = 0.0001\n","nTrain = 1600\n","\n","path_train = \"./HAE/mode_new/II_I_mode2_LV\"+str(Latent_num)+\"_Eran\"+str(2000) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\"_train.csv\"\n","path_valid = \"./HAE/mode_new/II_I_mode2_LV\"+str(Latent_num)+\"_Eran\"+str(2000) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\"_valid.csv\"\n","path_test = \"./HAE/mode_new/II_I_mode2_LV\"+str(Latent_num)+\"_Eran\"+str(2000) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\"_test.csv\"\n","print(path_train)\n","# saveMode(path_train,path_valid,path_test,mode_2train,mode_2valid,mode_2test)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["./HAE/mode_new/II_I_mode2_LV170_Eran2000_B16_n1600_L0.0001_train.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jx1SQSvxOAE2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629606487858,"user_tz":-60,"elapsed":17586,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"e4fdd663-0e54-46d1-c298-913853010d44"},"source":["mode_2train,mode_2valid,mode_2test = getMode(path_train,path_valid,path_test)\n","mode_2train = torch.from_numpy(mode_2train).to(device)\n","mode_2valid = torch.from_numpy(mode_2valid).to(device)\n","mode_2test = torch.from_numpy(mode_2test).to(device)\n","\n","print(mode_2train.shape)\n","print(mode_2test.shape)\n","print(mode_2valid.shape)\n","print(mode_2valid)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["torch.Size([1600, 170])\n","torch.Size([200, 170])\n","torch.Size([200, 170])\n","tensor([[-0.8740, -0.9048, -0.9133,  ..., -0.0548, -0.0548, -0.0902],\n","        [-0.8414, -0.9251, -0.8809,  ..., -0.0346, -0.1521,  0.0833],\n","        [-0.9248, -0.8965, -0.9035,  ...,  0.0942,  0.0981, -0.0380],\n","        ...,\n","        [-0.8939, -0.9025, -0.9134,  ...,  0.0741,  0.1380, -0.0084],\n","        [-0.9099, -0.8965, -0.9114,  ...,  0.0851,  0.1180, -0.2118],\n","        [-0.9107, -0.9103, -0.9186,  ...,  0.0725,  0.1835, -0.0982]],\n","       device='cuda:0', dtype=torch.float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LonTRLAIAphl"},"source":["## Third network"]},{"cell_type":"markdown","metadata":{"id":"PPoxGeLCAusY"},"source":["### Network structure"]},{"cell_type":"code","metadata":{"id":"obFYlnxTAruH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629606487859,"user_tz":-60,"elapsed":16,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"f8424376-08e3-49db-a007-ae694b59c605"},"source":["# SFC-HAE: one curve with nearest neighbour smoothing and compressing to 128 latent variables\n","print(\"compress to 256\")\n","torch.manual_seed(42)\n","# Hyper-parameters\n","Latent_num = 256\n","EPOCH = 2001\n","BATCH_SIZE = 16\n","LR = 0.0001\n","k = nNodes # number of nodes - this has to match training_data.shape[0]\n","print(training_data.shape) # nTrain by number of nodes by 5\n","\n","# Combing the input data and the mode\n","train_set = TensorDataset(torch.from_numpy(training_data), mode_2train)\n","\n","# Data Loader for easy mini-batch return in training\n","train_loader = Data.DataLoader(dataset = train_set, batch_size =BATCH_SIZE , shuffle = True)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["compress to 256\n","(1600, 20550, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H3JOCadJAx1p","executionInfo":{"status":"ok","timestamp":1629606487859,"user_tz":-60,"elapsed":13,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["class CNN_3(nn.Module):\n","    def __init__(self):\n","        super(CNN_3, self).__init__()\n","        self.encoder_h1 = nn.Sequential(\n","            nn.Tanh(),\n","            nn.Conv1d(4, 16, 32, 2, 16),\n","            nn.Tanh(),\n","            nn.MaxPool1d(3,stride = 2),\n","            nn.Conv1d(16, 16, 32, 2, 16),\n","            nn.Tanh(),\n","            nn.MaxPool1d(3,stride = 2),\n","            nn.Conv1d(16, 16, 32, 2, 16),\n","            nn.Tanh(),\n","            nn.MaxPool1d(3,stride = 2),\n","            nn.Conv1d(16, 16, 32, 2, 16),\n","            nn.Tanh(),\n","            nn.MaxPool1d(3,stride =2),\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(16*80, 86),\n","            nn.Tanh(),\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(86+170, 16*80),\n","            nn.Tanh(),\n","        )\n","        self.decoder_h1 = nn.Sequential(\n","            nn.Tanh(),\n","            nn.Upsample(scale_factor =2, mode=\"nearest\"),\n","            nn.ConvTranspose1d(16, 16, 32, 2, 14), \n","            nn.Tanh(),\n","            nn.Upsample(scale_factor =2, mode=\"nearest\"),\n","            nn.ConvTranspose1d(16, 16, 32, 2, 16), \n","            nn.Tanh(),\n","            nn.Upsample(scale_factor =2, mode=\"nearest\"),\n","            nn.ConvTranspose1d(16, 16, 32, 2, 18), \n","            nn.Tanh(),\n","            nn.Upsample(scale_factor =2, mode=\"nearest\"),\n","            nn.ConvTranspose1d(16, 4, 32, 2, 16), \n","            nn.Tanh(),\n","        )\n","\n","        # input sparse layers, initialize weight as 0.33, bias as 0\n","        self.weight1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight1_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight1_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias1 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight11 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight11_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight11_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias11 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight2 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight2_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight2_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias2 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight22 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight22_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight22_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias22 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.weight3 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight3_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight3_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias3 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight33 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight33_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight33_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias33 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight4 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight4_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight4_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias4 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        self.weight44 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight44_0 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.weight44_1 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.ones(k)),requires_grad = True)\n","        self.bias44 = torch.nn.Parameter(torch.FloatTensor(0.33 * torch.zeros(k)),requires_grad = True)\n","        \n","        # output sparse layers, initialize weight as 0.083, bias as 0\n","        self.weight_out1 = torch.nn.Parameter(torch.FloatTensor(0.083 *torch.ones(k)),requires_grad = True) \n","        self.weight_out1_0 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True) \n","        self.weight_out1_1 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out11 = torch.nn.Parameter(torch.FloatTensor(0.083 *torch.ones(k)),requires_grad = True) \n","        self.weight_out11_0 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True) \n","        self.weight_out11_1 = torch.nn.Parameter(torch.FloatTensor(0.083* torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out2 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out2_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out2_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out22 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out22_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        self.weight_out22_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.weight_out3 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out3_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out3_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out33 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out33_0 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out33_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out4 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out4_0= torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out4_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        \n","        self.weight_out44 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out44_0= torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True) \n","        self.weight_out44_1 = torch.nn.Parameter(torch.FloatTensor(0.083 * torch.ones(k)),requires_grad = True)\n","        \n","        self.bias_out1 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","        self.bias_out2 = torch.nn.Parameter(torch.FloatTensor(torch.zeros(k)),requires_grad = True)\n","\n","\n","    def forward(self, x, mode):\n","        # print(\"X_size\",x.size())\n","        # first curve\n","        ToSFC1 = x[:, :, 0] # sfc indices\n","        ToSFC1Up = torch.zeros_like(ToSFC1)\n","        ToSFC1Down = torch.zeros_like(ToSFC1)\n","        ToSFC1Up[:-1] = ToSFC1[1:]\n","        ToSFC1Up[-1] = ToSFC1[-1]\n","        ToSFC1Down[1:] = ToSFC1[:-1]\n","        ToSFC1Down[0] = ToSFC1[0]\n","\n","        batch_num = ToSFC1.shape[0]\n","        #print(\"ToSFC1\",ToSFC1.shape) # (16, 20550)\n","        x1 = x[:, :, 3:5] # u and v\n","        #print(\"x1\", x1.shape) #        # (16, 20550, 2)\n","        x1_1d = torch.zeros((batch_num, 4, k)).to(device)\n","        # first input sparse layer, then transform to sfc order1\n","        for j in range(batch_num):\n","            x1_1d[j, 0, :] = x1[j, :, 0][ToSFC1[j].long()] * self.weight1 + \\\n","                             x1[j, :, 0][ToSFC1Up[j].long()] * self.weight1_0 + \\\n","                             x1[j, :, 0][ToSFC1Down[j].long()] * self.weight1_1 + self.bias1\n","        \n","            x1_1d[j, 1, :] = x1[j, :, 0][ToSFC1[j].long()] * self.weight11 + \\\n","                             x1[j, :, 0][ToSFC1Up[j].long()] * self.weight11_0 + \\\n","                             x1[j, :, 0][ToSFC1Down[j].long()] * self.weight11_1 + self.bias11\n","\n","            x1_1d[j, 2, :] = x1[j, :, 1][ToSFC1[j].long()] * self.weight2 + \\\n","                             x1[j, :, 1][ToSFC1Up[j].long()] * self.weight2_0 + \\\n","                             x1[j, :, 1][ToSFC1Down[j].long()] * self.weight2_1 + self.bias2\n","\n","            x1_1d[j, 3, :] = x1[j, :, 1][ToSFC1[j].long()] * self.weight22 + \\\n","                             x1[j, :, 1][ToSFC1Up[j].long()] * self.weight22_0 + \\\n","                             x1[j, :, 1][ToSFC1Down[j].long()] * self.weight22_1 + self.bias22\n","\n","        # first cnn encoder\n","        encoded_1 = self.encoder_h1(x1_1d.view(-1, 4, k)) #(16,4,20550)\n","        # print(\"encoded\", encoded_1.shape)\n","        # flatten and concatenate\n","        encoded_3 = encoded_1.view(-1,16*80)\n","        # print(\"Before FC\", encoded_3.shape)\n","        # fully connection\n","        encoded = self.fc1(encoded_3) # (b,128)\n","        # print(\"After encoder FC，the output of encoder\",encoded.shape)     \n","        encoded = torch.cat((encoded, mode),axis = 1)  # Combine the mode_1 to the x1\n","        \n","        # print(\"encoded_combine\",encoded.shape)\n","        decoded_3 = self.decoder_h1(self.fc2(encoded).view(-1, 16, 80))\n","        # print(\"The output of decoder: \", decoded_3.shape) # (16, 2, 20550)\n","        BackSFC1 = torch.argsort(ToSFC1)\n","        BackSFC1Up = torch.argsort(ToSFC1Up)\n","        BackSFC1Down = torch.argsort(ToSFC1Down)\n","\n","        # k = 20550\n","        # batch_num = ToSFC1.shape[0]\n","        decoded_sp = torch.zeros((batch_num, k, 2)).to(device)\n","        # output sparse layer, resort according to sfc transform\n","        for j in range(batch_num):\n","            decoded_sp[j, :, 0] = decoded_3[j, 0, :][BackSFC1[j].long()]* self.weight_out1 + \\\n","                                  decoded_3[j, 0, :][BackSFC1Up[j].long()] * self.weight_out1_0 + \\\n","                                  decoded_3[j, 0, :][BackSFC1Down[j].long()] * self.weight_out1_1 + \\\n","                                  decoded_3[j, 1, :][BackSFC1[j].long()]* self.weight_out11 + \\\n","                                  decoded_3[j, 1, :][BackSFC1Up[j].long()] * self.weight_out11_0 + \\\n","                                  decoded_3[j, 1, :][BackSFC1Down[j].long()] * self.weight_out11_1 + self.bias_out1\n","\n","            decoded_sp[j, :, 1] = decoded_3[j, 2, :][BackSFC1[j].long()] * self.weight_out3 + \\\n","                                  decoded_3[j, 2, :][BackSFC1Up[j].long()] * self.weight_out3_0 + \\\n","                                  decoded_3[j, 2, :][BackSFC1Down[j].long()] * self.weight_out3_1 + \\\n","                                  decoded_3[j, 3, :][BackSFC1[j].long()] * self.weight_out33 + \\\n","                                  decoded_3[j, 3, :][BackSFC1Up[j].long()] * self.weight_out33_0 + \\\n","                                  decoded_3[j, 3, :][BackSFC1Down[j].long()] * self.weight_out33_1 + self.bias_out2       \n","        # resort 1D to 2D\n","        decoded = F.tanh(decoded_sp) # both are BATCH_SIZE by nNodes by 2\n","        return encoded, decoded"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTi8noN6Ayig"},"source":["### Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PrTRLLIKEyW_","executionInfo":{"status":"ok","timestamp":1628490606256,"user_tz":-60,"elapsed":24176671,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"b804585a-c426-48c2-f0c4-7f82aa9a19ab"},"source":["# train the autoencoder\n","\n","t_train_0 = time.time()\n","autoencoder_3 = CNN_3().to(device)\n","optimizer = torch.optim.Adam(autoencoder_3.parameters(), lr=LR)\n","loss_func = nn.MSELoss()\n","\n","loss_list = []\n","loss_valid = []\n","epoch_list=[]\n","\n","for epoch in range(EPOCH):\n","    for x, mode in train_loader:\n","        detach_mode = mode.detach()\n","        b_y = x[:, :, 3:5].to(device)   # b_y= False\n","        b_x = x.to(device)    # b_x: False\n","      \n","        b_mode = detach_mode.to(device)  #b_mode = True  #The size is [16,128]\n","        \n","        # print(\"b_mode\",b_mode.requires_grad)\n","        encoded, decoded = autoencoder_3(b_x.float(),b_mode.float())   #decoded true\n","        # decoded.detach_()\n","        # decoded = decoded.detach()\n","      \n","        loss = loss_func(decoded, b_y.float()) # Loss: True       # mean square error\n","        optimizer.zero_grad()                  # clear gradients for this training step\n","        loss.backward()                     # backpropagation, compute gradients\n","        optimizer.step()                     # apply gradients\n","\n","    loss_list.append(loss)\n","\n","    encoded, decoded = autoencoder_3(torch.tensor(valid_data).to(device),mode_2valid.float().to(device))\n","    error_autoencoder_3 = (decoded.detach() - torch.tensor(valid_data[:,:, 3:5]).to(device))\n","    MSE_valid = (error_autoencoder_3**2).mean()\n","    loss_valid.append(MSE_valid)\n","    epoch_list.append(epoch)\n","    print('Epoch: ', epoch, '| train loss: %.6f' % loss.cpu().data.numpy(), '| valid loss: %.6f' % MSE_valid)\n","\n","    #save the weights every 500 epochs \n","    if (epoch%500 == 0):\n","        torch.save(autoencoder_3, \"./HAE/pkl/II_I_X_Eran\"+str(epoch) +\"_LV\"+str(Latent_num)+ \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\".pkl\")\n","        pathcsv= \"./HAE/csv/II_I_X_Eran\"+str(epoch)+\"_LV\"+str(Latent_num) + \"_B\"+str(BATCH_SIZE)+\"_n\"+ str(nTrain)+\"_L\"+str(LR)+\".csv\"\n","        saveCsv(pathcsv,epoch+1)\n","\n","t_train_1 = time.time()\n","# torch.save(autoencoder_2, path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:  0 | train loss: 0.026077 | valid loss: 0.025370\n","Epoch:  1 | train loss: 0.018477 | valid loss: 0.017759\n","Epoch:  2 | train loss: 0.009792 | valid loss: 0.010237\n","Epoch:  3 | train loss: 0.008482 | valid loss: 0.008314\n","Epoch:  4 | train loss: 0.007445 | valid loss: 0.007105\n","Epoch:  5 | train loss: 0.006224 | valid loss: 0.006063\n","Epoch:  6 | train loss: 0.005161 | valid loss: 0.005296\n","Epoch:  7 | train loss: 0.004508 | valid loss: 0.004714\n","Epoch:  8 | train loss: 0.004528 | valid loss: 0.004258\n","Epoch:  9 | train loss: 0.003968 | valid loss: 0.003905\n","Epoch:  10 | train loss: 0.003630 | valid loss: 0.003617\n","Epoch:  11 | train loss: 0.003605 | valid loss: 0.003377\n","Epoch:  12 | train loss: 0.003328 | valid loss: 0.003181\n","Epoch:  13 | train loss: 0.003360 | valid loss: 0.002996\n","Epoch:  14 | train loss: 0.002879 | valid loss: 0.002836\n","Epoch:  15 | train loss: 0.002652 | valid loss: 0.002675\n","Epoch:  16 | train loss: 0.002600 | valid loss: 0.002520\n","Epoch:  17 | train loss: 0.002572 | valid loss: 0.002371\n","Epoch:  18 | train loss: 0.002497 | valid loss: 0.002236\n","Epoch:  19 | train loss: 0.002560 | valid loss: 0.002123\n","Epoch:  20 | train loss: 0.002025 | valid loss: 0.002019\n","Epoch:  21 | train loss: 0.001796 | valid loss: 0.001923\n","Epoch:  22 | train loss: 0.001769 | valid loss: 0.001837\n","Epoch:  23 | train loss: 0.001873 | valid loss: 0.001768\n","Epoch:  24 | train loss: 0.001525 | valid loss: 0.001704\n","Epoch:  25 | train loss: 0.001988 | valid loss: 0.001642\n","Epoch:  26 | train loss: 0.001880 | valid loss: 0.001583\n","Epoch:  27 | train loss: 0.001964 | valid loss: 0.001533\n","Epoch:  28 | train loss: 0.001502 | valid loss: 0.001484\n","Epoch:  29 | train loss: 0.001515 | valid loss: 0.001441\n","Epoch:  30 | train loss: 0.001445 | valid loss: 0.001400\n","Epoch:  31 | train loss: 0.001342 | valid loss: 0.001359\n","Epoch:  32 | train loss: 0.001130 | valid loss: 0.001324\n","Epoch:  33 | train loss: 0.001603 | valid loss: 0.001293\n","Epoch:  34 | train loss: 0.001384 | valid loss: 0.001262\n","Epoch:  35 | train loss: 0.001240 | valid loss: 0.001232\n","Epoch:  36 | train loss: 0.001204 | valid loss: 0.001201\n","Epoch:  37 | train loss: 0.001233 | valid loss: 0.001183\n","Epoch:  38 | train loss: 0.001147 | valid loss: 0.001150\n","Epoch:  39 | train loss: 0.001346 | valid loss: 0.001129\n","Epoch:  40 | train loss: 0.001169 | valid loss: 0.001106\n","Epoch:  41 | train loss: 0.001109 | valid loss: 0.001085\n","Epoch:  42 | train loss: 0.000908 | valid loss: 0.001063\n","Epoch:  43 | train loss: 0.001029 | valid loss: 0.001045\n","Epoch:  44 | train loss: 0.000868 | valid loss: 0.001023\n","Epoch:  45 | train loss: 0.000978 | valid loss: 0.001005\n","Epoch:  46 | train loss: 0.000958 | valid loss: 0.000989\n","Epoch:  47 | train loss: 0.000997 | valid loss: 0.000974\n","Epoch:  48 | train loss: 0.000982 | valid loss: 0.000961\n","Epoch:  49 | train loss: 0.000916 | valid loss: 0.000942\n","Epoch:  50 | train loss: 0.001044 | valid loss: 0.000928\n","Epoch:  51 | train loss: 0.000838 | valid loss: 0.000911\n","Epoch:  52 | train loss: 0.000840 | valid loss: 0.000900\n","Epoch:  53 | train loss: 0.000999 | valid loss: 0.000894\n","Epoch:  54 | train loss: 0.000868 | valid loss: 0.000879\n","Epoch:  55 | train loss: 0.001025 | valid loss: 0.000865\n","Epoch:  56 | train loss: 0.000729 | valid loss: 0.000854\n","Epoch:  57 | train loss: 0.000723 | valid loss: 0.000845\n","Epoch:  58 | train loss: 0.000982 | valid loss: 0.000838\n","Epoch:  59 | train loss: 0.000897 | valid loss: 0.000831\n","Epoch:  60 | train loss: 0.000619 | valid loss: 0.000814\n","Epoch:  61 | train loss: 0.000985 | valid loss: 0.000815\n","Epoch:  62 | train loss: 0.000797 | valid loss: 0.000800\n","Epoch:  63 | train loss: 0.000805 | valid loss: 0.000790\n","Epoch:  64 | train loss: 0.000763 | valid loss: 0.000786\n","Epoch:  65 | train loss: 0.000813 | valid loss: 0.000773\n","Epoch:  66 | train loss: 0.000655 | valid loss: 0.000766\n","Epoch:  67 | train loss: 0.000972 | valid loss: 0.000757\n","Epoch:  68 | train loss: 0.000800 | valid loss: 0.000758\n","Epoch:  69 | train loss: 0.000756 | valid loss: 0.000749\n","Epoch:  70 | train loss: 0.000649 | valid loss: 0.000742\n","Epoch:  71 | train loss: 0.000647 | valid loss: 0.000734\n","Epoch:  72 | train loss: 0.000650 | valid loss: 0.000728\n","Epoch:  73 | train loss: 0.000568 | valid loss: 0.000719\n","Epoch:  74 | train loss: 0.000910 | valid loss: 0.000715\n","Epoch:  75 | train loss: 0.000791 | valid loss: 0.000707\n","Epoch:  76 | train loss: 0.000669 | valid loss: 0.000703\n","Epoch:  77 | train loss: 0.000705 | valid loss: 0.000702\n","Epoch:  78 | train loss: 0.000851 | valid loss: 0.000691\n","Epoch:  79 | train loss: 0.000649 | valid loss: 0.000686\n","Epoch:  80 | train loss: 0.000568 | valid loss: 0.000684\n","Epoch:  81 | train loss: 0.000749 | valid loss: 0.000681\n","Epoch:  82 | train loss: 0.000707 | valid loss: 0.000674\n","Epoch:  83 | train loss: 0.000707 | valid loss: 0.000668\n","Epoch:  84 | train loss: 0.000578 | valid loss: 0.000667\n","Epoch:  85 | train loss: 0.000577 | valid loss: 0.000660\n","Epoch:  86 | train loss: 0.000797 | valid loss: 0.000653\n","Epoch:  87 | train loss: 0.000738 | valid loss: 0.000652\n","Epoch:  88 | train loss: 0.000721 | valid loss: 0.000644\n","Epoch:  89 | train loss: 0.000713 | valid loss: 0.000641\n","Epoch:  90 | train loss: 0.000681 | valid loss: 0.000636\n","Epoch:  91 | train loss: 0.000609 | valid loss: 0.000635\n","Epoch:  92 | train loss: 0.000562 | valid loss: 0.000636\n","Epoch:  93 | train loss: 0.000585 | valid loss: 0.000624\n","Epoch:  94 | train loss: 0.000679 | valid loss: 0.000619\n","Epoch:  95 | train loss: 0.000747 | valid loss: 0.000618\n","Epoch:  96 | train loss: 0.000666 | valid loss: 0.000614\n","Epoch:  97 | train loss: 0.000619 | valid loss: 0.000608\n","Epoch:  98 | train loss: 0.000605 | valid loss: 0.000607\n","Epoch:  99 | train loss: 0.000570 | valid loss: 0.000607\n","Epoch:  100 | train loss: 0.000619 | valid loss: 0.000600\n","Epoch:  101 | train loss: 0.000629 | valid loss: 0.000602\n","Epoch:  102 | train loss: 0.000680 | valid loss: 0.000594\n","Epoch:  103 | train loss: 0.000604 | valid loss: 0.000589\n","Epoch:  104 | train loss: 0.000727 | valid loss: 0.000590\n","Epoch:  105 | train loss: 0.000568 | valid loss: 0.000581\n","Epoch:  106 | train loss: 0.000546 | valid loss: 0.000579\n","Epoch:  107 | train loss: 0.000774 | valid loss: 0.000578\n","Epoch:  108 | train loss: 0.000589 | valid loss: 0.000575\n","Epoch:  109 | train loss: 0.000456 | valid loss: 0.000573\n","Epoch:  110 | train loss: 0.000500 | valid loss: 0.000571\n","Epoch:  111 | train loss: 0.000471 | valid loss: 0.000569\n","Epoch:  112 | train loss: 0.000659 | valid loss: 0.000570\n","Epoch:  113 | train loss: 0.000477 | valid loss: 0.000562\n","Epoch:  114 | train loss: 0.000850 | valid loss: 0.000560\n","Epoch:  115 | train loss: 0.000615 | valid loss: 0.000558\n","Epoch:  116 | train loss: 0.000450 | valid loss: 0.000554\n","Epoch:  117 | train loss: 0.000594 | valid loss: 0.000552\n","Epoch:  118 | train loss: 0.000613 | valid loss: 0.000547\n","Epoch:  119 | train loss: 0.000552 | valid loss: 0.000545\n","Epoch:  120 | train loss: 0.000597 | valid loss: 0.000545\n","Epoch:  121 | train loss: 0.000533 | valid loss: 0.000542\n","Epoch:  122 | train loss: 0.000671 | valid loss: 0.000539\n","Epoch:  123 | train loss: 0.000440 | valid loss: 0.000537\n","Epoch:  124 | train loss: 0.000487 | valid loss: 0.000534\n","Epoch:  125 | train loss: 0.000536 | valid loss: 0.000534\n","Epoch:  126 | train loss: 0.000442 | valid loss: 0.000531\n","Epoch:  127 | train loss: 0.000535 | valid loss: 0.000529\n","Epoch:  128 | train loss: 0.000581 | valid loss: 0.000527\n","Epoch:  129 | train loss: 0.000570 | valid loss: 0.000525\n","Epoch:  130 | train loss: 0.000582 | valid loss: 0.000523\n","Epoch:  131 | train loss: 0.000668 | valid loss: 0.000534\n","Epoch:  132 | train loss: 0.000473 | valid loss: 0.000523\n","Epoch:  133 | train loss: 0.000538 | valid loss: 0.000515\n","Epoch:  134 | train loss: 0.000421 | valid loss: 0.000514\n","Epoch:  135 | train loss: 0.000502 | valid loss: 0.000512\n","Epoch:  136 | train loss: 0.000471 | valid loss: 0.000515\n","Epoch:  137 | train loss: 0.000543 | valid loss: 0.000511\n","Epoch:  138 | train loss: 0.000432 | valid loss: 0.000508\n","Epoch:  139 | train loss: 0.000468 | valid loss: 0.000506\n","Epoch:  140 | train loss: 0.000543 | valid loss: 0.000503\n","Epoch:  141 | train loss: 0.000570 | valid loss: 0.000502\n","Epoch:  142 | train loss: 0.000430 | valid loss: 0.000501\n","Epoch:  143 | train loss: 0.000555 | valid loss: 0.000499\n","Epoch:  144 | train loss: 0.000514 | valid loss: 0.000498\n","Epoch:  145 | train loss: 0.000519 | valid loss: 0.000494\n","Epoch:  146 | train loss: 0.000624 | valid loss: 0.000494\n","Epoch:  147 | train loss: 0.000421 | valid loss: 0.000495\n","Epoch:  148 | train loss: 0.000573 | valid loss: 0.000498\n","Epoch:  149 | train loss: 0.000585 | valid loss: 0.000492\n","Epoch:  150 | train loss: 0.000468 | valid loss: 0.000489\n","Epoch:  151 | train loss: 0.000462 | valid loss: 0.000487\n","Epoch:  152 | train loss: 0.000598 | valid loss: 0.000485\n","Epoch:  153 | train loss: 0.000422 | valid loss: 0.000490\n","Epoch:  154 | train loss: 0.000499 | valid loss: 0.000483\n","Epoch:  155 | train loss: 0.000469 | valid loss: 0.000483\n","Epoch:  156 | train loss: 0.000407 | valid loss: 0.000478\n","Epoch:  157 | train loss: 0.000439 | valid loss: 0.000478\n","Epoch:  158 | train loss: 0.000440 | valid loss: 0.000477\n","Epoch:  159 | train loss: 0.000477 | valid loss: 0.000477\n","Epoch:  160 | train loss: 0.000441 | valid loss: 0.000473\n","Epoch:  161 | train loss: 0.000431 | valid loss: 0.000475\n","Epoch:  162 | train loss: 0.000514 | valid loss: 0.000472\n","Epoch:  163 | train loss: 0.000549 | valid loss: 0.000471\n","Epoch:  164 | train loss: 0.000597 | valid loss: 0.000469\n","Epoch:  165 | train loss: 0.000543 | valid loss: 0.000468\n","Epoch:  166 | train loss: 0.000468 | valid loss: 0.000467\n","Epoch:  167 | train loss: 0.000426 | valid loss: 0.000469\n","Epoch:  168 | train loss: 0.000514 | valid loss: 0.000463\n","Epoch:  169 | train loss: 0.000538 | valid loss: 0.000467\n","Epoch:  170 | train loss: 0.000454 | valid loss: 0.000462\n","Epoch:  171 | train loss: 0.000533 | valid loss: 0.000461\n","Epoch:  172 | train loss: 0.000373 | valid loss: 0.000461\n","Epoch:  173 | train loss: 0.000506 | valid loss: 0.000459\n","Epoch:  174 | train loss: 0.000476 | valid loss: 0.000457\n","Epoch:  175 | train loss: 0.000547 | valid loss: 0.000459\n","Epoch:  176 | train loss: 0.000496 | valid loss: 0.000458\n","Epoch:  177 | train loss: 0.000447 | valid loss: 0.000455\n","Epoch:  178 | train loss: 0.000497 | valid loss: 0.000455\n","Epoch:  179 | train loss: 0.000536 | valid loss: 0.000454\n","Epoch:  180 | train loss: 0.000536 | valid loss: 0.000449\n","Epoch:  181 | train loss: 0.000487 | valid loss: 0.000451\n","Epoch:  182 | train loss: 0.000565 | valid loss: 0.000448\n","Epoch:  183 | train loss: 0.000390 | valid loss: 0.000447\n","Epoch:  184 | train loss: 0.000367 | valid loss: 0.000452\n","Epoch:  185 | train loss: 0.000460 | valid loss: 0.000445\n","Epoch:  186 | train loss: 0.000439 | valid loss: 0.000444\n","Epoch:  187 | train loss: 0.000479 | valid loss: 0.000447\n","Epoch:  188 | train loss: 0.000508 | valid loss: 0.000444\n","Epoch:  189 | train loss: 0.000426 | valid loss: 0.000439\n","Epoch:  190 | train loss: 0.000482 | valid loss: 0.000440\n","Epoch:  191 | train loss: 0.000451 | valid loss: 0.000438\n","Epoch:  192 | train loss: 0.000506 | valid loss: 0.000440\n","Epoch:  193 | train loss: 0.000560 | valid loss: 0.000441\n","Epoch:  194 | train loss: 0.000458 | valid loss: 0.000438\n","Epoch:  195 | train loss: 0.000462 | valid loss: 0.000436\n","Epoch:  196 | train loss: 0.000417 | valid loss: 0.000436\n","Epoch:  197 | train loss: 0.000359 | valid loss: 0.000434\n","Epoch:  198 | train loss: 0.000518 | valid loss: 0.000435\n","Epoch:  199 | train loss: 0.000448 | valid loss: 0.000437\n","Epoch:  200 | train loss: 0.000519 | valid loss: 0.000432\n","Epoch:  201 | train loss: 0.000452 | valid loss: 0.000433\n","Epoch:  202 | train loss: 0.000539 | valid loss: 0.000429\n","Epoch:  203 | train loss: 0.000530 | valid loss: 0.000431\n","Epoch:  204 | train loss: 0.000418 | valid loss: 0.000428\n","Epoch:  205 | train loss: 0.000443 | valid loss: 0.000426\n","Epoch:  206 | train loss: 0.000502 | valid loss: 0.000426\n","Epoch:  207 | train loss: 0.000436 | valid loss: 0.000426\n","Epoch:  208 | train loss: 0.000424 | valid loss: 0.000424\n","Epoch:  209 | train loss: 0.000388 | valid loss: 0.000424\n","Epoch:  210 | train loss: 0.000416 | valid loss: 0.000423\n","Epoch:  211 | train loss: 0.000372 | valid loss: 0.000427\n","Epoch:  212 | train loss: 0.000502 | valid loss: 0.000420\n","Epoch:  213 | train loss: 0.000457 | valid loss: 0.000420\n","Epoch:  214 | train loss: 0.000494 | valid loss: 0.000418\n","Epoch:  215 | train loss: 0.000347 | valid loss: 0.000428\n","Epoch:  216 | train loss: 0.000475 | valid loss: 0.000417\n","Epoch:  217 | train loss: 0.000520 | valid loss: 0.000417\n","Epoch:  218 | train loss: 0.000428 | valid loss: 0.000417\n","Epoch:  219 | train loss: 0.000366 | valid loss: 0.000413\n","Epoch:  220 | train loss: 0.000379 | valid loss: 0.000418\n","Epoch:  221 | train loss: 0.000420 | valid loss: 0.000415\n","Epoch:  222 | train loss: 0.000473 | valid loss: 0.000414\n","Epoch:  223 | train loss: 0.000462 | valid loss: 0.000411\n","Epoch:  224 | train loss: 0.000379 | valid loss: 0.000412\n","Epoch:  225 | train loss: 0.000496 | valid loss: 0.000413\n","Epoch:  226 | train loss: 0.000415 | valid loss: 0.000412\n","Epoch:  227 | train loss: 0.000471 | valid loss: 0.000414\n","Epoch:  228 | train loss: 0.000396 | valid loss: 0.000409\n","Epoch:  229 | train loss: 0.000403 | valid loss: 0.000408\n","Epoch:  230 | train loss: 0.000374 | valid loss: 0.000406\n","Epoch:  231 | train loss: 0.000399 | valid loss: 0.000408\n","Epoch:  232 | train loss: 0.000483 | valid loss: 0.000410\n","Epoch:  233 | train loss: 0.000394 | valid loss: 0.000407\n","Epoch:  234 | train loss: 0.000386 | valid loss: 0.000406\n","Epoch:  235 | train loss: 0.000458 | valid loss: 0.000411\n","Epoch:  236 | train loss: 0.000493 | valid loss: 0.000404\n","Epoch:  237 | train loss: 0.000418 | valid loss: 0.000405\n","Epoch:  238 | train loss: 0.000398 | valid loss: 0.000402\n","Epoch:  239 | train loss: 0.000388 | valid loss: 0.000408\n","Epoch:  240 | train loss: 0.000354 | valid loss: 0.000403\n","Epoch:  241 | train loss: 0.000504 | valid loss: 0.000401\n","Epoch:  242 | train loss: 0.000333 | valid loss: 0.000403\n","Epoch:  243 | train loss: 0.000362 | valid loss: 0.000401\n","Epoch:  244 | train loss: 0.000448 | valid loss: 0.000405\n","Epoch:  245 | train loss: 0.000486 | valid loss: 0.000401\n","Epoch:  246 | train loss: 0.000408 | valid loss: 0.000397\n","Epoch:  247 | train loss: 0.000399 | valid loss: 0.000400\n","Epoch:  248 | train loss: 0.000488 | valid loss: 0.000396\n","Epoch:  249 | train loss: 0.000401 | valid loss: 0.000396\n","Epoch:  250 | train loss: 0.000392 | valid loss: 0.000395\n","Epoch:  251 | train loss: 0.000434 | valid loss: 0.000395\n","Epoch:  252 | train loss: 0.000480 | valid loss: 0.000394\n","Epoch:  253 | train loss: 0.000403 | valid loss: 0.000392\n","Epoch:  254 | train loss: 0.000307 | valid loss: 0.000392\n","Epoch:  255 | train loss: 0.000426 | valid loss: 0.000391\n","Epoch:  256 | train loss: 0.000727 | valid loss: 0.000398\n","Epoch:  257 | train loss: 0.000479 | valid loss: 0.000392\n","Epoch:  258 | train loss: 0.000365 | valid loss: 0.000394\n","Epoch:  259 | train loss: 0.000363 | valid loss: 0.000389\n","Epoch:  260 | train loss: 0.000383 | valid loss: 0.000391\n","Epoch:  261 | train loss: 0.000344 | valid loss: 0.000394\n","Epoch:  262 | train loss: 0.000308 | valid loss: 0.000388\n","Epoch:  263 | train loss: 0.000497 | valid loss: 0.000387\n","Epoch:  264 | train loss: 0.000330 | valid loss: 0.000388\n","Epoch:  265 | train loss: 0.000461 | valid loss: 0.000389\n","Epoch:  266 | train loss: 0.000367 | valid loss: 0.000387\n","Epoch:  267 | train loss: 0.000391 | valid loss: 0.000385\n","Epoch:  268 | train loss: 0.000385 | valid loss: 0.000384\n","Epoch:  269 | train loss: 0.000386 | valid loss: 0.000384\n","Epoch:  270 | train loss: 0.000362 | valid loss: 0.000385\n","Epoch:  271 | train loss: 0.000366 | valid loss: 0.000385\n","Epoch:  272 | train loss: 0.000336 | valid loss: 0.000384\n","Epoch:  273 | train loss: 0.000415 | valid loss: 0.000382\n","Epoch:  274 | train loss: 0.000385 | valid loss: 0.000389\n","Epoch:  275 | train loss: 0.000416 | valid loss: 0.000382\n","Epoch:  276 | train loss: 0.000303 | valid loss: 0.000379\n","Epoch:  277 | train loss: 0.000344 | valid loss: 0.000380\n","Epoch:  278 | train loss: 0.000363 | valid loss: 0.000379\n","Epoch:  279 | train loss: 0.000334 | valid loss: 0.000378\n","Epoch:  280 | train loss: 0.000308 | valid loss: 0.000379\n","Epoch:  281 | train loss: 0.000341 | valid loss: 0.000379\n","Epoch:  282 | train loss: 0.000323 | valid loss: 0.000378\n","Epoch:  283 | train loss: 0.000330 | valid loss: 0.000379\n","Epoch:  284 | train loss: 0.000295 | valid loss: 0.000382\n","Epoch:  285 | train loss: 0.000396 | valid loss: 0.000380\n","Epoch:  286 | train loss: 0.000376 | valid loss: 0.000378\n","Epoch:  287 | train loss: 0.000441 | valid loss: 0.000381\n","Epoch:  288 | train loss: 0.000353 | valid loss: 0.000374\n","Epoch:  289 | train loss: 0.000344 | valid loss: 0.000375\n","Epoch:  290 | train loss: 0.000412 | valid loss: 0.000375\n","Epoch:  291 | train loss: 0.000416 | valid loss: 0.000371\n","Epoch:  292 | train loss: 0.000360 | valid loss: 0.000374\n","Epoch:  293 | train loss: 0.000339 | valid loss: 0.000371\n","Epoch:  294 | train loss: 0.000395 | valid loss: 0.000372\n","Epoch:  295 | train loss: 0.000297 | valid loss: 0.000374\n","Epoch:  296 | train loss: 0.000352 | valid loss: 0.000374\n","Epoch:  297 | train loss: 0.000322 | valid loss: 0.000372\n","Epoch:  298 | train loss: 0.000372 | valid loss: 0.000370\n","Epoch:  299 | train loss: 0.000354 | valid loss: 0.000371\n","Epoch:  300 | train loss: 0.000299 | valid loss: 0.000368\n","Epoch:  301 | train loss: 0.000316 | valid loss: 0.000369\n","Epoch:  302 | train loss: 0.000450 | valid loss: 0.000368\n","Epoch:  303 | train loss: 0.000432 | valid loss: 0.000370\n","Epoch:  304 | train loss: 0.000533 | valid loss: 0.000371\n","Epoch:  305 | train loss: 0.000369 | valid loss: 0.000378\n","Epoch:  306 | train loss: 0.000382 | valid loss: 0.000374\n","Epoch:  307 | train loss: 0.000337 | valid loss: 0.000370\n","Epoch:  308 | train loss: 0.000346 | valid loss: 0.000367\n","Epoch:  309 | train loss: 0.000401 | valid loss: 0.000365\n","Epoch:  310 | train loss: 0.000326 | valid loss: 0.000364\n","Epoch:  311 | train loss: 0.000350 | valid loss: 0.000363\n","Epoch:  312 | train loss: 0.000365 | valid loss: 0.000365\n","Epoch:  313 | train loss: 0.000370 | valid loss: 0.000366\n","Epoch:  314 | train loss: 0.000376 | valid loss: 0.000366\n","Epoch:  315 | train loss: 0.000409 | valid loss: 0.000365\n","Epoch:  316 | train loss: 0.000345 | valid loss: 0.000362\n","Epoch:  317 | train loss: 0.000351 | valid loss: 0.000365\n","Epoch:  318 | train loss: 0.000374 | valid loss: 0.000363\n","Epoch:  319 | train loss: 0.000355 | valid loss: 0.000363\n","Epoch:  320 | train loss: 0.000407 | valid loss: 0.000361\n","Epoch:  321 | train loss: 0.000365 | valid loss: 0.000360\n","Epoch:  322 | train loss: 0.000394 | valid loss: 0.000361\n","Epoch:  323 | train loss: 0.000351 | valid loss: 0.000360\n","Epoch:  324 | train loss: 0.000401 | valid loss: 0.000361\n","Epoch:  325 | train loss: 0.000402 | valid loss: 0.000365\n","Epoch:  326 | train loss: 0.000372 | valid loss: 0.000361\n","Epoch:  327 | train loss: 0.000360 | valid loss: 0.000360\n","Epoch:  328 | train loss: 0.000375 | valid loss: 0.000363\n","Epoch:  329 | train loss: 0.000356 | valid loss: 0.000361\n","Epoch:  330 | train loss: 0.000315 | valid loss: 0.000360\n","Epoch:  331 | train loss: 0.000411 | valid loss: 0.000356\n","Epoch:  332 | train loss: 0.000462 | valid loss: 0.000359\n","Epoch:  333 | train loss: 0.000687 | valid loss: 0.000364\n","Epoch:  334 | train loss: 0.000289 | valid loss: 0.000355\n","Epoch:  335 | train loss: 0.000390 | valid loss: 0.000356\n","Epoch:  336 | train loss: 0.000354 | valid loss: 0.000353\n","Epoch:  337 | train loss: 0.000323 | valid loss: 0.000357\n","Epoch:  338 | train loss: 0.000359 | valid loss: 0.000356\n","Epoch:  339 | train loss: 0.000292 | valid loss: 0.000354\n","Epoch:  340 | train loss: 0.000283 | valid loss: 0.000355\n","Epoch:  341 | train loss: 0.000333 | valid loss: 0.000356\n","Epoch:  342 | train loss: 0.000418 | valid loss: 0.000355\n","Epoch:  343 | train loss: 0.000296 | valid loss: 0.000365\n","Epoch:  344 | train loss: 0.000316 | valid loss: 0.000354\n","Epoch:  345 | train loss: 0.000430 | valid loss: 0.000355\n","Epoch:  346 | train loss: 0.000413 | valid loss: 0.000353\n","Epoch:  347 | train loss: 0.000448 | valid loss: 0.000352\n","Epoch:  348 | train loss: 0.000352 | valid loss: 0.000351\n","Epoch:  349 | train loss: 0.000317 | valid loss: 0.000352\n","Epoch:  350 | train loss: 0.000330 | valid loss: 0.000349\n","Epoch:  351 | train loss: 0.000465 | valid loss: 0.000350\n","Epoch:  352 | train loss: 0.000418 | valid loss: 0.000350\n","Epoch:  353 | train loss: 0.000389 | valid loss: 0.000350\n","Epoch:  354 | train loss: 0.000334 | valid loss: 0.000349\n","Epoch:  355 | train loss: 0.000386 | valid loss: 0.000348\n","Epoch:  356 | train loss: 0.000292 | valid loss: 0.000349\n","Epoch:  357 | train loss: 0.000282 | valid loss: 0.000351\n","Epoch:  358 | train loss: 0.000298 | valid loss: 0.000350\n","Epoch:  359 | train loss: 0.000306 | valid loss: 0.000347\n","Epoch:  360 | train loss: 0.000328 | valid loss: 0.000347\n","Epoch:  361 | train loss: 0.000428 | valid loss: 0.000349\n","Epoch:  362 | train loss: 0.000343 | valid loss: 0.000347\n","Epoch:  363 | train loss: 0.000339 | valid loss: 0.000346\n","Epoch:  364 | train loss: 0.000314 | valid loss: 0.000344\n","Epoch:  365 | train loss: 0.000366 | valid loss: 0.000349\n","Epoch:  366 | train loss: 0.000353 | valid loss: 0.000347\n","Epoch:  367 | train loss: 0.000264 | valid loss: 0.000346\n","Epoch:  368 | train loss: 0.000423 | valid loss: 0.000344\n","Epoch:  369 | train loss: 0.000316 | valid loss: 0.000348\n","Epoch:  370 | train loss: 0.000345 | valid loss: 0.000343\n","Epoch:  371 | train loss: 0.000297 | valid loss: 0.000346\n","Epoch:  372 | train loss: 0.000324 | valid loss: 0.000343\n","Epoch:  373 | train loss: 0.000388 | valid loss: 0.000344\n","Epoch:  374 | train loss: 0.000329 | valid loss: 0.000345\n","Epoch:  375 | train loss: 0.000379 | valid loss: 0.000341\n","Epoch:  376 | train loss: 0.000378 | valid loss: 0.000344\n","Epoch:  377 | train loss: 0.000368 | valid loss: 0.000345\n","Epoch:  378 | train loss: 0.000311 | valid loss: 0.000341\n","Epoch:  379 | train loss: 0.000332 | valid loss: 0.000340\n","Epoch:  380 | train loss: 0.000388 | valid loss: 0.000341\n","Epoch:  381 | train loss: 0.000404 | valid loss: 0.000343\n","Epoch:  382 | train loss: 0.000350 | valid loss: 0.000345\n","Epoch:  383 | train loss: 0.000305 | valid loss: 0.000341\n","Epoch:  384 | train loss: 0.000424 | valid loss: 0.000340\n","Epoch:  385 | train loss: 0.000319 | valid loss: 0.000341\n","Epoch:  386 | train loss: 0.000367 | valid loss: 0.000342\n","Epoch:  387 | train loss: 0.000302 | valid loss: 0.000337\n","Epoch:  388 | train loss: 0.000349 | valid loss: 0.000341\n","Epoch:  389 | train loss: 0.000320 | valid loss: 0.000338\n","Epoch:  390 | train loss: 0.000346 | valid loss: 0.000341\n","Epoch:  391 | train loss: 0.000328 | valid loss: 0.000338\n","Epoch:  392 | train loss: 0.000322 | valid loss: 0.000344\n","Epoch:  393 | train loss: 0.000288 | valid loss: 0.000340\n","Epoch:  394 | train loss: 0.000401 | valid loss: 0.000337\n","Epoch:  395 | train loss: 0.000356 | valid loss: 0.000336\n","Epoch:  396 | train loss: 0.000415 | valid loss: 0.000339\n","Epoch:  397 | train loss: 0.000347 | valid loss: 0.000342\n","Epoch:  398 | train loss: 0.000294 | valid loss: 0.000339\n","Epoch:  399 | train loss: 0.000421 | valid loss: 0.000336\n","Epoch:  400 | train loss: 0.000352 | valid loss: 0.000336\n","Epoch:  401 | train loss: 0.000296 | valid loss: 0.000334\n","Epoch:  402 | train loss: 0.000302 | valid loss: 0.000337\n","Epoch:  403 | train loss: 0.000324 | valid loss: 0.000337\n","Epoch:  404 | train loss: 0.000277 | valid loss: 0.000333\n","Epoch:  405 | train loss: 0.000274 | valid loss: 0.000335\n","Epoch:  406 | train loss: 0.000342 | valid loss: 0.000333\n","Epoch:  407 | train loss: 0.000342 | valid loss: 0.000334\n","Epoch:  408 | train loss: 0.000267 | valid loss: 0.000332\n","Epoch:  409 | train loss: 0.000351 | valid loss: 0.000336\n","Epoch:  410 | train loss: 0.000357 | valid loss: 0.000334\n","Epoch:  411 | train loss: 0.000293 | valid loss: 0.000334\n","Epoch:  412 | train loss: 0.000367 | valid loss: 0.000336\n","Epoch:  413 | train loss: 0.000316 | valid loss: 0.000333\n","Epoch:  414 | train loss: 0.000395 | valid loss: 0.000337\n","Epoch:  415 | train loss: 0.000286 | valid loss: 0.000332\n","Epoch:  416 | train loss: 0.000368 | valid loss: 0.000330\n","Epoch:  417 | train loss: 0.000342 | valid loss: 0.000331\n","Epoch:  418 | train loss: 0.000305 | valid loss: 0.000331\n","Epoch:  419 | train loss: 0.000343 | valid loss: 0.000330\n","Epoch:  420 | train loss: 0.000306 | valid loss: 0.000331\n","Epoch:  421 | train loss: 0.000390 | valid loss: 0.000340\n","Epoch:  422 | train loss: 0.000324 | valid loss: 0.000331\n","Epoch:  423 | train loss: 0.000329 | valid loss: 0.000331\n","Epoch:  424 | train loss: 0.000303 | valid loss: 0.000328\n","Epoch:  425 | train loss: 0.000440 | valid loss: 0.000330\n","Epoch:  426 | train loss: 0.000313 | valid loss: 0.000330\n","Epoch:  427 | train loss: 0.000363 | valid loss: 0.000332\n","Epoch:  428 | train loss: 0.000369 | valid loss: 0.000329\n","Epoch:  429 | train loss: 0.000415 | valid loss: 0.000330\n","Epoch:  430 | train loss: 0.000315 | valid loss: 0.000334\n","Epoch:  431 | train loss: 0.000292 | valid loss: 0.000330\n","Epoch:  432 | train loss: 0.000319 | valid loss: 0.000328\n","Epoch:  433 | train loss: 0.000428 | valid loss: 0.000330\n","Epoch:  434 | train loss: 0.000250 | valid loss: 0.000327\n","Epoch:  435 | train loss: 0.000413 | valid loss: 0.000328\n","Epoch:  436 | train loss: 0.000332 | valid loss: 0.000328\n","Epoch:  437 | train loss: 0.000429 | valid loss: 0.000326\n","Epoch:  438 | train loss: 0.000344 | valid loss: 0.000327\n","Epoch:  439 | train loss: 0.000292 | valid loss: 0.000328\n","Epoch:  440 | train loss: 0.000403 | valid loss: 0.000330\n","Epoch:  441 | train loss: 0.000308 | valid loss: 0.000328\n","Epoch:  442 | train loss: 0.000316 | valid loss: 0.000328\n","Epoch:  443 | train loss: 0.000291 | valid loss: 0.000325\n","Epoch:  444 | train loss: 0.000287 | valid loss: 0.000325\n","Epoch:  445 | train loss: 0.000344 | valid loss: 0.000325\n","Epoch:  446 | train loss: 0.000367 | valid loss: 0.000325\n","Epoch:  447 | train loss: 0.000358 | valid loss: 0.000324\n","Epoch:  448 | train loss: 0.000364 | valid loss: 0.000325\n","Epoch:  449 | train loss: 0.000370 | valid loss: 0.000325\n","Epoch:  450 | train loss: 0.000363 | valid loss: 0.000323\n","Epoch:  451 | train loss: 0.000257 | valid loss: 0.000324\n","Epoch:  452 | train loss: 0.000341 | valid loss: 0.000324\n","Epoch:  453 | train loss: 0.000368 | valid loss: 0.000325\n","Epoch:  454 | train loss: 0.000337 | valid loss: 0.000325\n","Epoch:  455 | train loss: 0.000381 | valid loss: 0.000326\n","Epoch:  456 | train loss: 0.000290 | valid loss: 0.000325\n","Epoch:  457 | train loss: 0.000256 | valid loss: 0.000322\n","Epoch:  458 | train loss: 0.000316 | valid loss: 0.000325\n","Epoch:  459 | train loss: 0.000322 | valid loss: 0.000322\n","Epoch:  460 | train loss: 0.000334 | valid loss: 0.000326\n","Epoch:  461 | train loss: 0.000320 | valid loss: 0.000326\n","Epoch:  462 | train loss: 0.000348 | valid loss: 0.000322\n","Epoch:  463 | train loss: 0.000434 | valid loss: 0.000323\n","Epoch:  464 | train loss: 0.000334 | valid loss: 0.000321\n","Epoch:  465 | train loss: 0.000395 | valid loss: 0.000320\n","Epoch:  466 | train loss: 0.000356 | valid loss: 0.000323\n","Epoch:  467 | train loss: 0.000296 | valid loss: 0.000321\n","Epoch:  468 | train loss: 0.000280 | valid loss: 0.000320\n","Epoch:  469 | train loss: 0.000291 | valid loss: 0.000320\n","Epoch:  470 | train loss: 0.000416 | valid loss: 0.000321\n","Epoch:  471 | train loss: 0.000312 | valid loss: 0.000320\n","Epoch:  472 | train loss: 0.000315 | valid loss: 0.000319\n","Epoch:  473 | train loss: 0.000388 | valid loss: 0.000324\n","Epoch:  474 | train loss: 0.000286 | valid loss: 0.000318\n","Epoch:  475 | train loss: 0.000485 | valid loss: 0.000321\n","Epoch:  476 | train loss: 0.000319 | valid loss: 0.000321\n","Epoch:  477 | train loss: 0.000324 | valid loss: 0.000322\n","Epoch:  478 | train loss: 0.000329 | valid loss: 0.000318\n","Epoch:  479 | train loss: 0.000421 | valid loss: 0.000317\n","Epoch:  480 | train loss: 0.000258 | valid loss: 0.000318\n","Epoch:  481 | train loss: 0.000331 | valid loss: 0.000318\n","Epoch:  482 | train loss: 0.000272 | valid loss: 0.000321\n","Epoch:  483 | train loss: 0.000304 | valid loss: 0.000317\n","Epoch:  484 | train loss: 0.000352 | valid loss: 0.000318\n","Epoch:  485 | train loss: 0.000251 | valid loss: 0.000316\n","Epoch:  486 | train loss: 0.000368 | valid loss: 0.000319\n","Epoch:  487 | train loss: 0.000321 | valid loss: 0.000317\n","Epoch:  488 | train loss: 0.000305 | valid loss: 0.000319\n","Epoch:  489 | train loss: 0.000315 | valid loss: 0.000315\n","Epoch:  490 | train loss: 0.000380 | valid loss: 0.000316\n","Epoch:  491 | train loss: 0.000406 | valid loss: 0.000319\n","Epoch:  492 | train loss: 0.000344 | valid loss: 0.000316\n","Epoch:  493 | train loss: 0.000283 | valid loss: 0.000316\n","Epoch:  494 | train loss: 0.000369 | valid loss: 0.000317\n","Epoch:  495 | train loss: 0.000369 | valid loss: 0.000316\n","Epoch:  496 | train loss: 0.000295 | valid loss: 0.000317\n","Epoch:  497 | train loss: 0.000305 | valid loss: 0.000313\n","Epoch:  498 | train loss: 0.000281 | valid loss: 0.000317\n","Epoch:  499 | train loss: 0.000299 | valid loss: 0.000324\n","Epoch:  500 | train loss: 0.000273 | valid loss: 0.000313\n","Epoch:  501 | train loss: 0.000360 | valid loss: 0.000312\n","Epoch:  502 | train loss: 0.000242 | valid loss: 0.000319\n","Epoch:  503 | train loss: 0.000337 | valid loss: 0.000316\n","Epoch:  504 | train loss: 0.000271 | valid loss: 0.000318\n","Epoch:  505 | train loss: 0.000311 | valid loss: 0.000320\n","Epoch:  506 | train loss: 0.000337 | valid loss: 0.000317\n","Epoch:  507 | train loss: 0.000388 | valid loss: 0.000312\n","Epoch:  508 | train loss: 0.000327 | valid loss: 0.000317\n","Epoch:  509 | train loss: 0.000345 | valid loss: 0.000311\n","Epoch:  510 | train loss: 0.000327 | valid loss: 0.000313\n","Epoch:  511 | train loss: 0.000421 | valid loss: 0.000311\n","Epoch:  512 | train loss: 0.000258 | valid loss: 0.000313\n","Epoch:  513 | train loss: 0.000310 | valid loss: 0.000313\n","Epoch:  514 | train loss: 0.000291 | valid loss: 0.000312\n","Epoch:  515 | train loss: 0.000283 | valid loss: 0.000311\n","Epoch:  516 | train loss: 0.000283 | valid loss: 0.000310\n","Epoch:  517 | train loss: 0.000277 | valid loss: 0.000312\n","Epoch:  518 | train loss: 0.000286 | valid loss: 0.000311\n","Epoch:  519 | train loss: 0.000342 | valid loss: 0.000311\n","Epoch:  520 | train loss: 0.000337 | valid loss: 0.000310\n","Epoch:  521 | train loss: 0.000249 | valid loss: 0.000313\n","Epoch:  522 | train loss: 0.000320 | valid loss: 0.000314\n","Epoch:  523 | train loss: 0.000297 | valid loss: 0.000310\n","Epoch:  524 | train loss: 0.000429 | valid loss: 0.000310\n","Epoch:  525 | train loss: 0.000276 | valid loss: 0.000314\n","Epoch:  526 | train loss: 0.000296 | valid loss: 0.000313\n","Epoch:  527 | train loss: 0.000282 | valid loss: 0.000311\n","Epoch:  528 | train loss: 0.000315 | valid loss: 0.000310\n","Epoch:  529 | train loss: 0.000263 | valid loss: 0.000309\n","Epoch:  530 | train loss: 0.000358 | valid loss: 0.000308\n","Epoch:  531 | train loss: 0.000397 | valid loss: 0.000311\n","Epoch:  532 | train loss: 0.000277 | valid loss: 0.000317\n","Epoch:  533 | train loss: 0.000366 | valid loss: 0.000310\n","Epoch:  534 | train loss: 0.000366 | valid loss: 0.000308\n","Epoch:  535 | train loss: 0.000337 | valid loss: 0.000307\n","Epoch:  536 | train loss: 0.000393 | valid loss: 0.000307\n","Epoch:  537 | train loss: 0.000259 | valid loss: 0.000307\n","Epoch:  538 | train loss: 0.000286 | valid loss: 0.000307\n","Epoch:  539 | train loss: 0.000310 | valid loss: 0.000308\n","Epoch:  540 | train loss: 0.000311 | valid loss: 0.000314\n","Epoch:  541 | train loss: 0.000254 | valid loss: 0.000306\n","Epoch:  542 | train loss: 0.000306 | valid loss: 0.000314\n","Epoch:  543 | train loss: 0.000303 | valid loss: 0.000309\n","Epoch:  544 | train loss: 0.000292 | valid loss: 0.000308\n","Epoch:  545 | train loss: 0.000361 | valid loss: 0.000306\n","Epoch:  546 | train loss: 0.000361 | valid loss: 0.000307\n","Epoch:  547 | train loss: 0.000247 | valid loss: 0.000309\n","Epoch:  548 | train loss: 0.000253 | valid loss: 0.000306\n","Epoch:  549 | train loss: 0.000403 | valid loss: 0.000305\n","Epoch:  550 | train loss: 0.000393 | valid loss: 0.000305\n","Epoch:  551 | train loss: 0.000366 | valid loss: 0.000309\n","Epoch:  552 | train loss: 0.000275 | valid loss: 0.000306\n","Epoch:  553 | train loss: 0.000327 | valid loss: 0.000307\n","Epoch:  554 | train loss: 0.000275 | valid loss: 0.000306\n","Epoch:  555 | train loss: 0.000388 | valid loss: 0.000312\n","Epoch:  556 | train loss: 0.000318 | valid loss: 0.000307\n","Epoch:  557 | train loss: 0.000345 | valid loss: 0.000310\n","Epoch:  558 | train loss: 0.000317 | valid loss: 0.000305\n","Epoch:  559 | train loss: 0.000329 | valid loss: 0.000306\n","Epoch:  560 | train loss: 0.000364 | valid loss: 0.000305\n","Epoch:  561 | train loss: 0.000309 | valid loss: 0.000305\n","Epoch:  562 | train loss: 0.000300 | valid loss: 0.000303\n","Epoch:  563 | train loss: 0.000273 | valid loss: 0.000308\n","Epoch:  564 | train loss: 0.000345 | valid loss: 0.000311\n","Epoch:  565 | train loss: 0.000274 | valid loss: 0.000305\n","Epoch:  566 | train loss: 0.000333 | valid loss: 0.000303\n","Epoch:  567 | train loss: 0.000257 | valid loss: 0.000303\n","Epoch:  568 | train loss: 0.000314 | valid loss: 0.000302\n","Epoch:  569 | train loss: 0.000279 | valid loss: 0.000303\n","Epoch:  570 | train loss: 0.000343 | valid loss: 0.000304\n","Epoch:  571 | train loss: 0.000381 | valid loss: 0.000304\n","Epoch:  572 | train loss: 0.000346 | valid loss: 0.000305\n","Epoch:  573 | train loss: 0.000236 | valid loss: 0.000302\n","Epoch:  574 | train loss: 0.000260 | valid loss: 0.000306\n","Epoch:  575 | train loss: 0.000336 | valid loss: 0.000308\n","Epoch:  576 | train loss: 0.000311 | valid loss: 0.000304\n","Epoch:  577 | train loss: 0.000396 | valid loss: 0.000307\n","Epoch:  578 | train loss: 0.000312 | valid loss: 0.000301\n","Epoch:  579 | train loss: 0.000312 | valid loss: 0.000304\n","Epoch:  580 | train loss: 0.000285 | valid loss: 0.000303\n","Epoch:  581 | train loss: 0.000308 | valid loss: 0.000301\n","Epoch:  582 | train loss: 0.000321 | valid loss: 0.000304\n","Epoch:  583 | train loss: 0.000304 | valid loss: 0.000301\n","Epoch:  584 | train loss: 0.000302 | valid loss: 0.000302\n","Epoch:  585 | train loss: 0.000327 | valid loss: 0.000303\n","Epoch:  586 | train loss: 0.000329 | valid loss: 0.000301\n","Epoch:  587 | train loss: 0.000308 | valid loss: 0.000301\n","Epoch:  588 | train loss: 0.000325 | valid loss: 0.000299\n","Epoch:  589 | train loss: 0.000287 | valid loss: 0.000299\n","Epoch:  590 | train loss: 0.000321 | valid loss: 0.000303\n","Epoch:  591 | train loss: 0.000246 | valid loss: 0.000299\n","Epoch:  592 | train loss: 0.000376 | valid loss: 0.000303\n","Epoch:  593 | train loss: 0.000296 | valid loss: 0.000300\n","Epoch:  594 | train loss: 0.000272 | valid loss: 0.000302\n","Epoch:  595 | train loss: 0.000311 | valid loss: 0.000300\n","Epoch:  596 | train loss: 0.000307 | valid loss: 0.000299\n","Epoch:  597 | train loss: 0.000245 | valid loss: 0.000298\n","Epoch:  598 | train loss: 0.000360 | valid loss: 0.000302\n","Epoch:  599 | train loss: 0.000322 | valid loss: 0.000300\n","Epoch:  600 | train loss: 0.000315 | valid loss: 0.000299\n","Epoch:  601 | train loss: 0.000303 | valid loss: 0.000301\n","Epoch:  602 | train loss: 0.000294 | valid loss: 0.000299\n","Epoch:  603 | train loss: 0.000362 | valid loss: 0.000297\n","Epoch:  604 | train loss: 0.000311 | valid loss: 0.000298\n","Epoch:  605 | train loss: 0.000363 | valid loss: 0.000301\n","Epoch:  606 | train loss: 0.000292 | valid loss: 0.000298\n","Epoch:  607 | train loss: 0.000290 | valid loss: 0.000296\n","Epoch:  608 | train loss: 0.000296 | valid loss: 0.000295\n","Epoch:  609 | train loss: 0.000240 | valid loss: 0.000298\n","Epoch:  610 | train loss: 0.000351 | valid loss: 0.000303\n","Epoch:  611 | train loss: 0.000346 | valid loss: 0.000299\n","Epoch:  612 | train loss: 0.000316 | valid loss: 0.000298\n","Epoch:  613 | train loss: 0.000240 | valid loss: 0.000298\n","Epoch:  614 | train loss: 0.000262 | valid loss: 0.000295\n","Epoch:  615 | train loss: 0.000253 | valid loss: 0.000299\n","Epoch:  616 | train loss: 0.000274 | valid loss: 0.000298\n","Epoch:  617 | train loss: 0.000382 | valid loss: 0.000296\n","Epoch:  618 | train loss: 0.000341 | valid loss: 0.000296\n","Epoch:  619 | train loss: 0.000349 | valid loss: 0.000305\n","Epoch:  620 | train loss: 0.000255 | valid loss: 0.000297\n","Epoch:  621 | train loss: 0.000240 | valid loss: 0.000294\n","Epoch:  622 | train loss: 0.000283 | valid loss: 0.000296\n","Epoch:  623 | train loss: 0.000334 | valid loss: 0.000299\n","Epoch:  624 | train loss: 0.000237 | valid loss: 0.000301\n","Epoch:  625 | train loss: 0.000387 | valid loss: 0.000295\n","Epoch:  626 | train loss: 0.000250 | valid loss: 0.000294\n","Epoch:  627 | train loss: 0.000293 | valid loss: 0.000296\n","Epoch:  628 | train loss: 0.000268 | valid loss: 0.000297\n","Epoch:  629 | train loss: 0.000301 | valid loss: 0.000295\n","Epoch:  630 | train loss: 0.000269 | valid loss: 0.000294\n","Epoch:  631 | train loss: 0.000352 | valid loss: 0.000300\n","Epoch:  632 | train loss: 0.000286 | valid loss: 0.000297\n","Epoch:  633 | train loss: 0.000222 | valid loss: 0.000298\n","Epoch:  634 | train loss: 0.000246 | valid loss: 0.000298\n","Epoch:  635 | train loss: 0.000236 | valid loss: 0.000296\n","Epoch:  636 | train loss: 0.000286 | valid loss: 0.000294\n","Epoch:  637 | train loss: 0.000308 | valid loss: 0.000293\n","Epoch:  638 | train loss: 0.000325 | valid loss: 0.000294\n","Epoch:  639 | train loss: 0.000385 | valid loss: 0.000299\n","Epoch:  640 | train loss: 0.000329 | valid loss: 0.000293\n","Epoch:  641 | train loss: 0.000320 | valid loss: 0.000298\n","Epoch:  642 | train loss: 0.000287 | valid loss: 0.000297\n","Epoch:  643 | train loss: 0.000306 | valid loss: 0.000295\n","Epoch:  644 | train loss: 0.000286 | valid loss: 0.000297\n","Epoch:  645 | train loss: 0.000439 | valid loss: 0.000292\n","Epoch:  646 | train loss: 0.000311 | valid loss: 0.000294\n","Epoch:  647 | train loss: 0.000279 | valid loss: 0.000292\n","Epoch:  648 | train loss: 0.000331 | valid loss: 0.000294\n","Epoch:  649 | train loss: 0.000252 | valid loss: 0.000291\n","Epoch:  650 | train loss: 0.000373 | valid loss: 0.000294\n","Epoch:  651 | train loss: 0.000275 | valid loss: 0.000291\n","Epoch:  652 | train loss: 0.000286 | valid loss: 0.000297\n","Epoch:  653 | train loss: 0.000372 | valid loss: 0.000292\n","Epoch:  654 | train loss: 0.000316 | valid loss: 0.000294\n","Epoch:  655 | train loss: 0.000332 | valid loss: 0.000291\n","Epoch:  656 | train loss: 0.000321 | valid loss: 0.000291\n","Epoch:  657 | train loss: 0.000236 | valid loss: 0.000293\n","Epoch:  658 | train loss: 0.000260 | valid loss: 0.000293\n","Epoch:  659 | train loss: 0.000240 | valid loss: 0.000291\n","Epoch:  660 | train loss: 0.000307 | valid loss: 0.000290\n","Epoch:  661 | train loss: 0.000329 | valid loss: 0.000295\n","Epoch:  662 | train loss: 0.000412 | valid loss: 0.000297\n","Epoch:  663 | train loss: 0.000286 | valid loss: 0.000291\n","Epoch:  664 | train loss: 0.000306 | valid loss: 0.000293\n","Epoch:  665 | train loss: 0.000259 | valid loss: 0.000290\n","Epoch:  666 | train loss: 0.000318 | valid loss: 0.000290\n","Epoch:  667 | train loss: 0.000254 | valid loss: 0.000291\n","Epoch:  668 | train loss: 0.000351 | valid loss: 0.000288\n","Epoch:  669 | train loss: 0.000226 | valid loss: 0.000293\n","Epoch:  670 | train loss: 0.000247 | valid loss: 0.000289\n","Epoch:  671 | train loss: 0.000362 | valid loss: 0.000291\n","Epoch:  672 | train loss: 0.000310 | valid loss: 0.000290\n","Epoch:  673 | train loss: 0.000263 | valid loss: 0.000295\n","Epoch:  674 | train loss: 0.000257 | valid loss: 0.000294\n","Epoch:  675 | train loss: 0.000296 | valid loss: 0.000288\n","Epoch:  676 | train loss: 0.000300 | valid loss: 0.000290\n","Epoch:  677 | train loss: 0.000234 | valid loss: 0.000290\n","Epoch:  678 | train loss: 0.000349 | valid loss: 0.000289\n","Epoch:  679 | train loss: 0.000267 | valid loss: 0.000290\n","Epoch:  680 | train loss: 0.000289 | valid loss: 0.000292\n","Epoch:  681 | train loss: 0.000263 | valid loss: 0.000289\n","Epoch:  682 | train loss: 0.000317 | valid loss: 0.000292\n","Epoch:  683 | train loss: 0.000320 | valid loss: 0.000291\n","Epoch:  684 | train loss: 0.000248 | valid loss: 0.000289\n","Epoch:  685 | train loss: 0.000388 | valid loss: 0.000289\n","Epoch:  686 | train loss: 0.000321 | valid loss: 0.000288\n","Epoch:  687 | train loss: 0.000350 | valid loss: 0.000289\n","Epoch:  688 | train loss: 0.000296 | valid loss: 0.000288\n","Epoch:  689 | train loss: 0.000281 | valid loss: 0.000288\n","Epoch:  690 | train loss: 0.000347 | valid loss: 0.000287\n","Epoch:  691 | train loss: 0.000301 | valid loss: 0.000291\n","Epoch:  692 | train loss: 0.000385 | valid loss: 0.000286\n","Epoch:  693 | train loss: 0.000271 | valid loss: 0.000288\n","Epoch:  694 | train loss: 0.000311 | valid loss: 0.000298\n","Epoch:  695 | train loss: 0.000287 | valid loss: 0.000288\n","Epoch:  696 | train loss: 0.000220 | valid loss: 0.000290\n","Epoch:  697 | train loss: 0.000299 | valid loss: 0.000287\n","Epoch:  698 | train loss: 0.000299 | valid loss: 0.000287\n","Epoch:  699 | train loss: 0.000384 | valid loss: 0.000287\n","Epoch:  700 | train loss: 0.000370 | valid loss: 0.000287\n","Epoch:  701 | train loss: 0.000324 | valid loss: 0.000286\n","Epoch:  702 | train loss: 0.000356 | valid loss: 0.000285\n","Epoch:  703 | train loss: 0.000313 | valid loss: 0.000288\n","Epoch:  704 | train loss: 0.000360 | valid loss: 0.000286\n","Epoch:  705 | train loss: 0.000315 | valid loss: 0.000290\n","Epoch:  706 | train loss: 0.000237 | valid loss: 0.000298\n","Epoch:  707 | train loss: 0.000340 | valid loss: 0.000288\n","Epoch:  708 | train loss: 0.000230 | valid loss: 0.000286\n","Epoch:  709 | train loss: 0.000260 | valid loss: 0.000285\n","Epoch:  710 | train loss: 0.000254 | valid loss: 0.000287\n","Epoch:  711 | train loss: 0.000261 | valid loss: 0.000285\n","Epoch:  712 | train loss: 0.000257 | valid loss: 0.000286\n","Epoch:  713 | train loss: 0.000294 | valid loss: 0.000285\n","Epoch:  714 | train loss: 0.000242 | valid loss: 0.000287\n","Epoch:  715 | train loss: 0.000336 | valid loss: 0.000286\n","Epoch:  716 | train loss: 0.000380 | valid loss: 0.000286\n","Epoch:  717 | train loss: 0.000285 | valid loss: 0.000304\n","Epoch:  718 | train loss: 0.000312 | valid loss: 0.000295\n","Epoch:  719 | train loss: 0.000324 | valid loss: 0.000285\n","Epoch:  720 | train loss: 0.000260 | valid loss: 0.000286\n","Epoch:  721 | train loss: 0.000294 | valid loss: 0.000287\n","Epoch:  722 | train loss: 0.000259 | valid loss: 0.000286\n","Epoch:  723 | train loss: 0.000250 | valid loss: 0.000285\n","Epoch:  724 | train loss: 0.000309 | valid loss: 0.000287\n","Epoch:  725 | train loss: 0.000275 | valid loss: 0.000283\n","Epoch:  726 | train loss: 0.000250 | valid loss: 0.000286\n","Epoch:  727 | train loss: 0.000287 | valid loss: 0.000284\n","Epoch:  728 | train loss: 0.000300 | valid loss: 0.000288\n","Epoch:  729 | train loss: 0.000260 | valid loss: 0.000287\n","Epoch:  730 | train loss: 0.000334 | valid loss: 0.000285\n","Epoch:  731 | train loss: 0.000413 | valid loss: 0.000282\n","Epoch:  732 | train loss: 0.000275 | valid loss: 0.000284\n","Epoch:  733 | train loss: 0.000254 | valid loss: 0.000284\n","Epoch:  734 | train loss: 0.000343 | valid loss: 0.000288\n","Epoch:  735 | train loss: 0.000215 | valid loss: 0.000286\n","Epoch:  736 | train loss: 0.000310 | valid loss: 0.000285\n","Epoch:  737 | train loss: 0.000248 | valid loss: 0.000283\n","Epoch:  738 | train loss: 0.000292 | valid loss: 0.000284\n","Epoch:  739 | train loss: 0.000237 | valid loss: 0.000285\n","Epoch:  740 | train loss: 0.000234 | valid loss: 0.000281\n","Epoch:  741 | train loss: 0.000309 | valid loss: 0.000282\n","Epoch:  742 | train loss: 0.000218 | valid loss: 0.000283\n","Epoch:  743 | train loss: 0.000359 | valid loss: 0.000286\n","Epoch:  744 | train loss: 0.000263 | valid loss: 0.000285\n","Epoch:  745 | train loss: 0.000328 | valid loss: 0.000287\n","Epoch:  746 | train loss: 0.000279 | valid loss: 0.000282\n","Epoch:  747 | train loss: 0.000313 | valid loss: 0.000285\n","Epoch:  748 | train loss: 0.000275 | valid loss: 0.000282\n","Epoch:  749 | train loss: 0.000317 | valid loss: 0.000281\n","Epoch:  750 | train loss: 0.000339 | valid loss: 0.000285\n","Epoch:  751 | train loss: 0.000216 | valid loss: 0.000283\n","Epoch:  752 | train loss: 0.000242 | valid loss: 0.000283\n","Epoch:  753 | train loss: 0.000291 | valid loss: 0.000280\n","Epoch:  754 | train loss: 0.000226 | valid loss: 0.000283\n","Epoch:  755 | train loss: 0.000334 | valid loss: 0.000282\n","Epoch:  756 | train loss: 0.000332 | valid loss: 0.000281\n","Epoch:  757 | train loss: 0.000288 | valid loss: 0.000283\n","Epoch:  758 | train loss: 0.000332 | valid loss: 0.000281\n","Epoch:  759 | train loss: 0.000308 | valid loss: 0.000283\n","Epoch:  760 | train loss: 0.000279 | valid loss: 0.000287\n","Epoch:  761 | train loss: 0.000226 | valid loss: 0.000281\n","Epoch:  762 | train loss: 0.000252 | valid loss: 0.000285\n","Epoch:  763 | train loss: 0.000311 | valid loss: 0.000283\n","Epoch:  764 | train loss: 0.000237 | valid loss: 0.000288\n","Epoch:  765 | train loss: 0.000318 | valid loss: 0.000282\n","Epoch:  766 | train loss: 0.000305 | valid loss: 0.000284\n","Epoch:  767 | train loss: 0.000320 | valid loss: 0.000284\n","Epoch:  768 | train loss: 0.000308 | valid loss: 0.000283\n","Epoch:  769 | train loss: 0.000262 | valid loss: 0.000280\n","Epoch:  770 | train loss: 0.000281 | valid loss: 0.000293\n","Epoch:  771 | train loss: 0.000289 | valid loss: 0.000280\n","Epoch:  772 | train loss: 0.000288 | valid loss: 0.000279\n","Epoch:  773 | train loss: 0.000244 | valid loss: 0.000285\n","Epoch:  774 | train loss: 0.000357 | valid loss: 0.000281\n","Epoch:  775 | train loss: 0.000508 | valid loss: 0.000288\n","Epoch:  776 | train loss: 0.000251 | valid loss: 0.000280\n","Epoch:  777 | train loss: 0.000267 | valid loss: 0.000279\n","Epoch:  778 | train loss: 0.000362 | valid loss: 0.000282\n","Epoch:  779 | train loss: 0.000239 | valid loss: 0.000281\n","Epoch:  780 | train loss: 0.000273 | valid loss: 0.000280\n","Epoch:  781 | train loss: 0.000293 | valid loss: 0.000278\n","Epoch:  782 | train loss: 0.000251 | valid loss: 0.000281\n","Epoch:  783 | train loss: 0.000269 | valid loss: 0.000279\n","Epoch:  784 | train loss: 0.000220 | valid loss: 0.000279\n","Epoch:  785 | train loss: 0.000335 | valid loss: 0.000280\n","Epoch:  786 | train loss: 0.000279 | valid loss: 0.000282\n","Epoch:  787 | train loss: 0.000310 | valid loss: 0.000280\n","Epoch:  788 | train loss: 0.000256 | valid loss: 0.000278\n","Epoch:  789 | train loss: 0.000295 | valid loss: 0.000281\n","Epoch:  790 | train loss: 0.000260 | valid loss: 0.000284\n","Epoch:  791 | train loss: 0.000347 | valid loss: 0.000279\n","Epoch:  792 | train loss: 0.000261 | valid loss: 0.000279\n","Epoch:  793 | train loss: 0.000308 | valid loss: 0.000279\n","Epoch:  794 | train loss: 0.000372 | valid loss: 0.000277\n","Epoch:  795 | train loss: 0.000247 | valid loss: 0.000279\n","Epoch:  796 | train loss: 0.000244 | valid loss: 0.000280\n","Epoch:  797 | train loss: 0.000275 | valid loss: 0.000278\n","Epoch:  798 | train loss: 0.000316 | valid loss: 0.000277\n","Epoch:  799 | train loss: 0.000270 | valid loss: 0.000278\n","Epoch:  800 | train loss: 0.000322 | valid loss: 0.000277\n","Epoch:  801 | train loss: 0.000288 | valid loss: 0.000279\n","Epoch:  802 | train loss: 0.000313 | valid loss: 0.000281\n","Epoch:  803 | train loss: 0.000328 | valid loss: 0.000277\n","Epoch:  804 | train loss: 0.000277 | valid loss: 0.000282\n","Epoch:  805 | train loss: 0.000240 | valid loss: 0.000276\n","Epoch:  806 | train loss: 0.000246 | valid loss: 0.000278\n","Epoch:  807 | train loss: 0.000356 | valid loss: 0.000277\n","Epoch:  808 | train loss: 0.000251 | valid loss: 0.000277\n","Epoch:  809 | train loss: 0.000408 | valid loss: 0.000282\n","Epoch:  810 | train loss: 0.000266 | valid loss: 0.000277\n","Epoch:  811 | train loss: 0.000261 | valid loss: 0.000277\n","Epoch:  812 | train loss: 0.000318 | valid loss: 0.000278\n","Epoch:  813 | train loss: 0.000228 | valid loss: 0.000276\n","Epoch:  814 | train loss: 0.000268 | valid loss: 0.000278\n","Epoch:  815 | train loss: 0.000280 | valid loss: 0.000277\n","Epoch:  816 | train loss: 0.000225 | valid loss: 0.000276\n","Epoch:  817 | train loss: 0.000265 | valid loss: 0.000274\n","Epoch:  818 | train loss: 0.000242 | valid loss: 0.000277\n","Epoch:  819 | train loss: 0.000254 | valid loss: 0.000277\n","Epoch:  820 | train loss: 0.000245 | valid loss: 0.000276\n","Epoch:  821 | train loss: 0.000230 | valid loss: 0.000276\n","Epoch:  822 | train loss: 0.000267 | valid loss: 0.000276\n","Epoch:  823 | train loss: 0.000260 | valid loss: 0.000278\n","Epoch:  824 | train loss: 0.000336 | valid loss: 0.000274\n","Epoch:  825 | train loss: 0.000315 | valid loss: 0.000276\n","Epoch:  826 | train loss: 0.000281 | valid loss: 0.000278\n","Epoch:  827 | train loss: 0.000239 | valid loss: 0.000275\n","Epoch:  828 | train loss: 0.000287 | valid loss: 0.000273\n","Epoch:  829 | train loss: 0.000313 | valid loss: 0.000279\n","Epoch:  830 | train loss: 0.000303 | valid loss: 0.000279\n","Epoch:  831 | train loss: 0.000309 | valid loss: 0.000276\n","Epoch:  832 | train loss: 0.000344 | valid loss: 0.000278\n","Epoch:  833 | train loss: 0.000310 | valid loss: 0.000284\n","Epoch:  834 | train loss: 0.000243 | valid loss: 0.000274\n","Epoch:  835 | train loss: 0.000232 | valid loss: 0.000275\n","Epoch:  836 | train loss: 0.000233 | valid loss: 0.000274\n","Epoch:  837 | train loss: 0.000342 | valid loss: 0.000276\n","Epoch:  838 | train loss: 0.000305 | valid loss: 0.000279\n","Epoch:  839 | train loss: 0.000308 | valid loss: 0.000275\n","Epoch:  840 | train loss: 0.000323 | valid loss: 0.000273\n","Epoch:  841 | train loss: 0.000259 | valid loss: 0.000276\n","Epoch:  842 | train loss: 0.000252 | valid loss: 0.000276\n","Epoch:  843 | train loss: 0.000236 | valid loss: 0.000274\n","Epoch:  844 | train loss: 0.000268 | valid loss: 0.000275\n","Epoch:  845 | train loss: 0.000272 | valid loss: 0.000273\n","Epoch:  846 | train loss: 0.000306 | valid loss: 0.000275\n","Epoch:  847 | train loss: 0.000517 | valid loss: 0.000279\n","Epoch:  848 | train loss: 0.000379 | valid loss: 0.000275\n","Epoch:  849 | train loss: 0.000244 | valid loss: 0.000277\n","Epoch:  850 | train loss: 0.000258 | valid loss: 0.000275\n","Epoch:  851 | train loss: 0.000243 | valid loss: 0.000273\n","Epoch:  852 | train loss: 0.000240 | valid loss: 0.000274\n","Epoch:  853 | train loss: 0.000223 | valid loss: 0.000275\n","Epoch:  854 | train loss: 0.000291 | valid loss: 0.000273\n","Epoch:  855 | train loss: 0.000229 | valid loss: 0.000273\n","Epoch:  856 | train loss: 0.000247 | valid loss: 0.000273\n","Epoch:  857 | train loss: 0.000290 | valid loss: 0.000276\n","Epoch:  858 | train loss: 0.000315 | valid loss: 0.000272\n","Epoch:  859 | train loss: 0.000239 | valid loss: 0.000273\n","Epoch:  860 | train loss: 0.000275 | valid loss: 0.000274\n","Epoch:  861 | train loss: 0.000356 | valid loss: 0.000271\n","Epoch:  862 | train loss: 0.000348 | valid loss: 0.000276\n","Epoch:  863 | train loss: 0.000281 | valid loss: 0.000277\n","Epoch:  864 | train loss: 0.000269 | valid loss: 0.000272\n","Epoch:  865 | train loss: 0.000295 | valid loss: 0.000272\n","Epoch:  866 | train loss: 0.000267 | valid loss: 0.000273\n","Epoch:  867 | train loss: 0.000326 | valid loss: 0.000276\n","Epoch:  868 | train loss: 0.000387 | valid loss: 0.000272\n","Epoch:  869 | train loss: 0.000265 | valid loss: 0.000272\n","Epoch:  870 | train loss: 0.000235 | valid loss: 0.000270\n","Epoch:  871 | train loss: 0.000322 | valid loss: 0.000271\n","Epoch:  872 | train loss: 0.000276 | valid loss: 0.000271\n","Epoch:  873 | train loss: 0.000269 | valid loss: 0.000272\n","Epoch:  874 | train loss: 0.000270 | valid loss: 0.000275\n","Epoch:  875 | train loss: 0.000210 | valid loss: 0.000271\n","Epoch:  876 | train loss: 0.000245 | valid loss: 0.000271\n","Epoch:  877 | train loss: 0.000292 | valid loss: 0.000271\n","Epoch:  878 | train loss: 0.000315 | valid loss: 0.000273\n","Epoch:  879 | train loss: 0.000240 | valid loss: 0.000272\n","Epoch:  880 | train loss: 0.000218 | valid loss: 0.000277\n","Epoch:  881 | train loss: 0.000260 | valid loss: 0.000272\n","Epoch:  882 | train loss: 0.000336 | valid loss: 0.000274\n","Epoch:  883 | train loss: 0.000242 | valid loss: 0.000275\n","Epoch:  884 | train loss: 0.000284 | valid loss: 0.000272\n","Epoch:  885 | train loss: 0.000237 | valid loss: 0.000269\n","Epoch:  886 | train loss: 0.000325 | valid loss: 0.000269\n","Epoch:  887 | train loss: 0.000316 | valid loss: 0.000270\n","Epoch:  888 | train loss: 0.000259 | valid loss: 0.000271\n","Epoch:  889 | train loss: 0.000263 | valid loss: 0.000270\n","Epoch:  890 | train loss: 0.000341 | valid loss: 0.000274\n","Epoch:  891 | train loss: 0.000212 | valid loss: 0.000271\n","Epoch:  892 | train loss: 0.000217 | valid loss: 0.000277\n","Epoch:  893 | train loss: 0.000292 | valid loss: 0.000272\n","Epoch:  894 | train loss: 0.000262 | valid loss: 0.000272\n","Epoch:  895 | train loss: 0.000219 | valid loss: 0.000269\n","Epoch:  896 | train loss: 0.000291 | valid loss: 0.000272\n","Epoch:  897 | train loss: 0.000348 | valid loss: 0.000270\n","Epoch:  898 | train loss: 0.000260 | valid loss: 0.000273\n","Epoch:  899 | train loss: 0.000318 | valid loss: 0.000271\n","Epoch:  900 | train loss: 0.000220 | valid loss: 0.000270\n","Epoch:  901 | train loss: 0.000241 | valid loss: 0.000269\n","Epoch:  902 | train loss: 0.000236 | valid loss: 0.000272\n","Epoch:  903 | train loss: 0.000318 | valid loss: 0.000287\n","Epoch:  904 | train loss: 0.000305 | valid loss: 0.000271\n","Epoch:  905 | train loss: 0.000236 | valid loss: 0.000270\n","Epoch:  906 | train loss: 0.000283 | valid loss: 0.000273\n","Epoch:  907 | train loss: 0.000298 | valid loss: 0.000269\n","Epoch:  908 | train loss: 0.000238 | valid loss: 0.000269\n","Epoch:  909 | train loss: 0.000261 | valid loss: 0.000270\n","Epoch:  910 | train loss: 0.000268 | valid loss: 0.000269\n","Epoch:  911 | train loss: 0.000234 | valid loss: 0.000267\n","Epoch:  912 | train loss: 0.000249 | valid loss: 0.000270\n","Epoch:  913 | train loss: 0.000253 | valid loss: 0.000268\n","Epoch:  914 | train loss: 0.000306 | valid loss: 0.000270\n","Epoch:  915 | train loss: 0.000334 | valid loss: 0.000270\n","Epoch:  916 | train loss: 0.000225 | valid loss: 0.000269\n","Epoch:  917 | train loss: 0.000251 | valid loss: 0.000269\n","Epoch:  918 | train loss: 0.000239 | valid loss: 0.000268\n","Epoch:  919 | train loss: 0.000322 | valid loss: 0.000268\n","Epoch:  920 | train loss: 0.000353 | valid loss: 0.000276\n","Epoch:  921 | train loss: 0.000214 | valid loss: 0.000267\n","Epoch:  922 | train loss: 0.000292 | valid loss: 0.000269\n","Epoch:  923 | train loss: 0.000256 | valid loss: 0.000273\n","Epoch:  924 | train loss: 0.000228 | valid loss: 0.000269\n","Epoch:  925 | train loss: 0.000270 | valid loss: 0.000267\n","Epoch:  926 | train loss: 0.000286 | valid loss: 0.000268\n","Epoch:  927 | train loss: 0.000217 | valid loss: 0.000272\n","Epoch:  928 | train loss: 0.000218 | valid loss: 0.000266\n","Epoch:  929 | train loss: 0.000311 | valid loss: 0.000271\n","Epoch:  930 | train loss: 0.000296 | valid loss: 0.000270\n","Epoch:  931 | train loss: 0.000276 | valid loss: 0.000266\n","Epoch:  932 | train loss: 0.000532 | valid loss: 0.000275\n","Epoch:  933 | train loss: 0.000257 | valid loss: 0.000271\n","Epoch:  934 | train loss: 0.000253 | valid loss: 0.000268\n","Epoch:  935 | train loss: 0.000297 | valid loss: 0.000267\n","Epoch:  936 | train loss: 0.000250 | valid loss: 0.000270\n","Epoch:  937 | train loss: 0.000275 | valid loss: 0.000267\n","Epoch:  938 | train loss: 0.000210 | valid loss: 0.000267\n","Epoch:  939 | train loss: 0.000356 | valid loss: 0.000266\n","Epoch:  940 | train loss: 0.000289 | valid loss: 0.000267\n","Epoch:  941 | train loss: 0.000292 | valid loss: 0.000268\n","Epoch:  942 | train loss: 0.000282 | valid loss: 0.000267\n","Epoch:  943 | train loss: 0.000296 | valid loss: 0.000268\n","Epoch:  944 | train loss: 0.000239 | valid loss: 0.000266\n","Epoch:  945 | train loss: 0.000217 | valid loss: 0.000270\n","Epoch:  946 | train loss: 0.000323 | valid loss: 0.000268\n","Epoch:  947 | train loss: 0.000279 | valid loss: 0.000265\n","Epoch:  948 | train loss: 0.000314 | valid loss: 0.000267\n","Epoch:  949 | train loss: 0.000294 | valid loss: 0.000267\n","Epoch:  950 | train loss: 0.000304 | valid loss: 0.000268\n","Epoch:  951 | train loss: 0.000312 | valid loss: 0.000266\n","Epoch:  952 | train loss: 0.000289 | valid loss: 0.000269\n","Epoch:  953 | train loss: 0.000298 | valid loss: 0.000268\n","Epoch:  954 | train loss: 0.000282 | valid loss: 0.000267\n","Epoch:  955 | train loss: 0.000276 | valid loss: 0.000271\n","Epoch:  956 | train loss: 0.000332 | valid loss: 0.000266\n","Epoch:  957 | train loss: 0.000306 | valid loss: 0.000270\n","Epoch:  958 | train loss: 0.000292 | valid loss: 0.000267\n","Epoch:  959 | train loss: 0.000243 | valid loss: 0.000267\n","Epoch:  960 | train loss: 0.000284 | valid loss: 0.000264\n","Epoch:  961 | train loss: 0.000308 | valid loss: 0.000264\n","Epoch:  962 | train loss: 0.000239 | valid loss: 0.000267\n","Epoch:  963 | train loss: 0.000239 | valid loss: 0.000264\n","Epoch:  964 | train loss: 0.000325 | valid loss: 0.000268\n","Epoch:  965 | train loss: 0.000240 | valid loss: 0.000274\n","Epoch:  966 | train loss: 0.000240 | valid loss: 0.000266\n","Epoch:  967 | train loss: 0.000339 | valid loss: 0.000265\n","Epoch:  968 | train loss: 0.000303 | valid loss: 0.000267\n","Epoch:  969 | train loss: 0.000253 | valid loss: 0.000268\n","Epoch:  970 | train loss: 0.000281 | valid loss: 0.000267\n","Epoch:  971 | train loss: 0.000349 | valid loss: 0.000265\n","Epoch:  972 | train loss: 0.000265 | valid loss: 0.000266\n","Epoch:  973 | train loss: 0.000271 | valid loss: 0.000266\n","Epoch:  974 | train loss: 0.000264 | valid loss: 0.000267\n","Epoch:  975 | train loss: 0.000274 | valid loss: 0.000266\n","Epoch:  976 | train loss: 0.000200 | valid loss: 0.000268\n","Epoch:  977 | train loss: 0.000234 | valid loss: 0.000265\n","Epoch:  978 | train loss: 0.000340 | valid loss: 0.000268\n","Epoch:  979 | train loss: 0.000227 | valid loss: 0.000264\n","Epoch:  980 | train loss: 0.000231 | valid loss: 0.000264\n","Epoch:  981 | train loss: 0.000304 | valid loss: 0.000266\n","Epoch:  982 | train loss: 0.000291 | valid loss: 0.000267\n","Epoch:  983 | train loss: 0.000199 | valid loss: 0.000266\n","Epoch:  984 | train loss: 0.000252 | valid loss: 0.000264\n","Epoch:  985 | train loss: 0.000309 | valid loss: 0.000264\n","Epoch:  986 | train loss: 0.000239 | valid loss: 0.000263\n","Epoch:  987 | train loss: 0.000262 | valid loss: 0.000268\n","Epoch:  988 | train loss: 0.000269 | valid loss: 0.000268\n","Epoch:  989 | train loss: 0.000269 | valid loss: 0.000264\n","Epoch:  990 | train loss: 0.000253 | valid loss: 0.000263\n","Epoch:  991 | train loss: 0.000246 | valid loss: 0.000264\n","Epoch:  992 | train loss: 0.000239 | valid loss: 0.000263\n","Epoch:  993 | train loss: 0.000299 | valid loss: 0.000268\n","Epoch:  994 | train loss: 0.000316 | valid loss: 0.000265\n","Epoch:  995 | train loss: 0.000306 | valid loss: 0.000269\n","Epoch:  996 | train loss: 0.000277 | valid loss: 0.000263\n","Epoch:  997 | train loss: 0.000261 | valid loss: 0.000264\n","Epoch:  998 | train loss: 0.000277 | valid loss: 0.000263\n","Epoch:  999 | train loss: 0.000271 | valid loss: 0.000264\n","Epoch:  1000 | train loss: 0.000252 | valid loss: 0.000266\n","Epoch:  1001 | train loss: 0.000257 | valid loss: 0.000263\n","Epoch:  1002 | train loss: 0.000225 | valid loss: 0.000263\n","Epoch:  1003 | train loss: 0.000229 | valid loss: 0.000262\n","Epoch:  1004 | train loss: 0.000297 | valid loss: 0.000264\n","Epoch:  1005 | train loss: 0.000233 | valid loss: 0.000265\n","Epoch:  1006 | train loss: 0.000210 | valid loss: 0.000263\n","Epoch:  1007 | train loss: 0.000266 | valid loss: 0.000261\n","Epoch:  1008 | train loss: 0.000253 | valid loss: 0.000262\n","Epoch:  1009 | train loss: 0.000244 | valid loss: 0.000265\n","Epoch:  1010 | train loss: 0.000254 | valid loss: 0.000263\n","Epoch:  1011 | train loss: 0.000247 | valid loss: 0.000262\n","Epoch:  1012 | train loss: 0.000230 | valid loss: 0.000268\n","Epoch:  1013 | train loss: 0.000269 | valid loss: 0.000267\n","Epoch:  1014 | train loss: 0.000554 | valid loss: 0.000271\n","Epoch:  1015 | train loss: 0.000317 | valid loss: 0.000265\n","Epoch:  1016 | train loss: 0.000274 | valid loss: 0.000261\n","Epoch:  1017 | train loss: 0.000237 | valid loss: 0.000262\n","Epoch:  1018 | train loss: 0.000280 | valid loss: 0.000270\n","Epoch:  1019 | train loss: 0.000298 | valid loss: 0.000266\n","Epoch:  1020 | train loss: 0.000246 | valid loss: 0.000262\n","Epoch:  1021 | train loss: 0.000301 | valid loss: 0.000261\n","Epoch:  1022 | train loss: 0.000280 | valid loss: 0.000261\n","Epoch:  1023 | train loss: 0.000268 | valid loss: 0.000268\n","Epoch:  1024 | train loss: 0.000278 | valid loss: 0.000263\n","Epoch:  1025 | train loss: 0.000239 | valid loss: 0.000264\n","Epoch:  1026 | train loss: 0.000245 | valid loss: 0.000260\n","Epoch:  1027 | train loss: 0.000212 | valid loss: 0.000264\n","Epoch:  1028 | train loss: 0.000295 | valid loss: 0.000262\n","Epoch:  1029 | train loss: 0.000267 | valid loss: 0.000261\n","Epoch:  1030 | train loss: 0.000331 | valid loss: 0.000260\n","Epoch:  1031 | train loss: 0.000266 | valid loss: 0.000266\n","Epoch:  1032 | train loss: 0.000233 | valid loss: 0.000265\n","Epoch:  1033 | train loss: 0.000287 | valid loss: 0.000262\n","Epoch:  1034 | train loss: 0.000205 | valid loss: 0.000259\n","Epoch:  1035 | train loss: 0.000287 | valid loss: 0.000266\n","Epoch:  1036 | train loss: 0.000222 | valid loss: 0.000261\n","Epoch:  1037 | train loss: 0.000302 | valid loss: 0.000262\n","Epoch:  1038 | train loss: 0.000338 | valid loss: 0.000261\n","Epoch:  1039 | train loss: 0.000210 | valid loss: 0.000261\n","Epoch:  1040 | train loss: 0.000255 | valid loss: 0.000261\n","Epoch:  1041 | train loss: 0.000211 | valid loss: 0.000264\n","Epoch:  1042 | train loss: 0.000271 | valid loss: 0.000262\n","Epoch:  1043 | train loss: 0.000242 | valid loss: 0.000262\n","Epoch:  1044 | train loss: 0.000223 | valid loss: 0.000261\n","Epoch:  1045 | train loss: 0.000225 | valid loss: 0.000262\n","Epoch:  1046 | train loss: 0.000242 | valid loss: 0.000262\n","Epoch:  1047 | train loss: 0.000316 | valid loss: 0.000263\n","Epoch:  1048 | train loss: 0.000224 | valid loss: 0.000263\n","Epoch:  1049 | train loss: 0.000238 | valid loss: 0.000262\n","Epoch:  1050 | train loss: 0.000263 | valid loss: 0.000263\n","Epoch:  1051 | train loss: 0.000230 | valid loss: 0.000259\n","Epoch:  1052 | train loss: 0.000286 | valid loss: 0.000264\n","Epoch:  1053 | train loss: 0.000352 | valid loss: 0.000262\n","Epoch:  1054 | train loss: 0.000276 | valid loss: 0.000260\n","Epoch:  1055 | train loss: 0.000256 | valid loss: 0.000262\n","Epoch:  1056 | train loss: 0.000256 | valid loss: 0.000261\n","Epoch:  1057 | train loss: 0.000223 | valid loss: 0.000264\n","Epoch:  1058 | train loss: 0.000227 | valid loss: 0.000259\n","Epoch:  1059 | train loss: 0.000246 | valid loss: 0.000260\n","Epoch:  1060 | train loss: 0.000340 | valid loss: 0.000264\n","Epoch:  1061 | train loss: 0.000274 | valid loss: 0.000260\n","Epoch:  1062 | train loss: 0.000410 | valid loss: 0.000261\n","Epoch:  1063 | train loss: 0.000254 | valid loss: 0.000259\n","Epoch:  1064 | train loss: 0.000243 | valid loss: 0.000261\n","Epoch:  1065 | train loss: 0.000339 | valid loss: 0.000268\n","Epoch:  1066 | train loss: 0.000296 | valid loss: 0.000260\n","Epoch:  1067 | train loss: 0.000285 | valid loss: 0.000258\n","Epoch:  1068 | train loss: 0.000296 | valid loss: 0.000261\n","Epoch:  1069 | train loss: 0.000274 | valid loss: 0.000261\n","Epoch:  1070 | train loss: 0.000278 | valid loss: 0.000260\n","Epoch:  1071 | train loss: 0.000266 | valid loss: 0.000263\n","Epoch:  1072 | train loss: 0.000205 | valid loss: 0.000264\n","Epoch:  1073 | train loss: 0.000249 | valid loss: 0.000261\n","Epoch:  1074 | train loss: 0.000248 | valid loss: 0.000266\n","Epoch:  1075 | train loss: 0.000205 | valid loss: 0.000258\n","Epoch:  1076 | train loss: 0.000252 | valid loss: 0.000259\n","Epoch:  1077 | train loss: 0.000245 | valid loss: 0.000258\n","Epoch:  1078 | train loss: 0.000252 | valid loss: 0.000258\n","Epoch:  1079 | train loss: 0.000290 | valid loss: 0.000258\n","Epoch:  1080 | train loss: 0.000218 | valid loss: 0.000259\n","Epoch:  1081 | train loss: 0.000225 | valid loss: 0.000260\n","Epoch:  1082 | train loss: 0.000302 | valid loss: 0.000262\n","Epoch:  1083 | train loss: 0.000354 | valid loss: 0.000264\n","Epoch:  1084 | train loss: 0.000263 | valid loss: 0.000273\n","Epoch:  1085 | train loss: 0.000326 | valid loss: 0.000263\n","Epoch:  1086 | train loss: 0.000274 | valid loss: 0.000257\n","Epoch:  1087 | train loss: 0.000270 | valid loss: 0.000257\n","Epoch:  1088 | train loss: 0.000349 | valid loss: 0.000262\n","Epoch:  1089 | train loss: 0.000241 | valid loss: 0.000260\n","Epoch:  1090 | train loss: 0.000310 | valid loss: 0.000257\n","Epoch:  1091 | train loss: 0.000252 | valid loss: 0.000257\n","Epoch:  1092 | train loss: 0.000218 | valid loss: 0.000261\n","Epoch:  1093 | train loss: 0.000273 | valid loss: 0.000258\n","Epoch:  1094 | train loss: 0.000271 | valid loss: 0.000257\n","Epoch:  1095 | train loss: 0.000263 | valid loss: 0.000259\n","Epoch:  1096 | train loss: 0.000204 | valid loss: 0.000258\n","Epoch:  1097 | train loss: 0.000236 | valid loss: 0.000259\n","Epoch:  1098 | train loss: 0.000316 | valid loss: 0.000262\n","Epoch:  1099 | train loss: 0.000245 | valid loss: 0.000260\n","Epoch:  1100 | train loss: 0.000220 | valid loss: 0.000258\n","Epoch:  1101 | train loss: 0.000271 | valid loss: 0.000257\n","Epoch:  1102 | train loss: 0.000281 | valid loss: 0.000259\n","Epoch:  1103 | train loss: 0.000248 | valid loss: 0.000257\n","Epoch:  1104 | train loss: 0.000319 | valid loss: 0.000261\n","Epoch:  1105 | train loss: 0.000213 | valid loss: 0.000260\n","Epoch:  1106 | train loss: 0.000244 | valid loss: 0.000261\n","Epoch:  1107 | train loss: 0.000223 | valid loss: 0.000258\n","Epoch:  1108 | train loss: 0.000246 | valid loss: 0.000261\n","Epoch:  1109 | train loss: 0.000286 | valid loss: 0.000257\n","Epoch:  1110 | train loss: 0.000284 | valid loss: 0.000256\n","Epoch:  1111 | train loss: 0.000286 | valid loss: 0.000259\n","Epoch:  1112 | train loss: 0.000210 | valid loss: 0.000263\n","Epoch:  1113 | train loss: 0.000291 | valid loss: 0.000256\n","Epoch:  1114 | train loss: 0.000281 | valid loss: 0.000259\n","Epoch:  1115 | train loss: 0.000339 | valid loss: 0.000259\n","Epoch:  1116 | train loss: 0.000268 | valid loss: 0.000258\n","Epoch:  1117 | train loss: 0.000288 | valid loss: 0.000260\n","Epoch:  1118 | train loss: 0.000251 | valid loss: 0.000259\n","Epoch:  1119 | train loss: 0.000266 | valid loss: 0.000255\n","Epoch:  1120 | train loss: 0.000538 | valid loss: 0.000265\n","Epoch:  1121 | train loss: 0.000309 | valid loss: 0.000260\n","Epoch:  1122 | train loss: 0.000214 | valid loss: 0.000257\n","Epoch:  1123 | train loss: 0.000324 | valid loss: 0.000261\n","Epoch:  1124 | train loss: 0.000224 | valid loss: 0.000259\n","Epoch:  1125 | train loss: 0.000281 | valid loss: 0.000255\n","Epoch:  1126 | train loss: 0.000199 | valid loss: 0.000259\n","Epoch:  1127 | train loss: 0.000249 | valid loss: 0.000258\n","Epoch:  1128 | train loss: 0.000260 | valid loss: 0.000255\n","Epoch:  1129 | train loss: 0.000287 | valid loss: 0.000258\n","Epoch:  1130 | train loss: 0.000252 | valid loss: 0.000255\n","Epoch:  1131 | train loss: 0.000307 | valid loss: 0.000256\n","Epoch:  1132 | train loss: 0.000362 | valid loss: 0.000259\n","Epoch:  1133 | train loss: 0.000495 | valid loss: 0.000263\n","Epoch:  1134 | train loss: 0.000213 | valid loss: 0.000256\n","Epoch:  1135 | train loss: 0.000265 | valid loss: 0.000255\n","Epoch:  1136 | train loss: 0.000269 | valid loss: 0.000257\n","Epoch:  1137 | train loss: 0.000305 | valid loss: 0.000259\n","Epoch:  1138 | train loss: 0.000257 | valid loss: 0.000260\n","Epoch:  1139 | train loss: 0.000227 | valid loss: 0.000255\n","Epoch:  1140 | train loss: 0.000249 | valid loss: 0.000258\n","Epoch:  1141 | train loss: 0.000245 | valid loss: 0.000254\n","Epoch:  1142 | train loss: 0.000232 | valid loss: 0.000256\n","Epoch:  1143 | train loss: 0.000251 | valid loss: 0.000258\n","Epoch:  1144 | train loss: 0.000292 | valid loss: 0.000255\n","Epoch:  1145 | train loss: 0.000241 | valid loss: 0.000258\n","Epoch:  1146 | train loss: 0.000201 | valid loss: 0.000261\n","Epoch:  1147 | train loss: 0.000355 | valid loss: 0.000259\n","Epoch:  1148 | train loss: 0.000323 | valid loss: 0.000257\n","Epoch:  1149 | train loss: 0.000333 | valid loss: 0.000256\n","Epoch:  1150 | train loss: 0.000220 | valid loss: 0.000259\n","Epoch:  1151 | train loss: 0.000246 | valid loss: 0.000257\n","Epoch:  1152 | train loss: 0.000234 | valid loss: 0.000255\n","Epoch:  1153 | train loss: 0.000565 | valid loss: 0.000260\n","Epoch:  1154 | train loss: 0.000277 | valid loss: 0.000263\n","Epoch:  1155 | train loss: 0.000265 | valid loss: 0.000253\n","Epoch:  1156 | train loss: 0.000229 | valid loss: 0.000255\n","Epoch:  1157 | train loss: 0.000313 | valid loss: 0.000254\n","Epoch:  1158 | train loss: 0.000244 | valid loss: 0.000256\n","Epoch:  1159 | train loss: 0.000230 | valid loss: 0.000260\n","Epoch:  1160 | train loss: 0.000288 | valid loss: 0.000255\n","Epoch:  1161 | train loss: 0.000223 | valid loss: 0.000256\n","Epoch:  1162 | train loss: 0.000301 | valid loss: 0.000261\n","Epoch:  1163 | train loss: 0.000216 | valid loss: 0.000253\n","Epoch:  1164 | train loss: 0.000284 | valid loss: 0.000256\n","Epoch:  1165 | train loss: 0.000274 | valid loss: 0.000255\n","Epoch:  1166 | train loss: 0.000230 | valid loss: 0.000254\n","Epoch:  1167 | train loss: 0.000233 | valid loss: 0.000257\n","Epoch:  1168 | train loss: 0.000232 | valid loss: 0.000257\n","Epoch:  1169 | train loss: 0.000237 | valid loss: 0.000254\n","Epoch:  1170 | train loss: 0.000284 | valid loss: 0.000255\n","Epoch:  1171 | train loss: 0.000258 | valid loss: 0.000253\n","Epoch:  1172 | train loss: 0.000227 | valid loss: 0.000255\n","Epoch:  1173 | train loss: 0.000254 | valid loss: 0.000255\n","Epoch:  1174 | train loss: 0.000269 | valid loss: 0.000257\n","Epoch:  1175 | train loss: 0.000376 | valid loss: 0.000258\n","Epoch:  1176 | train loss: 0.000257 | valid loss: 0.000257\n","Epoch:  1177 | train loss: 0.000382 | valid loss: 0.000255\n","Epoch:  1178 | train loss: 0.000264 | valid loss: 0.000253\n","Epoch:  1179 | train loss: 0.000305 | valid loss: 0.000255\n","Epoch:  1180 | train loss: 0.000336 | valid loss: 0.000256\n","Epoch:  1181 | train loss: 0.000227 | valid loss: 0.000255\n","Epoch:  1182 | train loss: 0.000211 | valid loss: 0.000255\n","Epoch:  1183 | train loss: 0.000277 | valid loss: 0.000253\n","Epoch:  1184 | train loss: 0.000201 | valid loss: 0.000253\n","Epoch:  1185 | train loss: 0.000212 | valid loss: 0.000254\n","Epoch:  1186 | train loss: 0.000238 | valid loss: 0.000253\n","Epoch:  1187 | train loss: 0.000234 | valid loss: 0.000253\n","Epoch:  1188 | train loss: 0.000220 | valid loss: 0.000253\n","Epoch:  1189 | train loss: 0.000223 | valid loss: 0.000255\n","Epoch:  1190 | train loss: 0.000265 | valid loss: 0.000254\n","Epoch:  1191 | train loss: 0.000242 | valid loss: 0.000264\n","Epoch:  1192 | train loss: 0.000246 | valid loss: 0.000258\n","Epoch:  1193 | train loss: 0.000300 | valid loss: 0.000253\n","Epoch:  1194 | train loss: 0.000269 | valid loss: 0.000257\n","Epoch:  1195 | train loss: 0.000227 | valid loss: 0.000255\n","Epoch:  1196 | train loss: 0.000296 | valid loss: 0.000252\n","Epoch:  1197 | train loss: 0.000221 | valid loss: 0.000254\n","Epoch:  1198 | train loss: 0.000253 | valid loss: 0.000253\n","Epoch:  1199 | train loss: 0.000260 | valid loss: 0.000255\n","Epoch:  1200 | train loss: 0.000277 | valid loss: 0.000257\n","Epoch:  1201 | train loss: 0.000208 | valid loss: 0.000256\n","Epoch:  1202 | train loss: 0.000257 | valid loss: 0.000260\n","Epoch:  1203 | train loss: 0.000259 | valid loss: 0.000254\n","Epoch:  1204 | train loss: 0.000353 | valid loss: 0.000253\n","Epoch:  1205 | train loss: 0.000339 | valid loss: 0.000253\n","Epoch:  1206 | train loss: 0.000267 | valid loss: 0.000253\n","Epoch:  1207 | train loss: 0.000241 | valid loss: 0.000252\n","Epoch:  1208 | train loss: 0.000272 | valid loss: 0.000253\n","Epoch:  1209 | train loss: 0.000272 | valid loss: 0.000254\n","Epoch:  1210 | train loss: 0.000273 | valid loss: 0.000251\n","Epoch:  1211 | train loss: 0.000246 | valid loss: 0.000254\n","Epoch:  1212 | train loss: 0.000234 | valid loss: 0.000252\n","Epoch:  1213 | train loss: 0.000217 | valid loss: 0.000252\n","Epoch:  1214 | train loss: 0.000268 | valid loss: 0.000251\n","Epoch:  1215 | train loss: 0.000290 | valid loss: 0.000251\n","Epoch:  1216 | train loss: 0.000235 | valid loss: 0.000258\n","Epoch:  1217 | train loss: 0.000240 | valid loss: 0.000253\n","Epoch:  1218 | train loss: 0.000258 | valid loss: 0.000256\n","Epoch:  1219 | train loss: 0.000331 | valid loss: 0.000253\n","Epoch:  1220 | train loss: 0.000236 | valid loss: 0.000254\n","Epoch:  1221 | train loss: 0.000228 | valid loss: 0.000252\n","Epoch:  1222 | train loss: 0.000243 | valid loss: 0.000253\n","Epoch:  1223 | train loss: 0.000287 | valid loss: 0.000254\n","Epoch:  1224 | train loss: 0.000260 | valid loss: 0.000253\n","Epoch:  1225 | train loss: 0.000242 | valid loss: 0.000252\n","Epoch:  1226 | train loss: 0.000214 | valid loss: 0.000250\n","Epoch:  1227 | train loss: 0.000300 | valid loss: 0.000251\n","Epoch:  1228 | train loss: 0.000290 | valid loss: 0.000252\n","Epoch:  1229 | train loss: 0.000226 | valid loss: 0.000253\n","Epoch:  1230 | train loss: 0.000255 | valid loss: 0.000256\n","Epoch:  1231 | train loss: 0.000237 | valid loss: 0.000257\n","Epoch:  1232 | train loss: 0.000255 | valid loss: 0.000251\n","Epoch:  1233 | train loss: 0.000247 | valid loss: 0.000250\n","Epoch:  1234 | train loss: 0.000249 | valid loss: 0.000254\n","Epoch:  1235 | train loss: 0.000282 | valid loss: 0.000252\n","Epoch:  1236 | train loss: 0.000244 | valid loss: 0.000252\n","Epoch:  1237 | train loss: 0.000271 | valid loss: 0.000253\n","Epoch:  1238 | train loss: 0.000233 | valid loss: 0.000253\n","Epoch:  1239 | train loss: 0.000281 | valid loss: 0.000251\n","Epoch:  1240 | train loss: 0.000305 | valid loss: 0.000251\n","Epoch:  1241 | train loss: 0.000310 | valid loss: 0.000254\n","Epoch:  1242 | train loss: 0.000239 | valid loss: 0.000253\n","Epoch:  1243 | train loss: 0.000253 | valid loss: 0.000250\n","Epoch:  1244 | train loss: 0.000221 | valid loss: 0.000256\n","Epoch:  1245 | train loss: 0.000221 | valid loss: 0.000252\n","Epoch:  1246 | train loss: 0.000234 | valid loss: 0.000255\n","Epoch:  1247 | train loss: 0.000277 | valid loss: 0.000250\n","Epoch:  1248 | train loss: 0.000250 | valid loss: 0.000251\n","Epoch:  1249 | train loss: 0.000216 | valid loss: 0.000250\n","Epoch:  1250 | train loss: 0.000297 | valid loss: 0.000254\n","Epoch:  1251 | train loss: 0.000271 | valid loss: 0.000252\n","Epoch:  1252 | train loss: 0.000270 | valid loss: 0.000250\n","Epoch:  1253 | train loss: 0.000216 | valid loss: 0.000252\n","Epoch:  1254 | train loss: 0.000250 | valid loss: 0.000249\n","Epoch:  1255 | train loss: 0.000221 | valid loss: 0.000252\n","Epoch:  1256 | train loss: 0.000208 | valid loss: 0.000255\n","Epoch:  1257 | train loss: 0.000272 | valid loss: 0.000249\n","Epoch:  1258 | train loss: 0.000249 | valid loss: 0.000250\n","Epoch:  1259 | train loss: 0.000260 | valid loss: 0.000252\n","Epoch:  1260 | train loss: 0.000230 | valid loss: 0.000254\n","Epoch:  1261 | train loss: 0.000228 | valid loss: 0.000251\n","Epoch:  1262 | train loss: 0.000255 | valid loss: 0.000259\n","Epoch:  1263 | train loss: 0.000238 | valid loss: 0.000252\n","Epoch:  1264 | train loss: 0.000251 | valid loss: 0.000252\n","Epoch:  1265 | train loss: 0.000239 | valid loss: 0.000250\n","Epoch:  1266 | train loss: 0.000300 | valid loss: 0.000249\n","Epoch:  1267 | train loss: 0.000234 | valid loss: 0.000249\n","Epoch:  1268 | train loss: 0.000228 | valid loss: 0.000253\n","Epoch:  1269 | train loss: 0.000250 | valid loss: 0.000250\n","Epoch:  1270 | train loss: 0.000245 | valid loss: 0.000252\n","Epoch:  1271 | train loss: 0.000267 | valid loss: 0.000252\n","Epoch:  1272 | train loss: 0.000318 | valid loss: 0.000251\n","Epoch:  1273 | train loss: 0.000252 | valid loss: 0.000251\n","Epoch:  1274 | train loss: 0.000290 | valid loss: 0.000252\n","Epoch:  1275 | train loss: 0.000308 | valid loss: 0.000272\n","Epoch:  1276 | train loss: 0.000252 | valid loss: 0.000249\n","Epoch:  1277 | train loss: 0.000255 | valid loss: 0.000249\n","Epoch:  1278 | train loss: 0.000260 | valid loss: 0.000252\n","Epoch:  1279 | train loss: 0.000234 | valid loss: 0.000248\n","Epoch:  1280 | train loss: 0.000281 | valid loss: 0.000249\n","Epoch:  1281 | train loss: 0.000268 | valid loss: 0.000249\n","Epoch:  1282 | train loss: 0.000217 | valid loss: 0.000249\n","Epoch:  1283 | train loss: 0.000281 | valid loss: 0.000252\n","Epoch:  1284 | train loss: 0.000283 | valid loss: 0.000249\n","Epoch:  1285 | train loss: 0.000335 | valid loss: 0.000255\n","Epoch:  1286 | train loss: 0.000247 | valid loss: 0.000250\n","Epoch:  1287 | train loss: 0.000288 | valid loss: 0.000251\n","Epoch:  1288 | train loss: 0.000272 | valid loss: 0.000252\n","Epoch:  1289 | train loss: 0.000290 | valid loss: 0.000249\n","Epoch:  1290 | train loss: 0.000273 | valid loss: 0.000257\n","Epoch:  1291 | train loss: 0.000263 | valid loss: 0.000250\n","Epoch:  1292 | train loss: 0.000247 | valid loss: 0.000249\n","Epoch:  1293 | train loss: 0.000255 | valid loss: 0.000252\n","Epoch:  1294 | train loss: 0.000234 | valid loss: 0.000258\n","Epoch:  1295 | train loss: 0.000239 | valid loss: 0.000248\n","Epoch:  1296 | train loss: 0.000219 | valid loss: 0.000248\n","Epoch:  1297 | train loss: 0.000266 | valid loss: 0.000250\n","Epoch:  1298 | train loss: 0.000239 | valid loss: 0.000251\n","Epoch:  1299 | train loss: 0.000228 | valid loss: 0.000248\n","Epoch:  1300 | train loss: 0.000219 | valid loss: 0.000248\n","Epoch:  1301 | train loss: 0.000254 | valid loss: 0.000248\n","Epoch:  1302 | train loss: 0.000282 | valid loss: 0.000250\n","Epoch:  1303 | train loss: 0.000241 | valid loss: 0.000251\n","Epoch:  1304 | train loss: 0.000231 | valid loss: 0.000248\n","Epoch:  1305 | train loss: 0.000298 | valid loss: 0.000254\n","Epoch:  1306 | train loss: 0.000217 | valid loss: 0.000249\n","Epoch:  1307 | train loss: 0.000256 | valid loss: 0.000249\n","Epoch:  1308 | train loss: 0.000309 | valid loss: 0.000248\n","Epoch:  1309 | train loss: 0.000219 | valid loss: 0.000249\n","Epoch:  1310 | train loss: 0.000269 | valid loss: 0.000253\n","Epoch:  1311 | train loss: 0.000208 | valid loss: 0.000248\n","Epoch:  1312 | train loss: 0.000264 | valid loss: 0.000250\n","Epoch:  1313 | train loss: 0.000239 | valid loss: 0.000248\n","Epoch:  1314 | train loss: 0.000277 | valid loss: 0.000250\n","Epoch:  1315 | train loss: 0.000373 | valid loss: 0.000255\n","Epoch:  1316 | train loss: 0.000321 | valid loss: 0.000249\n","Epoch:  1317 | train loss: 0.000231 | valid loss: 0.000247\n","Epoch:  1318 | train loss: 0.000214 | valid loss: 0.000247\n","Epoch:  1319 | train loss: 0.000212 | valid loss: 0.000254\n","Epoch:  1320 | train loss: 0.000304 | valid loss: 0.000250\n","Epoch:  1321 | train loss: 0.000248 | valid loss: 0.000248\n","Epoch:  1322 | train loss: 0.000213 | valid loss: 0.000250\n","Epoch:  1323 | train loss: 0.000188 | valid loss: 0.000249\n","Epoch:  1324 | train loss: 0.000212 | valid loss: 0.000247\n","Epoch:  1325 | train loss: 0.000242 | valid loss: 0.000254\n","Epoch:  1326 | train loss: 0.000231 | valid loss: 0.000250\n","Epoch:  1327 | train loss: 0.000330 | valid loss: 0.000248\n","Epoch:  1328 | train loss: 0.000299 | valid loss: 0.000249\n","Epoch:  1329 | train loss: 0.000284 | valid loss: 0.000247\n","Epoch:  1330 | train loss: 0.000241 | valid loss: 0.000249\n","Epoch:  1331 | train loss: 0.000267 | valid loss: 0.000253\n","Epoch:  1332 | train loss: 0.000243 | valid loss: 0.000249\n","Epoch:  1333 | train loss: 0.000234 | valid loss: 0.000251\n","Epoch:  1334 | train loss: 0.000229 | valid loss: 0.000249\n","Epoch:  1335 | train loss: 0.000301 | valid loss: 0.000248\n","Epoch:  1336 | train loss: 0.000292 | valid loss: 0.000247\n","Epoch:  1337 | train loss: 0.000280 | valid loss: 0.000246\n","Epoch:  1338 | train loss: 0.000250 | valid loss: 0.000248\n","Epoch:  1339 | train loss: 0.000257 | valid loss: 0.000248\n","Epoch:  1340 | train loss: 0.000218 | valid loss: 0.000249\n","Epoch:  1341 | train loss: 0.000240 | valid loss: 0.000247\n","Epoch:  1342 | train loss: 0.000243 | valid loss: 0.000247\n","Epoch:  1343 | train loss: 0.000263 | valid loss: 0.000248\n","Epoch:  1344 | train loss: 0.000192 | valid loss: 0.000254\n","Epoch:  1345 | train loss: 0.000250 | valid loss: 0.000251\n","Epoch:  1346 | train loss: 0.000319 | valid loss: 0.000246\n","Epoch:  1347 | train loss: 0.000206 | valid loss: 0.000246\n","Epoch:  1348 | train loss: 0.000261 | valid loss: 0.000249\n","Epoch:  1349 | train loss: 0.000228 | valid loss: 0.000251\n","Epoch:  1350 | train loss: 0.000294 | valid loss: 0.000254\n","Epoch:  1351 | train loss: 0.000278 | valid loss: 0.000247\n","Epoch:  1352 | train loss: 0.000291 | valid loss: 0.000247\n","Epoch:  1353 | train loss: 0.000296 | valid loss: 0.000247\n","Epoch:  1354 | train loss: 0.000234 | valid loss: 0.000245\n","Epoch:  1355 | train loss: 0.000225 | valid loss: 0.000246\n","Epoch:  1356 | train loss: 0.000203 | valid loss: 0.000249\n","Epoch:  1357 | train loss: 0.000228 | valid loss: 0.000248\n","Epoch:  1358 | train loss: 0.000246 | valid loss: 0.000247\n","Epoch:  1359 | train loss: 0.000258 | valid loss: 0.000246\n","Epoch:  1360 | train loss: 0.000235 | valid loss: 0.000247\n","Epoch:  1361 | train loss: 0.000204 | valid loss: 0.000250\n","Epoch:  1362 | train loss: 0.000212 | valid loss: 0.000247\n","Epoch:  1363 | train loss: 0.000205 | valid loss: 0.000246\n","Epoch:  1364 | train loss: 0.000343 | valid loss: 0.000245\n","Epoch:  1365 | train loss: 0.000251 | valid loss: 0.000246\n","Epoch:  1366 | train loss: 0.000301 | valid loss: 0.000246\n","Epoch:  1367 | train loss: 0.000245 | valid loss: 0.000250\n","Epoch:  1368 | train loss: 0.000281 | valid loss: 0.000249\n","Epoch:  1369 | train loss: 0.000267 | valid loss: 0.000246\n","Epoch:  1370 | train loss: 0.000271 | valid loss: 0.000250\n","Epoch:  1371 | train loss: 0.000238 | valid loss: 0.000247\n","Epoch:  1372 | train loss: 0.000218 | valid loss: 0.000248\n","Epoch:  1373 | train loss: 0.000304 | valid loss: 0.000246\n","Epoch:  1374 | train loss: 0.000260 | valid loss: 0.000246\n","Epoch:  1375 | train loss: 0.000263 | valid loss: 0.000245\n","Epoch:  1376 | train loss: 0.000217 | valid loss: 0.000248\n","Epoch:  1377 | train loss: 0.000242 | valid loss: 0.000246\n","Epoch:  1378 | train loss: 0.000266 | valid loss: 0.000247\n","Epoch:  1379 | train loss: 0.000223 | valid loss: 0.000248\n","Epoch:  1380 | train loss: 0.000287 | valid loss: 0.000251\n","Epoch:  1381 | train loss: 0.000256 | valid loss: 0.000247\n","Epoch:  1382 | train loss: 0.000206 | valid loss: 0.000245\n","Epoch:  1383 | train loss: 0.000242 | valid loss: 0.000246\n","Epoch:  1384 | train loss: 0.000269 | valid loss: 0.000249\n","Epoch:  1385 | train loss: 0.000261 | valid loss: 0.000247\n","Epoch:  1386 | train loss: 0.000265 | valid loss: 0.000247\n","Epoch:  1387 | train loss: 0.000245 | valid loss: 0.000246\n","Epoch:  1388 | train loss: 0.000232 | valid loss: 0.000247\n","Epoch:  1389 | train loss: 0.000310 | valid loss: 0.000250\n","Epoch:  1390 | train loss: 0.000239 | valid loss: 0.000247\n","Epoch:  1391 | train loss: 0.000276 | valid loss: 0.000246\n","Epoch:  1392 | train loss: 0.000225 | valid loss: 0.000245\n","Epoch:  1393 | train loss: 0.000278 | valid loss: 0.000248\n","Epoch:  1394 | train loss: 0.000211 | valid loss: 0.000246\n","Epoch:  1395 | train loss: 0.000564 | valid loss: 0.000251\n","Epoch:  1396 | train loss: 0.000265 | valid loss: 0.000249\n","Epoch:  1397 | train loss: 0.000240 | valid loss: 0.000246\n","Epoch:  1398 | train loss: 0.000265 | valid loss: 0.000245\n","Epoch:  1399 | train loss: 0.000285 | valid loss: 0.000247\n","Epoch:  1400 | train loss: 0.000248 | valid loss: 0.000245\n","Epoch:  1401 | train loss: 0.000231 | valid loss: 0.000245\n","Epoch:  1402 | train loss: 0.000253 | valid loss: 0.000245\n","Epoch:  1403 | train loss: 0.000218 | valid loss: 0.000247\n","Epoch:  1404 | train loss: 0.000189 | valid loss: 0.000246\n","Epoch:  1405 | train loss: 0.000211 | valid loss: 0.000247\n","Epoch:  1406 | train loss: 0.000196 | valid loss: 0.000247\n","Epoch:  1407 | train loss: 0.000237 | valid loss: 0.000246\n","Epoch:  1408 | train loss: 0.000189 | valid loss: 0.000249\n","Epoch:  1409 | train loss: 0.000205 | valid loss: 0.000247\n","Epoch:  1410 | train loss: 0.000265 | valid loss: 0.000246\n","Epoch:  1411 | train loss: 0.000202 | valid loss: 0.000248\n","Epoch:  1412 | train loss: 0.000255 | valid loss: 0.000246\n","Epoch:  1413 | train loss: 0.000232 | valid loss: 0.000246\n","Epoch:  1414 | train loss: 0.000298 | valid loss: 0.000245\n","Epoch:  1415 | train loss: 0.000338 | valid loss: 0.000247\n","Epoch:  1416 | train loss: 0.000248 | valid loss: 0.000246\n","Epoch:  1417 | train loss: 0.000230 | valid loss: 0.000246\n","Epoch:  1418 | train loss: 0.000298 | valid loss: 0.000244\n","Epoch:  1419 | train loss: 0.000215 | valid loss: 0.000247\n","Epoch:  1420 | train loss: 0.000182 | valid loss: 0.000246\n","Epoch:  1421 | train loss: 0.000228 | valid loss: 0.000247\n","Epoch:  1422 | train loss: 0.000270 | valid loss: 0.000246\n","Epoch:  1423 | train loss: 0.000239 | valid loss: 0.000246\n","Epoch:  1424 | train loss: 0.000241 | valid loss: 0.000246\n","Epoch:  1425 | train loss: 0.000218 | valid loss: 0.000246\n","Epoch:  1426 | train loss: 0.000186 | valid loss: 0.000246\n","Epoch:  1427 | train loss: 0.000296 | valid loss: 0.000257\n","Epoch:  1428 | train loss: 0.000205 | valid loss: 0.000244\n","Epoch:  1429 | train loss: 0.000231 | valid loss: 0.000244\n","Epoch:  1430 | train loss: 0.000257 | valid loss: 0.000246\n","Epoch:  1431 | train loss: 0.000304 | valid loss: 0.000245\n","Epoch:  1432 | train loss: 0.000229 | valid loss: 0.000246\n","Epoch:  1433 | train loss: 0.000263 | valid loss: 0.000246\n","Epoch:  1434 | train loss: 0.000261 | valid loss: 0.000245\n","Epoch:  1435 | train loss: 0.000256 | valid loss: 0.000244\n","Epoch:  1436 | train loss: 0.000265 | valid loss: 0.000250\n","Epoch:  1437 | train loss: 0.000242 | valid loss: 0.000243\n","Epoch:  1438 | train loss: 0.000264 | valid loss: 0.000244\n","Epoch:  1439 | train loss: 0.000242 | valid loss: 0.000242\n","Epoch:  1440 | train loss: 0.000188 | valid loss: 0.000243\n","Epoch:  1441 | train loss: 0.000237 | valid loss: 0.000245\n","Epoch:  1442 | train loss: 0.000262 | valid loss: 0.000244\n","Epoch:  1443 | train loss: 0.000251 | valid loss: 0.000250\n","Epoch:  1444 | train loss: 0.000278 | valid loss: 0.000244\n","Epoch:  1445 | train loss: 0.000205 | valid loss: 0.000244\n","Epoch:  1446 | train loss: 0.000241 | valid loss: 0.000246\n","Epoch:  1447 | train loss: 0.000252 | valid loss: 0.000246\n","Epoch:  1448 | train loss: 0.000264 | valid loss: 0.000242\n","Epoch:  1449 | train loss: 0.000318 | valid loss: 0.000244\n","Epoch:  1450 | train loss: 0.000226 | valid loss: 0.000247\n","Epoch:  1451 | train loss: 0.000226 | valid loss: 0.000247\n","Epoch:  1452 | train loss: 0.000215 | valid loss: 0.000243\n","Epoch:  1453 | train loss: 0.000260 | valid loss: 0.000245\n","Epoch:  1454 | train loss: 0.000255 | valid loss: 0.000243\n","Epoch:  1455 | train loss: 0.000212 | valid loss: 0.000249\n","Epoch:  1456 | train loss: 0.000236 | valid loss: 0.000245\n","Epoch:  1457 | train loss: 0.000264 | valid loss: 0.000247\n","Epoch:  1458 | train loss: 0.000398 | valid loss: 0.000245\n","Epoch:  1459 | train loss: 0.000250 | valid loss: 0.000242\n","Epoch:  1460 | train loss: 0.000255 | valid loss: 0.000246\n","Epoch:  1461 | train loss: 0.000248 | valid loss: 0.000243\n","Epoch:  1462 | train loss: 0.000255 | valid loss: 0.000243\n","Epoch:  1463 | train loss: 0.000208 | valid loss: 0.000242\n","Epoch:  1464 | train loss: 0.000485 | valid loss: 0.000246\n","Epoch:  1465 | train loss: 0.000284 | valid loss: 0.000244\n","Epoch:  1466 | train loss: 0.000244 | valid loss: 0.000241\n","Epoch:  1467 | train loss: 0.000256 | valid loss: 0.000242\n","Epoch:  1468 | train loss: 0.000274 | valid loss: 0.000244\n","Epoch:  1469 | train loss: 0.000312 | valid loss: 0.000246\n","Epoch:  1470 | train loss: 0.000243 | valid loss: 0.000246\n","Epoch:  1471 | train loss: 0.000286 | valid loss: 0.000244\n","Epoch:  1472 | train loss: 0.000324 | valid loss: 0.000247\n","Epoch:  1473 | train loss: 0.000229 | valid loss: 0.000242\n","Epoch:  1474 | train loss: 0.000232 | valid loss: 0.000252\n","Epoch:  1475 | train loss: 0.000344 | valid loss: 0.000245\n","Epoch:  1476 | train loss: 0.000564 | valid loss: 0.000248\n","Epoch:  1477 | train loss: 0.000264 | valid loss: 0.000243\n","Epoch:  1478 | train loss: 0.000259 | valid loss: 0.000245\n","Epoch:  1479 | train loss: 0.000235 | valid loss: 0.000246\n","Epoch:  1480 | train loss: 0.000284 | valid loss: 0.000241\n","Epoch:  1481 | train loss: 0.000195 | valid loss: 0.000244\n","Epoch:  1482 | train loss: 0.000223 | valid loss: 0.000243\n","Epoch:  1483 | train loss: 0.000215 | valid loss: 0.000243\n","Epoch:  1484 | train loss: 0.000250 | valid loss: 0.000243\n","Epoch:  1485 | train loss: 0.000186 | valid loss: 0.000243\n","Epoch:  1486 | train loss: 0.000270 | valid loss: 0.000243\n","Epoch:  1487 | train loss: 0.000229 | valid loss: 0.000243\n","Epoch:  1488 | train loss: 0.000271 | valid loss: 0.000245\n","Epoch:  1489 | train loss: 0.000242 | valid loss: 0.000243\n","Epoch:  1490 | train loss: 0.000249 | valid loss: 0.000247\n","Epoch:  1491 | train loss: 0.000234 | valid loss: 0.000243\n","Epoch:  1492 | train loss: 0.000222 | valid loss: 0.000241\n","Epoch:  1493 | train loss: 0.000274 | valid loss: 0.000242\n","Epoch:  1494 | train loss: 0.000216 | valid loss: 0.000241\n","Epoch:  1495 | train loss: 0.000271 | valid loss: 0.000243\n","Epoch:  1496 | train loss: 0.000241 | valid loss: 0.000245\n","Epoch:  1497 | train loss: 0.000254 | valid loss: 0.000243\n","Epoch:  1498 | train loss: 0.000239 | valid loss: 0.000242\n","Epoch:  1499 | train loss: 0.000241 | valid loss: 0.000242\n","Epoch:  1500 | train loss: 0.000347 | valid loss: 0.000242\n","Epoch:  1501 | train loss: 0.000188 | valid loss: 0.000241\n","Epoch:  1502 | train loss: 0.000236 | valid loss: 0.000249\n","Epoch:  1503 | train loss: 0.000241 | valid loss: 0.000245\n","Epoch:  1504 | train loss: 0.000265 | valid loss: 0.000244\n","Epoch:  1505 | train loss: 0.000454 | valid loss: 0.000245\n","Epoch:  1506 | train loss: 0.000274 | valid loss: 0.000246\n","Epoch:  1507 | train loss: 0.000243 | valid loss: 0.000244\n","Epoch:  1508 | train loss: 0.000230 | valid loss: 0.000242\n","Epoch:  1509 | train loss: 0.000235 | valid loss: 0.000242\n","Epoch:  1510 | train loss: 0.000248 | valid loss: 0.000242\n","Epoch:  1511 | train loss: 0.000244 | valid loss: 0.000244\n","Epoch:  1512 | train loss: 0.000229 | valid loss: 0.000241\n","Epoch:  1513 | train loss: 0.000268 | valid loss: 0.000244\n","Epoch:  1514 | train loss: 0.000277 | valid loss: 0.000243\n","Epoch:  1515 | train loss: 0.000246 | valid loss: 0.000245\n","Epoch:  1516 | train loss: 0.000290 | valid loss: 0.000245\n","Epoch:  1517 | train loss: 0.000187 | valid loss: 0.000243\n","Epoch:  1518 | train loss: 0.000269 | valid loss: 0.000247\n","Epoch:  1519 | train loss: 0.000241 | valid loss: 0.000245\n","Epoch:  1520 | train loss: 0.000205 | valid loss: 0.000243\n","Epoch:  1521 | train loss: 0.000297 | valid loss: 0.000241\n","Epoch:  1522 | train loss: 0.000252 | valid loss: 0.000244\n","Epoch:  1523 | train loss: 0.000239 | valid loss: 0.000242\n","Epoch:  1524 | train loss: 0.000242 | valid loss: 0.000243\n","Epoch:  1525 | train loss: 0.000251 | valid loss: 0.000242\n","Epoch:  1526 | train loss: 0.000265 | valid loss: 0.000248\n","Epoch:  1527 | train loss: 0.000271 | valid loss: 0.000242\n","Epoch:  1528 | train loss: 0.000216 | valid loss: 0.000243\n","Epoch:  1529 | train loss: 0.000201 | valid loss: 0.000245\n","Epoch:  1530 | train loss: 0.000228 | valid loss: 0.000242\n","Epoch:  1531 | train loss: 0.000207 | valid loss: 0.000240\n","Epoch:  1532 | train loss: 0.000214 | valid loss: 0.000240\n","Epoch:  1533 | train loss: 0.000316 | valid loss: 0.000246\n","Epoch:  1534 | train loss: 0.000338 | valid loss: 0.000247\n","Epoch:  1535 | train loss: 0.000257 | valid loss: 0.000243\n","Epoch:  1536 | train loss: 0.000252 | valid loss: 0.000242\n","Epoch:  1537 | train loss: 0.000256 | valid loss: 0.000242\n","Epoch:  1538 | train loss: 0.000219 | valid loss: 0.000240\n","Epoch:  1539 | train loss: 0.000224 | valid loss: 0.000241\n","Epoch:  1540 | train loss: 0.000283 | valid loss: 0.000242\n","Epoch:  1541 | train loss: 0.000216 | valid loss: 0.000243\n","Epoch:  1542 | train loss: 0.000268 | valid loss: 0.000244\n","Epoch:  1543 | train loss: 0.000264 | valid loss: 0.000240\n","Epoch:  1544 | train loss: 0.000218 | valid loss: 0.000241\n","Epoch:  1545 | train loss: 0.000211 | valid loss: 0.000248\n","Epoch:  1546 | train loss: 0.000218 | valid loss: 0.000245\n","Epoch:  1547 | train loss: 0.000263 | valid loss: 0.000242\n","Epoch:  1548 | train loss: 0.000257 | valid loss: 0.000243\n","Epoch:  1549 | train loss: 0.000265 | valid loss: 0.000240\n","Epoch:  1550 | train loss: 0.000195 | valid loss: 0.000242\n","Epoch:  1551 | train loss: 0.000187 | valid loss: 0.000239\n","Epoch:  1552 | train loss: 0.000267 | valid loss: 0.000240\n","Epoch:  1553 | train loss: 0.000265 | valid loss: 0.000241\n","Epoch:  1554 | train loss: 0.000250 | valid loss: 0.000240\n","Epoch:  1555 | train loss: 0.000251 | valid loss: 0.000240\n","Epoch:  1556 | train loss: 0.000230 | valid loss: 0.000240\n","Epoch:  1557 | train loss: 0.000247 | valid loss: 0.000242\n","Epoch:  1558 | train loss: 0.000232 | valid loss: 0.000245\n","Epoch:  1559 | train loss: 0.000308 | valid loss: 0.000242\n","Epoch:  1560 | train loss: 0.000266 | valid loss: 0.000242\n","Epoch:  1561 | train loss: 0.000214 | valid loss: 0.000241\n","Epoch:  1562 | train loss: 0.000233 | valid loss: 0.000250\n","Epoch:  1563 | train loss: 0.000234 | valid loss: 0.000241\n","Epoch:  1564 | train loss: 0.000262 | valid loss: 0.000240\n","Epoch:  1565 | train loss: 0.000212 | valid loss: 0.000239\n","Epoch:  1566 | train loss: 0.000229 | valid loss: 0.000238\n","Epoch:  1567 | train loss: 0.000227 | valid loss: 0.000241\n","Epoch:  1568 | train loss: 0.000250 | valid loss: 0.000239\n","Epoch:  1569 | train loss: 0.000235 | valid loss: 0.000239\n","Epoch:  1570 | train loss: 0.000231 | valid loss: 0.000241\n","Epoch:  1571 | train loss: 0.000436 | valid loss: 0.000247\n","Epoch:  1572 | train loss: 0.000228 | valid loss: 0.000240\n","Epoch:  1573 | train loss: 0.000243 | valid loss: 0.000239\n","Epoch:  1574 | train loss: 0.000222 | valid loss: 0.000239\n","Epoch:  1575 | train loss: 0.000255 | valid loss: 0.000240\n","Epoch:  1576 | train loss: 0.000239 | valid loss: 0.000247\n","Epoch:  1577 | train loss: 0.000249 | valid loss: 0.000240\n","Epoch:  1578 | train loss: 0.000230 | valid loss: 0.000240\n","Epoch:  1579 | train loss: 0.000274 | valid loss: 0.000241\n","Epoch:  1580 | train loss: 0.000245 | valid loss: 0.000240\n","Epoch:  1581 | train loss: 0.000298 | valid loss: 0.000238\n","Epoch:  1582 | train loss: 0.000346 | valid loss: 0.000240\n","Epoch:  1583 | train loss: 0.000274 | valid loss: 0.000247\n","Epoch:  1584 | train loss: 0.000275 | valid loss: 0.000241\n","Epoch:  1585 | train loss: 0.000237 | valid loss: 0.000241\n","Epoch:  1586 | train loss: 0.000228 | valid loss: 0.000244\n","Epoch:  1587 | train loss: 0.000222 | valid loss: 0.000239\n","Epoch:  1588 | train loss: 0.000222 | valid loss: 0.000239\n","Epoch:  1589 | train loss: 0.000451 | valid loss: 0.000246\n","Epoch:  1590 | train loss: 0.000201 | valid loss: 0.000243\n","Epoch:  1591 | train loss: 0.000288 | valid loss: 0.000240\n","Epoch:  1592 | train loss: 0.000290 | valid loss: 0.000239\n","Epoch:  1593 | train loss: 0.000218 | valid loss: 0.000242\n","Epoch:  1594 | train loss: 0.000219 | valid loss: 0.000242\n","Epoch:  1595 | train loss: 0.000221 | valid loss: 0.000242\n","Epoch:  1596 | train loss: 0.000215 | valid loss: 0.000238\n","Epoch:  1597 | train loss: 0.000260 | valid loss: 0.000239\n","Epoch:  1598 | train loss: 0.000299 | valid loss: 0.000239\n","Epoch:  1599 | train loss: 0.000337 | valid loss: 0.000241\n","Epoch:  1600 | train loss: 0.000299 | valid loss: 0.000241\n","Epoch:  1601 | train loss: 0.000278 | valid loss: 0.000238\n","Epoch:  1602 | train loss: 0.000233 | valid loss: 0.000239\n","Epoch:  1603 | train loss: 0.000190 | valid loss: 0.000240\n","Epoch:  1604 | train loss: 0.000249 | valid loss: 0.000237\n","Epoch:  1605 | train loss: 0.000181 | valid loss: 0.000239\n","Epoch:  1606 | train loss: 0.000259 | valid loss: 0.000240\n","Epoch:  1607 | train loss: 0.000292 | valid loss: 0.000241\n","Epoch:  1608 | train loss: 0.000229 | valid loss: 0.000241\n","Epoch:  1609 | train loss: 0.000214 | valid loss: 0.000243\n","Epoch:  1610 | train loss: 0.000260 | valid loss: 0.000240\n","Epoch:  1611 | train loss: 0.000217 | valid loss: 0.000240\n","Epoch:  1612 | train loss: 0.000212 | valid loss: 0.000242\n","Epoch:  1613 | train loss: 0.000192 | valid loss: 0.000245\n","Epoch:  1614 | train loss: 0.000207 | valid loss: 0.000241\n","Epoch:  1615 | train loss: 0.000236 | valid loss: 0.000238\n","Epoch:  1616 | train loss: 0.000243 | valid loss: 0.000238\n","Epoch:  1617 | train loss: 0.000280 | valid loss: 0.000240\n","Epoch:  1618 | train loss: 0.000457 | valid loss: 0.000240\n","Epoch:  1619 | train loss: 0.000259 | valid loss: 0.000239\n","Epoch:  1620 | train loss: 0.000318 | valid loss: 0.000240\n","Epoch:  1621 | train loss: 0.000226 | valid loss: 0.000239\n","Epoch:  1622 | train loss: 0.000278 | valid loss: 0.000240\n","Epoch:  1623 | train loss: 0.000210 | valid loss: 0.000238\n","Epoch:  1624 | train loss: 0.000208 | valid loss: 0.000238\n","Epoch:  1625 | train loss: 0.000289 | valid loss: 0.000247\n","Epoch:  1626 | train loss: 0.000234 | valid loss: 0.000242\n","Epoch:  1627 | train loss: 0.000198 | valid loss: 0.000239\n","Epoch:  1628 | train loss: 0.000268 | valid loss: 0.000239\n","Epoch:  1629 | train loss: 0.000224 | valid loss: 0.000239\n","Epoch:  1630 | train loss: 0.000229 | valid loss: 0.000240\n","Epoch:  1631 | train loss: 0.000310 | valid loss: 0.000237\n","Epoch:  1632 | train loss: 0.000209 | valid loss: 0.000242\n","Epoch:  1633 | train loss: 0.000268 | valid loss: 0.000237\n","Epoch:  1634 | train loss: 0.000273 | valid loss: 0.000237\n","Epoch:  1635 | train loss: 0.000195 | valid loss: 0.000238\n","Epoch:  1636 | train loss: 0.000217 | valid loss: 0.000237\n","Epoch:  1637 | train loss: 0.000317 | valid loss: 0.000238\n","Epoch:  1638 | train loss: 0.000222 | valid loss: 0.000242\n","Epoch:  1639 | train loss: 0.000246 | valid loss: 0.000239\n","Epoch:  1640 | train loss: 0.000224 | valid loss: 0.000243\n","Epoch:  1641 | train loss: 0.000351 | valid loss: 0.000243\n","Epoch:  1642 | train loss: 0.000181 | valid loss: 0.000239\n","Epoch:  1643 | train loss: 0.000257 | valid loss: 0.000239\n","Epoch:  1644 | train loss: 0.000256 | valid loss: 0.000237\n","Epoch:  1645 | train loss: 0.000277 | valid loss: 0.000239\n","Epoch:  1646 | train loss: 0.000358 | valid loss: 0.000242\n","Epoch:  1647 | train loss: 0.000265 | valid loss: 0.000242\n","Epoch:  1648 | train loss: 0.000268 | valid loss: 0.000242\n","Epoch:  1649 | train loss: 0.000193 | valid loss: 0.000239\n","Epoch:  1650 | train loss: 0.000193 | valid loss: 0.000238\n","Epoch:  1651 | train loss: 0.000226 | valid loss: 0.000236\n","Epoch:  1652 | train loss: 0.000267 | valid loss: 0.000239\n","Epoch:  1653 | train loss: 0.000198 | valid loss: 0.000240\n","Epoch:  1654 | train loss: 0.000219 | valid loss: 0.000239\n","Epoch:  1655 | train loss: 0.000219 | valid loss: 0.000238\n","Epoch:  1656 | train loss: 0.000225 | valid loss: 0.000241\n","Epoch:  1657 | train loss: 0.000209 | valid loss: 0.000238\n","Epoch:  1658 | train loss: 0.000231 | valid loss: 0.000236\n","Epoch:  1659 | train loss: 0.000177 | valid loss: 0.000239\n","Epoch:  1660 | train loss: 0.000235 | valid loss: 0.000240\n","Epoch:  1661 | train loss: 0.000218 | valid loss: 0.000241\n","Epoch:  1662 | train loss: 0.000242 | valid loss: 0.000241\n","Epoch:  1663 | train loss: 0.000265 | valid loss: 0.000236\n","Epoch:  1664 | train loss: 0.000239 | valid loss: 0.000240\n","Epoch:  1665 | train loss: 0.000297 | valid loss: 0.000239\n","Epoch:  1666 | train loss: 0.000235 | valid loss: 0.000237\n","Epoch:  1667 | train loss: 0.000505 | valid loss: 0.000239\n","Epoch:  1668 | train loss: 0.000294 | valid loss: 0.000240\n","Epoch:  1669 | train loss: 0.000232 | valid loss: 0.000242\n","Epoch:  1670 | train loss: 0.000315 | valid loss: 0.000239\n","Epoch:  1671 | train loss: 0.000180 | valid loss: 0.000237\n","Epoch:  1672 | train loss: 0.000200 | valid loss: 0.000241\n","Epoch:  1673 | train loss: 0.000272 | valid loss: 0.000238\n","Epoch:  1674 | train loss: 0.000211 | valid loss: 0.000239\n","Epoch:  1675 | train loss: 0.000232 | valid loss: 0.000242\n","Epoch:  1676 | train loss: 0.000214 | valid loss: 0.000239\n","Epoch:  1677 | train loss: 0.000269 | valid loss: 0.000238\n","Epoch:  1678 | train loss: 0.000189 | valid loss: 0.000238\n","Epoch:  1679 | train loss: 0.000258 | valid loss: 0.000238\n","Epoch:  1680 | train loss: 0.000241 | valid loss: 0.000238\n","Epoch:  1681 | train loss: 0.000226 | valid loss: 0.000239\n","Epoch:  1682 | train loss: 0.000232 | valid loss: 0.000237\n","Epoch:  1683 | train loss: 0.000245 | valid loss: 0.000238\n","Epoch:  1684 | train loss: 0.000280 | valid loss: 0.000237\n","Epoch:  1685 | train loss: 0.000205 | valid loss: 0.000239\n","Epoch:  1686 | train loss: 0.000276 | valid loss: 0.000236\n","Epoch:  1687 | train loss: 0.000237 | valid loss: 0.000237\n","Epoch:  1688 | train loss: 0.000178 | valid loss: 0.000239\n","Epoch:  1689 | train loss: 0.000282 | valid loss: 0.000240\n","Epoch:  1690 | train loss: 0.000267 | valid loss: 0.000237\n","Epoch:  1691 | train loss: 0.000230 | valid loss: 0.000236\n","Epoch:  1692 | train loss: 0.000230 | valid loss: 0.000237\n","Epoch:  1693 | train loss: 0.000206 | valid loss: 0.000237\n","Epoch:  1694 | train loss: 0.000270 | valid loss: 0.000239\n","Epoch:  1695 | train loss: 0.000272 | valid loss: 0.000242\n","Epoch:  1696 | train loss: 0.000260 | valid loss: 0.000237\n","Epoch:  1697 | train loss: 0.000305 | valid loss: 0.000239\n","Epoch:  1698 | train loss: 0.000240 | valid loss: 0.000238\n","Epoch:  1699 | train loss: 0.000247 | valid loss: 0.000239\n","Epoch:  1700 | train loss: 0.000252 | valid loss: 0.000238\n","Epoch:  1701 | train loss: 0.000182 | valid loss: 0.000237\n","Epoch:  1702 | train loss: 0.000310 | valid loss: 0.000237\n","Epoch:  1703 | train loss: 0.000250 | valid loss: 0.000236\n","Epoch:  1704 | train loss: 0.000231 | valid loss: 0.000237\n","Epoch:  1705 | train loss: 0.000219 | valid loss: 0.000239\n","Epoch:  1706 | train loss: 0.000194 | valid loss: 0.000237\n","Epoch:  1707 | train loss: 0.000211 | valid loss: 0.000241\n","Epoch:  1708 | train loss: 0.000267 | valid loss: 0.000238\n","Epoch:  1709 | train loss: 0.000216 | valid loss: 0.000242\n","Epoch:  1710 | train loss: 0.000244 | valid loss: 0.000239\n","Epoch:  1711 | train loss: 0.000244 | valid loss: 0.000241\n","Epoch:  1712 | train loss: 0.000317 | valid loss: 0.000238\n","Epoch:  1713 | train loss: 0.000290 | valid loss: 0.000241\n","Epoch:  1714 | train loss: 0.000209 | valid loss: 0.000235\n","Epoch:  1715 | train loss: 0.000210 | valid loss: 0.000235\n","Epoch:  1716 | train loss: 0.000266 | valid loss: 0.000237\n","Epoch:  1717 | train loss: 0.000202 | valid loss: 0.000235\n","Epoch:  1718 | train loss: 0.000214 | valid loss: 0.000241\n","Epoch:  1719 | train loss: 0.000252 | valid loss: 0.000237\n","Epoch:  1720 | train loss: 0.000236 | valid loss: 0.000235\n","Epoch:  1721 | train loss: 0.000219 | valid loss: 0.000235\n","Epoch:  1722 | train loss: 0.000309 | valid loss: 0.000237\n","Epoch:  1723 | train loss: 0.000234 | valid loss: 0.000237\n","Epoch:  1724 | train loss: 0.000268 | valid loss: 0.000236\n","Epoch:  1725 | train loss: 0.000255 | valid loss: 0.000236\n","Epoch:  1726 | train loss: 0.000284 | valid loss: 0.000239\n","Epoch:  1727 | train loss: 0.000264 | valid loss: 0.000244\n","Epoch:  1728 | train loss: 0.000238 | valid loss: 0.000235\n","Epoch:  1729 | train loss: 0.000202 | valid loss: 0.000236\n","Epoch:  1730 | train loss: 0.000236 | valid loss: 0.000236\n","Epoch:  1731 | train loss: 0.000275 | valid loss: 0.000239\n","Epoch:  1732 | train loss: 0.000215 | valid loss: 0.000235\n","Epoch:  1733 | train loss: 0.000221 | valid loss: 0.000235\n","Epoch:  1734 | train loss: 0.000271 | valid loss: 0.000239\n","Epoch:  1735 | train loss: 0.000251 | valid loss: 0.000242\n","Epoch:  1736 | train loss: 0.000253 | valid loss: 0.000241\n","Epoch:  1737 | train loss: 0.000230 | valid loss: 0.000237\n","Epoch:  1738 | train loss: 0.000227 | valid loss: 0.000238\n","Epoch:  1739 | train loss: 0.000217 | valid loss: 0.000240\n","Epoch:  1740 | train loss: 0.000233 | valid loss: 0.000237\n","Epoch:  1741 | train loss: 0.000259 | valid loss: 0.000241\n","Epoch:  1742 | train loss: 0.000335 | valid loss: 0.000240\n","Epoch:  1743 | train loss: 0.000284 | valid loss: 0.000237\n","Epoch:  1744 | train loss: 0.000181 | valid loss: 0.000235\n","Epoch:  1745 | train loss: 0.000227 | valid loss: 0.000238\n","Epoch:  1746 | train loss: 0.000315 | valid loss: 0.000234\n","Epoch:  1747 | train loss: 0.000181 | valid loss: 0.000235\n","Epoch:  1748 | train loss: 0.000469 | valid loss: 0.000239\n","Epoch:  1749 | train loss: 0.000273 | valid loss: 0.000237\n","Epoch:  1750 | train loss: 0.000185 | valid loss: 0.000237\n","Epoch:  1751 | train loss: 0.000288 | valid loss: 0.000236\n","Epoch:  1752 | train loss: 0.000283 | valid loss: 0.000240\n","Epoch:  1753 | train loss: 0.000220 | valid loss: 0.000235\n","Epoch:  1754 | train loss: 0.000181 | valid loss: 0.000239\n","Epoch:  1755 | train loss: 0.000236 | valid loss: 0.000241\n","Epoch:  1756 | train loss: 0.000250 | valid loss: 0.000244\n","Epoch:  1757 | train loss: 0.000182 | valid loss: 0.000235\n","Epoch:  1758 | train loss: 0.000212 | valid loss: 0.000235\n","Epoch:  1759 | train loss: 0.000225 | valid loss: 0.000236\n","Epoch:  1760 | train loss: 0.000238 | valid loss: 0.000238\n","Epoch:  1761 | train loss: 0.000243 | valid loss: 0.000244\n","Epoch:  1762 | train loss: 0.000193 | valid loss: 0.000235\n","Epoch:  1763 | train loss: 0.000288 | valid loss: 0.000236\n","Epoch:  1764 | train loss: 0.000225 | valid loss: 0.000234\n","Epoch:  1765 | train loss: 0.000258 | valid loss: 0.000238\n","Epoch:  1766 | train loss: 0.000232 | valid loss: 0.000234\n","Epoch:  1767 | train loss: 0.000239 | valid loss: 0.000237\n","Epoch:  1768 | train loss: 0.000273 | valid loss: 0.000247\n","Epoch:  1769 | train loss: 0.000226 | valid loss: 0.000234\n","Epoch:  1770 | train loss: 0.000266 | valid loss: 0.000234\n","Epoch:  1771 | train loss: 0.000213 | valid loss: 0.000236\n","Epoch:  1772 | train loss: 0.000291 | valid loss: 0.000235\n","Epoch:  1773 | train loss: 0.000194 | valid loss: 0.000234\n","Epoch:  1774 | train loss: 0.000251 | valid loss: 0.000234\n","Epoch:  1775 | train loss: 0.000281 | valid loss: 0.000235\n","Epoch:  1776 | train loss: 0.000249 | valid loss: 0.000236\n","Epoch:  1777 | train loss: 0.000281 | valid loss: 0.000239\n","Epoch:  1778 | train loss: 0.000192 | valid loss: 0.000235\n","Epoch:  1779 | train loss: 0.000211 | valid loss: 0.000235\n","Epoch:  1780 | train loss: 0.000247 | valid loss: 0.000235\n","Epoch:  1781 | train loss: 0.000211 | valid loss: 0.000239\n","Epoch:  1782 | train loss: 0.000264 | valid loss: 0.000238\n","Epoch:  1783 | train loss: 0.000255 | valid loss: 0.000235\n","Epoch:  1784 | train loss: 0.000249 | valid loss: 0.000241\n","Epoch:  1785 | train loss: 0.000236 | valid loss: 0.000240\n","Epoch:  1786 | train loss: 0.000241 | valid loss: 0.000239\n","Epoch:  1787 | train loss: 0.000310 | valid loss: 0.000234\n","Epoch:  1788 | train loss: 0.000188 | valid loss: 0.000235\n","Epoch:  1789 | train loss: 0.000204 | valid loss: 0.000236\n","Epoch:  1790 | train loss: 0.000240 | valid loss: 0.000235\n","Epoch:  1791 | train loss: 0.000365 | valid loss: 0.000234\n","Epoch:  1792 | train loss: 0.000208 | valid loss: 0.000235\n","Epoch:  1793 | train loss: 0.000258 | valid loss: 0.000239\n","Epoch:  1794 | train loss: 0.000234 | valid loss: 0.000234\n","Epoch:  1795 | train loss: 0.000272 | valid loss: 0.000243\n","Epoch:  1796 | train loss: 0.000280 | valid loss: 0.000234\n","Epoch:  1797 | train loss: 0.000191 | valid loss: 0.000234\n","Epoch:  1798 | train loss: 0.000223 | valid loss: 0.000234\n","Epoch:  1799 | train loss: 0.000220 | valid loss: 0.000233\n","Epoch:  1800 | train loss: 0.000175 | valid loss: 0.000235\n","Epoch:  1801 | train loss: 0.000257 | valid loss: 0.000234\n","Epoch:  1802 | train loss: 0.000238 | valid loss: 0.000238\n","Epoch:  1803 | train loss: 0.000241 | valid loss: 0.000235\n","Epoch:  1804 | train loss: 0.000219 | valid loss: 0.000235\n","Epoch:  1805 | train loss: 0.000244 | valid loss: 0.000235\n","Epoch:  1806 | train loss: 0.000182 | valid loss: 0.000234\n","Epoch:  1807 | train loss: 0.000266 | valid loss: 0.000234\n","Epoch:  1808 | train loss: 0.000216 | valid loss: 0.000233\n","Epoch:  1809 | train loss: 0.000241 | valid loss: 0.000233\n","Epoch:  1810 | train loss: 0.000262 | valid loss: 0.000233\n","Epoch:  1811 | train loss: 0.000299 | valid loss: 0.000241\n","Epoch:  1812 | train loss: 0.000229 | valid loss: 0.000237\n","Epoch:  1813 | train loss: 0.000281 | valid loss: 0.000238\n","Epoch:  1814 | train loss: 0.000299 | valid loss: 0.000237\n","Epoch:  1815 | train loss: 0.000248 | valid loss: 0.000234\n","Epoch:  1816 | train loss: 0.000255 | valid loss: 0.000238\n","Epoch:  1817 | train loss: 0.000225 | valid loss: 0.000235\n","Epoch:  1818 | train loss: 0.000265 | valid loss: 0.000239\n","Epoch:  1819 | train loss: 0.000267 | valid loss: 0.000235\n","Epoch:  1820 | train loss: 0.000250 | valid loss: 0.000235\n","Epoch:  1821 | train loss: 0.000172 | valid loss: 0.000233\n","Epoch:  1822 | train loss: 0.000211 | valid loss: 0.000234\n","Epoch:  1823 | train loss: 0.000306 | valid loss: 0.000234\n","Epoch:  1824 | train loss: 0.000229 | valid loss: 0.000234\n","Epoch:  1825 | train loss: 0.000238 | valid loss: 0.000240\n","Epoch:  1826 | train loss: 0.000205 | valid loss: 0.000239\n","Epoch:  1827 | train loss: 0.000274 | valid loss: 0.000233\n","Epoch:  1828 | train loss: 0.000300 | valid loss: 0.000233\n","Epoch:  1829 | train loss: 0.000244 | valid loss: 0.000233\n","Epoch:  1830 | train loss: 0.000249 | valid loss: 0.000237\n","Epoch:  1831 | train loss: 0.000212 | valid loss: 0.000234\n","Epoch:  1832 | train loss: 0.000279 | valid loss: 0.000233\n","Epoch:  1833 | train loss: 0.000227 | valid loss: 0.000234\n","Epoch:  1834 | train loss: 0.000222 | valid loss: 0.000233\n","Epoch:  1835 | train loss: 0.000242 | valid loss: 0.000234\n","Epoch:  1836 | train loss: 0.000247 | valid loss: 0.000235\n","Epoch:  1837 | train loss: 0.000225 | valid loss: 0.000233\n","Epoch:  1838 | train loss: 0.000258 | valid loss: 0.000235\n","Epoch:  1839 | train loss: 0.000196 | valid loss: 0.000236\n","Epoch:  1840 | train loss: 0.000244 | valid loss: 0.000233\n","Epoch:  1841 | train loss: 0.000238 | valid loss: 0.000234\n","Epoch:  1842 | train loss: 0.000250 | valid loss: 0.000235\n","Epoch:  1843 | train loss: 0.000243 | valid loss: 0.000241\n","Epoch:  1844 | train loss: 0.000256 | valid loss: 0.000232\n","Epoch:  1845 | train loss: 0.000353 | valid loss: 0.000250\n","Epoch:  1846 | train loss: 0.000227 | valid loss: 0.000232\n","Epoch:  1847 | train loss: 0.000299 | valid loss: 0.000233\n","Epoch:  1848 | train loss: 0.000220 | valid loss: 0.000233\n","Epoch:  1849 | train loss: 0.000245 | valid loss: 0.000233\n","Epoch:  1850 | train loss: 0.000238 | valid loss: 0.000233\n","Epoch:  1851 | train loss: 0.000228 | valid loss: 0.000233\n","Epoch:  1852 | train loss: 0.000309 | valid loss: 0.000233\n","Epoch:  1853 | train loss: 0.000213 | valid loss: 0.000236\n","Epoch:  1854 | train loss: 0.000207 | valid loss: 0.000235\n","Epoch:  1855 | train loss: 0.000237 | valid loss: 0.000239\n","Epoch:  1856 | train loss: 0.000243 | valid loss: 0.000237\n","Epoch:  1857 | train loss: 0.000193 | valid loss: 0.000233\n","Epoch:  1858 | train loss: 0.000233 | valid loss: 0.000234\n","Epoch:  1859 | train loss: 0.000233 | valid loss: 0.000234\n","Epoch:  1860 | train loss: 0.000273 | valid loss: 0.000238\n","Epoch:  1861 | train loss: 0.000255 | valid loss: 0.000236\n","Epoch:  1862 | train loss: 0.000324 | valid loss: 0.000237\n","Epoch:  1863 | train loss: 0.000257 | valid loss: 0.000233\n","Epoch:  1864 | train loss: 0.000280 | valid loss: 0.000235\n","Epoch:  1865 | train loss: 0.000227 | valid loss: 0.000234\n","Epoch:  1866 | train loss: 0.000225 | valid loss: 0.000233\n","Epoch:  1867 | train loss: 0.000217 | valid loss: 0.000232\n","Epoch:  1868 | train loss: 0.000218 | valid loss: 0.000238\n","Epoch:  1869 | train loss: 0.000242 | valid loss: 0.000233\n","Epoch:  1870 | train loss: 0.000231 | valid loss: 0.000233\n","Epoch:  1871 | train loss: 0.000319 | valid loss: 0.000232\n","Epoch:  1872 | train loss: 0.000199 | valid loss: 0.000232\n","Epoch:  1873 | train loss: 0.000246 | valid loss: 0.000233\n","Epoch:  1874 | train loss: 0.000232 | valid loss: 0.000233\n","Epoch:  1875 | train loss: 0.000234 | valid loss: 0.000232\n","Epoch:  1876 | train loss: 0.000303 | valid loss: 0.000234\n","Epoch:  1877 | train loss: 0.000201 | valid loss: 0.000234\n","Epoch:  1878 | train loss: 0.000247 | valid loss: 0.000233\n","Epoch:  1879 | train loss: 0.000297 | valid loss: 0.000233\n","Epoch:  1880 | train loss: 0.000266 | valid loss: 0.000232\n","Epoch:  1881 | train loss: 0.000294 | valid loss: 0.000231\n","Epoch:  1882 | train loss: 0.000240 | valid loss: 0.000232\n","Epoch:  1883 | train loss: 0.000211 | valid loss: 0.000233\n","Epoch:  1884 | train loss: 0.000305 | valid loss: 0.000233\n","Epoch:  1885 | train loss: 0.000289 | valid loss: 0.000235\n","Epoch:  1886 | train loss: 0.000276 | valid loss: 0.000232\n","Epoch:  1887 | train loss: 0.000263 | valid loss: 0.000234\n","Epoch:  1888 | train loss: 0.000192 | valid loss: 0.000233\n","Epoch:  1889 | train loss: 0.000261 | valid loss: 0.000240\n","Epoch:  1890 | train loss: 0.000215 | valid loss: 0.000235\n","Epoch:  1891 | train loss: 0.000242 | valid loss: 0.000235\n","Epoch:  1892 | train loss: 0.000260 | valid loss: 0.000235\n","Epoch:  1893 | train loss: 0.000205 | valid loss: 0.000232\n","Epoch:  1894 | train loss: 0.000270 | valid loss: 0.000233\n","Epoch:  1895 | train loss: 0.000237 | valid loss: 0.000234\n","Epoch:  1896 | train loss: 0.000226 | valid loss: 0.000232\n","Epoch:  1897 | train loss: 0.000225 | valid loss: 0.000233\n","Epoch:  1898 | train loss: 0.000211 | valid loss: 0.000236\n","Epoch:  1899 | train loss: 0.000195 | valid loss: 0.000232\n","Epoch:  1900 | train loss: 0.000244 | valid loss: 0.000231\n","Epoch:  1901 | train loss: 0.000202 | valid loss: 0.000231\n","Epoch:  1902 | train loss: 0.000211 | valid loss: 0.000231\n","Epoch:  1903 | train loss: 0.000187 | valid loss: 0.000233\n","Epoch:  1904 | train loss: 0.000182 | valid loss: 0.000233\n","Epoch:  1905 | train loss: 0.000282 | valid loss: 0.000235\n","Epoch:  1906 | train loss: 0.000352 | valid loss: 0.000234\n","Epoch:  1907 | train loss: 0.000274 | valid loss: 0.000232\n","Epoch:  1908 | train loss: 0.000230 | valid loss: 0.000233\n","Epoch:  1909 | train loss: 0.000206 | valid loss: 0.000235\n","Epoch:  1910 | train loss: 0.000183 | valid loss: 0.000236\n","Epoch:  1911 | train loss: 0.000274 | valid loss: 0.000231\n","Epoch:  1912 | train loss: 0.000236 | valid loss: 0.000232\n","Epoch:  1913 | train loss: 0.000217 | valid loss: 0.000231\n","Epoch:  1914 | train loss: 0.000241 | valid loss: 0.000238\n","Epoch:  1915 | train loss: 0.000217 | valid loss: 0.000236\n","Epoch:  1916 | train loss: 0.000254 | valid loss: 0.000232\n","Epoch:  1917 | train loss: 0.000202 | valid loss: 0.000233\n","Epoch:  1918 | train loss: 0.000244 | valid loss: 0.000233\n","Epoch:  1919 | train loss: 0.000238 | valid loss: 0.000231\n","Epoch:  1920 | train loss: 0.000316 | valid loss: 0.000232\n","Epoch:  1921 | train loss: 0.000218 | valid loss: 0.000232\n","Epoch:  1922 | train loss: 0.000282 | valid loss: 0.000232\n","Epoch:  1923 | train loss: 0.000221 | valid loss: 0.000233\n","Epoch:  1924 | train loss: 0.000322 | valid loss: 0.000241\n","Epoch:  1925 | train loss: 0.000289 | valid loss: 0.000232\n","Epoch:  1926 | train loss: 0.000238 | valid loss: 0.000231\n","Epoch:  1927 | train loss: 0.000300 | valid loss: 0.000231\n","Epoch:  1928 | train loss: 0.000208 | valid loss: 0.000231\n","Epoch:  1929 | train loss: 0.000191 | valid loss: 0.000233\n","Epoch:  1930 | train loss: 0.000232 | valid loss: 0.000233\n","Epoch:  1931 | train loss: 0.000246 | valid loss: 0.000234\n","Epoch:  1932 | train loss: 0.000288 | valid loss: 0.000234\n","Epoch:  1933 | train loss: 0.000199 | valid loss: 0.000232\n","Epoch:  1934 | train loss: 0.000240 | valid loss: 0.000231\n","Epoch:  1935 | train loss: 0.000215 | valid loss: 0.000236\n","Epoch:  1936 | train loss: 0.000243 | valid loss: 0.000235\n","Epoch:  1937 | train loss: 0.000228 | valid loss: 0.000240\n","Epoch:  1938 | train loss: 0.000212 | valid loss: 0.000236\n","Epoch:  1939 | train loss: 0.000199 | valid loss: 0.000230\n","Epoch:  1940 | train loss: 0.000218 | valid loss: 0.000230\n","Epoch:  1941 | train loss: 0.000201 | valid loss: 0.000231\n","Epoch:  1942 | train loss: 0.000200 | valid loss: 0.000231\n","Epoch:  1943 | train loss: 0.000246 | valid loss: 0.000231\n","Epoch:  1944 | train loss: 0.000204 | valid loss: 0.000231\n","Epoch:  1945 | train loss: 0.000255 | valid loss: 0.000231\n","Epoch:  1946 | train loss: 0.000302 | valid loss: 0.000233\n","Epoch:  1947 | train loss: 0.000193 | valid loss: 0.000234\n","Epoch:  1948 | train loss: 0.000225 | valid loss: 0.000236\n","Epoch:  1949 | train loss: 0.000225 | valid loss: 0.000235\n","Epoch:  1950 | train loss: 0.000218 | valid loss: 0.000231\n","Epoch:  1951 | train loss: 0.000179 | valid loss: 0.000231\n","Epoch:  1952 | train loss: 0.000244 | valid loss: 0.000234\n","Epoch:  1953 | train loss: 0.000234 | valid loss: 0.000233\n","Epoch:  1954 | train loss: 0.000223 | valid loss: 0.000230\n","Epoch:  1955 | train loss: 0.000205 | valid loss: 0.000234\n","Epoch:  1956 | train loss: 0.000299 | valid loss: 0.000230\n","Epoch:  1957 | train loss: 0.000186 | valid loss: 0.000233\n","Epoch:  1958 | train loss: 0.000208 | valid loss: 0.000231\n","Epoch:  1959 | train loss: 0.000236 | valid loss: 0.000230\n","Epoch:  1960 | train loss: 0.000193 | valid loss: 0.000236\n","Epoch:  1961 | train loss: 0.000271 | valid loss: 0.000233\n","Epoch:  1962 | train loss: 0.000194 | valid loss: 0.000230\n","Epoch:  1963 | train loss: 0.000204 | valid loss: 0.000229\n","Epoch:  1964 | train loss: 0.000299 | valid loss: 0.000232\n","Epoch:  1965 | train loss: 0.000265 | valid loss: 0.000231\n","Epoch:  1966 | train loss: 0.000280 | valid loss: 0.000232\n","Epoch:  1967 | train loss: 0.000249 | valid loss: 0.000233\n","Epoch:  1968 | train loss: 0.000250 | valid loss: 0.000232\n","Epoch:  1969 | train loss: 0.000219 | valid loss: 0.000233\n","Epoch:  1970 | train loss: 0.000232 | valid loss: 0.000233\n","Epoch:  1971 | train loss: 0.000180 | valid loss: 0.000230\n","Epoch:  1972 | train loss: 0.000287 | valid loss: 0.000240\n","Epoch:  1973 | train loss: 0.000217 | valid loss: 0.000230\n","Epoch:  1974 | train loss: 0.000216 | valid loss: 0.000230\n","Epoch:  1975 | train loss: 0.000240 | valid loss: 0.000230\n","Epoch:  1976 | train loss: 0.000200 | valid loss: 0.000231\n","Epoch:  1977 | train loss: 0.000243 | valid loss: 0.000233\n","Epoch:  1978 | train loss: 0.000203 | valid loss: 0.000230\n","Epoch:  1979 | train loss: 0.000203 | valid loss: 0.000231\n","Epoch:  1980 | train loss: 0.000219 | valid loss: 0.000235\n","Epoch:  1981 | train loss: 0.000194 | valid loss: 0.000233\n","Epoch:  1982 | train loss: 0.000252 | valid loss: 0.000231\n","Epoch:  1983 | train loss: 0.000256 | valid loss: 0.000234\n","Epoch:  1984 | train loss: 0.000203 | valid loss: 0.000236\n","Epoch:  1985 | train loss: 0.000214 | valid loss: 0.000239\n","Epoch:  1986 | train loss: 0.000189 | valid loss: 0.000235\n","Epoch:  1987 | train loss: 0.000235 | valid loss: 0.000230\n","Epoch:  1988 | train loss: 0.000213 | valid loss: 0.000232\n","Epoch:  1989 | train loss: 0.000200 | valid loss: 0.000229\n","Epoch:  1990 | train loss: 0.000243 | valid loss: 0.000229\n","Epoch:  1991 | train loss: 0.000172 | valid loss: 0.000231\n","Epoch:  1992 | train loss: 0.000284 | valid loss: 0.000231\n","Epoch:  1993 | train loss: 0.000255 | valid loss: 0.000233\n","Epoch:  1994 | train loss: 0.000323 | valid loss: 0.000230\n","Epoch:  1995 | train loss: 0.000215 | valid loss: 0.000229\n","Epoch:  1996 | train loss: 0.000215 | valid loss: 0.000233\n","Epoch:  1997 | train loss: 0.000250 | valid loss: 0.000231\n","Epoch:  1998 | train loss: 0.000224 | valid loss: 0.000235\n","Epoch:  1999 | train loss: 0.000276 | valid loss: 0.000229\n","Epoch:  2000 | train loss: 0.000236 | valid loss: 0.000231\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7swNIOltJolK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628490606256,"user_tz":-60,"elapsed":29,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"cb9a38e8-253e-43b1-989d-652b9a914fef"},"source":["print(t_train_1-t_train_0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["24176.847862243652\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oSTwMkrcA1Hu"},"source":["### Save and Plot"]},{"cell_type":"code","metadata":{"id":"Wh66MCdZA2PI","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"ok","timestamp":1629606564602,"user_tz":-60,"elapsed":1626,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"b959a31c-88f1-4391-9b2a-45a7d82ce5fa"},"source":["pathName = \"./HAE/csv/II_I_X_Eran2000_LV256_B16_n1600_L0.0001.csv\"\n","name = \"SFC-HAE MSE loss of 256 compression variables\"\n","PlotMSELoss(pathName,name)"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x504 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"iSSbrROBA4bS"},"source":["### Calculate MSE"]},{"cell_type":"code","metadata":{"id":"xjk0juxQA4ym","executionInfo":{"status":"ok","timestamp":1629606684275,"user_tz":-60,"elapsed":604,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}}},"source":["autoencoder_3 = torch.load(\"./HAE/pkl/II_I_X_Eran2000_LV256_B16_n1600_L0.0001.pkl\")"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUObl12pKBWi","executionInfo":{"status":"ok","timestamp":1629606697271,"user_tz":-60,"elapsed":9089,"user":{"displayName":"杨钒","photoUrl":"","userId":"08129041439789093783"}},"outputId":"0ba594ec-eee7-41f9-e6ff-b4b24e7d6b4c"},"source":["# pass training, validation and test data through the autoencoder\n","t_predict_0 = time.time()\n","\n","mode_3train, training_decoded_3 = autoencoder_3.to(device)(torch.tensor(training_data).to(device),mode_2train.float().to(device))\n","error_autoencoder = (training_decoded_3.cpu().detach().numpy() - training_data[:,:,3:5])\n","print(\"MSE_err of training data\", (error_autoencoder**2).mean())\n","\n","mode_3valid, valid_decoded_3 = autoencoder_3.to(device)(torch.tensor(valid_data).to(device),mode_2valid.float().to(device))\n","error_autoencoder = (valid_decoded_3.cpu().detach().numpy() - valid_data[:, :, 3:5])\n","print(\"Mse_err of validation data\", (error_autoencoder**2).mean())\n","\n","mode_3test, test_decoded_3 = autoencoder_3.to(device)(torch.tensor(test_data).to(device),mode_2test.float().to(device))\n","error_autoencoder = (test_decoded_3.cpu().detach().numpy() - test_data[:, :, 3:5])\n","print(\"Mse_err of test data\", (error_autoencoder**2).mean())\n","\n","t_predict_1 = time.time()\n","# total_decoded2 = getTotal_decoded(training_decoded_2,valid_decoded_2,test_decoded_2,train_index,valid_index,test_index)\n","# error_autoencoder = (total_decoded2 - total_data[:, :, 3:5])\n","# print(\"Mse_err of total data\", (error_autoencoder**2).mean())\n","\n","print(mode_3train.shape)\n","print(mode_3valid.shape)\n","print(mode_3test.shape)\n","print(\"Prediction time: \",t_predict_1-t_predict_0)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["MSE_err of training data 0.00023654493707881105\n","Mse_err of validation data 0.00023092803624754837\n","Mse_err of test data 0.0002263468299229382\n","torch.Size([1600, 256])\n","torch.Size([200, 256])\n","torch.Size([200, 256])\n","Prediction time:  8.833876609802246\n"],"name":"stdout"}]}]}